{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "125c823e",
   "metadata": {},
   "source": [
    "# 第八章 &nbsp; &nbsp; Attention\n",
    "上一章我们使用 RNN 生成了文本，又通过连接两个 RNN，将一个时序数据转换为了另一个时序数据。我们将这个网络称为 seq2seq，并用它成功求解了简单的加法问题。之后，我们对这个 seq2seq 进行了几处改进，几乎完美地解决了这个简单的加法问题。\n",
    "\n",
    "本章我们将进一步探索 seq2seq 的可能性（以及 RNN 的可能性）。这里，Attention 这一强大而优美的技术将登场。Attention 毫无疑问是近年来深度学习领域最重要的技术之一。本章的目标是在代码层面理解 Attention 的结构，然后将其应用于实际问题，体验它的奇妙效果。\n",
    "\n",
    "## Attention的结构\n",
    "如上一章所述，seq2seq 是一个非常强大的框架，应用面很广。这里我们将介绍进一步强化 seq2seq 的**注意力机制**（attention mechanism，简称 Attention）。基于 Attention 机制，seq2seq 可以像我们人类一样，将 “注意力” 集中在必要的信息上。此外，使用 Attention 可以解决当前 seq2seq 面临的问题。\n",
    "\n",
    "本节我们将首先指出当前 seq2seq 存在的问题，然后一边说明 Attention 的结构，一边对其进行实现。\n",
    "\n",
    "上一章我们已经对seq2seq进行了改进，但那些只能算是“小改进”。下面将要说明的Attention技术才是解决seq2seq的问题的“大改进”。\n",
    "\n",
    "## seq2seq存在的问题\n",
    "seq2seq 中使用编码器对时序数据进行编码，然后将编码信息传递给解码器。此时，编码器的输出是固定长度的向量。实际上，这个 “固定长度” 存在很大问题。因为固定长度的向量意味着，无论输入语句的长度如何（无论多长），都会被转换为长度相同的向量。以上一章的翻译为例，如图所示，不管输入的文本如何，都需要将其塞入一个固定长度的向量中。\n",
    "\n",
    "<img src=\"./fig/encoder.png\" alt=\"encoder\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "无论多长的文本，当前的编码器都会将其转换为固定长度的向量。就像把一大堆西装塞入衣柜里一样，编码器强行把信息塞入固定长度的向量中。但是，这样做早晚会遇到瓶颈。就像最终西装会从衣柜中掉出来一样，有用的信息也会从向量中溢出。\n",
    "\n",
    "现在我们就来改进 seq2seq。首先改进编码器，然后再改进解码器。\n",
    "\n",
    "## 编码器的改进1\n",
    "到目前为止，我们都只将 LSTM 层的最后的隐藏状态传递给解码器，但是编码器的输出的长度应该根据输入文本的长度相应地改变。这是编码器的一个可以改进的地方。具体而言，如图所示，使用各个时刻的 LSTM 层的隐藏状态。\n",
    "\n",
    "<img src=\"./fig/encoder_all_time.png\" alt=\"encoder_all_time\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，使用各个时刻（各个单词）的隐藏状态向量，可以获得和输入的单词数相同数量的向量。在图中的例子中，输入了 5 个单词，此时编码器输出 5 个向量。这样一来，编码器就摆脱了 “一个固定长度的向量” 的制约。\n",
    "\n",
    "在许多深度学习框架中，在初始化RNN层（或者LSTM层、GRU层等）时，可以选择是返回“全部时刻的隐藏状态向量”，还是返回“最后时刻的隐藏状态向量”。比如，在Keras中，在初始化RNN层时，可以设置 <code>return_sequences</code> 为True或者False。\n",
    "\n",
    "上图中我们需要关注 LSTM 层的隐藏状态的 “内容”。此时，各个时刻的 LSTM 层的隐藏状态都充满了什么信息呢？有一点可以确定的是，各个时刻的隐藏状态中包含了大量当前时刻的输入单词的信息。就图中的例子来说，输入 “猫” 时的 LSTM 层的输出（隐藏状态）受此时输入的单词 “猫” 的影响最大。因此，可以认为这个隐藏状态向量蕴含许多 “猫的成分”。按照这样的理解，如图所示，编码器输出的 $\\boldsymbol{h}\\boldsymbol{s}$ 矩阵就可以视为各个单词对应的向量集合。\n",
    "\n",
    "<img src=\"./fig/encoder_hs.png\" alt=\"encoder_hs\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "因为编码器是从左向右处理的，所以严格来说，刚才的“猫”向量中含有“吾輩”“は”“猫”这3个单词的信息。考虑整体的平衡性，最好均衡地含有单词“猫”周围的信息。在这种情况下，从两个方向处理时序数据的双向RNN（或者双向LSTM）比较有效。我们后面再介绍双向RNN，这里先继续使用单向LSTM。\n",
    "\n",
    "以上就是对编码器的改进。这里我们所做的改进只是将编码器的全部时刻的隐藏状态取出来而已。通过这个小改动，编码器可以根据输入语句的长度，成比例地编码信息。那么，解码器又将如何处理这个编码器的输出呢？接下来，我们对解码器进行改进。因为解码器的改进有许多值得讨论的地方，所以我们分 3 部分进行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2068c3ae",
   "metadata": {},
   "source": [
    "## 解码器的改进\n",
    "编码器整体输出各个单词对应的 LSTM 层的隐藏状态向量 $\\boldsymbol{h}\\boldsymbol{s}$。然后，这个 $\\boldsymbol{h}\\boldsymbol{s}$ 被传递给解码器，以进行时间序列的转换。\n",
    "\n",
    "<img src=\"./fig/relation.png\" alt=\"relation\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "顺便说一下，在上一章的最简单的 seq2seq 中，仅将编码器最后的隐藏状态向量传递给了解码器。严格来说，这是将编码器的 LSTM 层的 “最后” 的隐藏状态放入了解码器的 LSTM 层的 “最初” 的隐藏状态。用图来表示的话，解码器的层结构如图所示。\n",
    "\n",
    "<img src=\"./fig/decoder_before.png\" alt=\"decoder_before\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，上一章的解码器只用了编码器的 LSTM 层的最后的隐藏状态。如果使用 $\\boldsymbol{h}\\boldsymbol{s}$，则只提取最后一行，再将其传递给解码器。下面我们改进解码器，以便能够使用全部 $\\boldsymbol{h}\\boldsymbol{s}$。\n",
    "\n",
    "我们在进行翻译时，大脑做了什么呢？比如，在将 “吾輩は猫である” 这句话翻译为英文时，肯定要用到诸如 “吾輩 = I”“猫 = cat” 这样的知识。也就是说，可以认为我们是专注于某个单词（或者单词集合），随时对这个单词进行转换的。那么，我们可以在 seq2seq 中重现同样的事情吗？确切地说，我们可以让 seq2seq 学习 “输入和输出中哪些单词与哪些单词有关” 这样的对应关系吗？\n",
    "\n",
    "在机器翻译的历史中，很多研究都利用“猫=cat”这样的单词对应关系的知识。这样的表示单词（或者词组）对应关系的信息称为<strong>对齐</strong>（alignment）。到目前为止，对齐主要是手工完成的，而我们将要介绍的 Attention 技术则成功地将对齐思想自动引入到了 seq2seq 中。这也是从“手工操作”到“机械自动化”的演变。\n",
    "\n",
    "从现在开始，我们的目标是找出与 “翻译目标词” 有对应关系的 “翻译源词” 的信息，然后利用这个信息进行翻译。也就是说，我们的目标是仅关注必要的信息，并根据该信息进行时序转换。这个机制称为 Attention，是本章的主题。\n",
    "\n",
    "在介绍 Attention 的细节之前，这里我们先给出它的整体框架。我们要实现的网络的层结构如下图所示。\n",
    "\n",
    "<img src=\"./fig/decoder_after.png\" alt=\"decoder_after\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，我们新增一个进行 “某种计算” 的层。这个 “某种计算” 接收（解码器）各个时刻的 LSTM 层的隐藏状态和编码器的 $\\boldsymbol{h}\\boldsymbol{s}$。然后，从中选出必要的信息，并输出到 Affine 层。与之前一样，编码器的最后的隐藏状态向量传递给解码器最初的 LSTM 层。\n",
    "\n",
    "上图中的网络所做的工作是提取单词对齐信息。具体来说，就是从 $\\boldsymbol{h}\\boldsymbol{s}$ 中选出与各个时刻解码器输出的单词有对应关系的单词向量。比如，当解码器输出 “I” 时，从 $\\boldsymbol{h}\\boldsymbol{s}$ 中选出 “吾輩” 的对应向量。也就是说，我们希望通过 “某种计算” 来实现这种选择操作。不过这里有个问题，就是选择（从多个事物中选取若干个）这一操作是无法进行微分的。\n",
    "\n",
    "神经网络的学习一般通过误差反向传播法进行。因此，如果使用可微分的运算构造网络，就可以在误差反向传播法的框架内进行学习；而如果不使用可微分的运算，基本上也就没有办法使用误差反向传播法。\n",
    "\n",
    "可否将 “选择” 这一操作换成可微分的运算呢？实际上，解决这个问题的思路很简单（但是，就像哥伦布蛋一样，第一个想到是很难的）。这个思路就是，与其 “单选”，不如 “全选”。如下图所示，我们另行计算表示各个单词重要度（贡献值）的权重。\n",
    "\n",
    "<img src=\"./fig/word_weight.png\" alt=\"word_weight\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，这里使用了表示各个单词重要度的权重（记为 $\\boldsymbol{a}$）。此时，$\\boldsymbol{a}$ 像概率分布一样，各元素是 0.0 ~ 1.0 的标量，总和是 1。然后，计算这个表示各个单词重要度的权重和单词向量 $\\boldsymbol{h}\\boldsymbol{s}$ 的加权和，可以获得目标向量。这一系列计算如下图所示。\n",
    "\n",
    "<img src=\"./fig/weight_addition.png\" alt=\"weight_addition\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，计算单词向量的加权和，这里将结果称为上下文向量，并用符号 $\\boldsymbol{c}$ 表示。顺便说一下，如果我们仔细观察，就可以发现 “吾輩” 对应的权重为 0.8。这意味着上下文向量 $\\boldsymbol{c}$ 中含有很多 “吾輩” 向量的成分，可以说这个加权和计算基本代替了 “选择” 向量的操作。假设 “吾輩” 对应的权重是 1，其他单词对应的权重是 0，那么这就相当于 “选择” 了 “吾輩” 向量。\n",
    "\n",
    "上下文向量 $\\boldsymbol{c}$ 中包含了当前时刻进行变换（翻译）所需的信息。更确切地说，模型要从数据中学习出这种能力。\n",
    "\n",
    "下面，我们从代码的角度来看一下目前为止的内容。这里随意地生成编码器的输出 $\\boldsymbol{h}\\boldsymbol{s}$ 和各个单词的权重 $\\boldsymbol{a}$，并给出求它们的加权和的实现，代码如下所示，请注意多维数组的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2200990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(5,)\n",
      "(5, 4)\n",
      "[[0.8  0.8  0.8  0.8 ]\n",
      " [0.1  0.1  0.1  0.1 ]\n",
      " [0.03 0.03 0.03 0.03]\n",
      " [0.05 0.05 0.05 0.05]\n",
      " [0.02 0.02 0.02 0.02]]\n",
      "(5, 4)\n",
      "(4,)\n",
      "[0.90783898 0.22390136 0.66314289 0.76434554]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T, H = 5, 4 # T: 时间步数, H: 隐藏状态向量的元素个数\n",
    "hs = np.random.rand(T, H) # 隐藏状态向量\n",
    "print(hs.shape) # (5, 4)\n",
    "\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02]) # 注意力权重\n",
    "print(a.shape) # (5,)\n",
    "\n",
    "ar = a.reshape(5, 1).repeat(4, axis=1) # repeat表示沿着指定轴复制元素，这里按照列方向复制4次\n",
    "print(ar.shape) # (5, 4)\n",
    "print(ar) # [[0.8 0.8 0.8 0.8] [0.1 0.1 0.1 0.1] [0.03 0.03 0.03 0.03] [0.05 0.05 0.05 0.05] [0.02 0.02 0.02 0.02]]\n",
    "\n",
    "t = hs * ar # 广播机制，对应元素相乘\n",
    "print(t.shape) # (5, 4)\n",
    "\n",
    "c = np.sum(t, axis=0) # 按行求和\n",
    "print(c.shape) # (4,)\n",
    "print(c) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5653ce",
   "metadata": {},
   "source": [
    "设时序数据的长度 $T=5$，隐藏状态向量的元素个数 $H=4$，这里给出了加权和的计算过程。我们先关注代码 $ar = a.reshape(5, 1).repeat(4, axis=1)$。如图 8-9 所示，这行代码将 $a$ 转化为 $ar$。\n",
    "\n",
    "<img src=\"./fig/reshape_and_repeat.png\" alt=\"reshape_and_repeat\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，我们要做的是复制形状为 `(5,)` 的 `a`，创建 `(5,4)` 的数组。因此，通过 `a.reshape(5, 1)` 将 `a` 的形状从 `(5,)` 转化为 `(5,1)`。然后，在第 1 个轴方向上（`axis=0`）重复这个变形后的数组 4 次，生成形状为 `(5,4)` 的数组。\n",
    "\n",
    "`repeat()` 方法复制多维数组的元素生成新的多维数组。设 `x` 为 NumPy 多维数组，则可以像 `x.repeat(rep, axis)` 这样使用。这里参数 `rep` 指定复制的次数，`axis` 指定要进行复制的轴（维度）。比如，在 `x` 的形状为 `(X, Y, Z)` 的情况下，`x.repeat(3, axis=1)` 沿 `x` 的第 1 个轴方向（第 1 个维度）进行复制，生成形状为 `(X, 3*Y, Z)` 的多维数组。\n",
    "\n",
    "此外，这里也可以不使用 `repeat()` 方法，而使用 NumPy 的广播功能。此时，令 `ar = a.reshape(5, 1)`，然后计算 `hs * ar`。如下图所示，`ar` 会自动扩展以匹配 `hs` 的形状。\n",
    "\n",
    "<img src=\"./fig/numpy_broadcast.png\" alt=\"numpy_broadcast\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "为了提高执行效率，这里应该使用 NumPy 的广播，而不是 `repeat()` 方法。但是，在这种情况下，需要注意的是，在许多我们看不见的地方多维数组的元素被复制了。由之前的小节可知，这相当于计算图中的 Repeat 节点。因此，在反向传播时，需要执行 Repeat 节点的反向传播。\n",
    "\n",
    "如上图所示，先计算对应元素的乘积，然后通过 `c = np.sum(hs*ar, axis=0)` 求和。这里，通过参数 `axis` 可以指定在哪个轴方向（维度）上求和。如果我们注意一下数组的形状，`axis` 的使用方法就会很清楚。比如，当 `x` 的形状为 `(X, Y, Z)` 时，`np.sum(x, axis=1)` 的输出（和）的形状为 `(X, Z)`。这里的重点是，求和会使一个轴 “消失”。在上面的例子中，`hs*ar` 的形状为 `(5,4)`，通过消除第 0 个轴，获得了形状为 `(4,)` 的矩阵（向量）。\n",
    "\n",
    "计算加权和最简单有效的方法是使用矩阵乘积。就上面的例子来说，只需要 `np.dot(a, hs)` 这一行代码就可以获得目标结果。不过，这样只能处理一笔数据（样本），很难将其扩展到批处理。如果非要扩展，就需要用到 “张量积”，这会使事情变得有些复杂（在这种情况下，需要使用 `np.tensordot()` 和 `np.einsum()` 方法）。简单起见，这里我们不使用矩阵乘积，而是通过 `repeat()` 和 `sum()` 方法来实现加权和的计算。\n",
    "\n",
    "下面进行批处理版的加权和的实现，具体如下所示（这里随机创建 `hs` 和 `a`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1f7f850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 4)\n",
      "(10, 5)\n",
      "(10, 5, 4)\n",
      "(10, 5, 4)\n",
      "(10, 4)\n",
      "[[1.01195067 1.03986192 1.40571894 1.64116333]\n",
      " [1.9763091  2.02767884 1.92690147 1.53661538]\n",
      " [1.51998232 1.13553073 1.51037607 1.21680207]\n",
      " [1.04276072 1.09254412 1.12593429 0.7116679 ]\n",
      " [1.8692966  1.47513212 1.42634727 1.71139373]\n",
      " [1.59130503 1.53944015 1.06293276 0.94520264]\n",
      " [0.5376699  0.71093776 0.69754462 0.72074355]\n",
      " [1.20785944 0.99786316 0.64943048 0.56494925]\n",
      " [1.88658035 1.12246527 2.68473801 2.40732741]\n",
      " [0.73441817 1.0343341  1.93592002 1.8492881 ]]\n"
     ]
    }
   ],
   "source": [
    "N, T, H = 10, 5, 4 # N: 批量大小, T: 时间步数, H: 隐藏状态向量的元素个数\n",
    "hs = np.random.rand(N, T, H) # 隐藏状态向量\n",
    "print(hs.shape) # (10, 5, 4)\n",
    "\n",
    "a = np.random.rand(N, T) # 注意力权重\n",
    "print(a.shape) # (10, 5)\n",
    "\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2) # repeat表示沿着指定轴复制元素，这里按照列方向复制4次\n",
    "# ar = a.reshape(N, T, 1) # 也可以不使用repeat，利用numpy的广播机制\n",
    "print(ar.shape) # (10, 5, 4)\n",
    "\n",
    "t = hs * ar # 广播机制，对应元素相乘\n",
    "print(t.shape) # (10, 5, 4)\n",
    "\n",
    "c = np.sum(t, axis=1) # 按行求和\n",
    "print(c.shape) # (10, 4)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef473a17",
   "metadata": {},
   "source": [
    "这里的批处理与之前的实现几乎一样。只要注意数组的形状，应该很快就能确定 `repeat()` 和 `sum()` 需要指定的维度（轴）。作为总结，我们把加权和的计算用计算图表示出来。\n",
    "\n",
    "<img src=\"./fig/addition_example.png\" alt=\"addition_example\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，这里使用 Repeat 节点复制 $a$。之后，通过 “$\\times$” 节点计算对应元素的乘积，通过 Sum 节点求和。现在考虑这个计算图的反向传播。其实，所需要的知识都已经齐备。第 1 章介绍了 Repeat 节点和 Sum 节点的反向传播。这里重述一下要点：“Repeat 的反向传播是 Sum”“Sum 的反向传播是 Repeat”。只要注意到张量的形状，就不难知道应该对哪个轴进行 Sum，对哪个轴进行 Repeat。\n",
    "\n",
    "现在我们将上图的计算图实现为层，这里称之为 Weight Sum 层，其实现如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb943999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape # N: 批量大小, T: 时间步数, H: 隐藏状态向量的元素个数\n",
    "\n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2) # repeat表示沿着指定轴复制元素，这里按照列方向复制H次\n",
    "        t = hs * ar # 广播机制，对应元素相乘\n",
    "        c = np.sum(t, axis=1) # 按行求和\n",
    "\n",
    "        self.cache = (hs, ar) # 反向传播时使用\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache # 从前向传播中取得缓存数据\n",
    "        N, T, H = hs.shape # N: 批量大小, T: 时间步数, H: 隐藏状态向量的元素个数\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1) # dc的维度为(N, H)，需要变形为(N, T, H)才能与hs和ar对应\n",
    "        dar = dt * hs # 广播机制，对应元素相乘\n",
    "        dhs = dt * ar # 广播机制，对应元素相乘\n",
    "        da = np.sum(dar, axis=2) # 按列求和\n",
    "\n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7426aa79",
   "metadata": {},
   "source": [
    "以上就是计算上下文向量的 Weight Sum 层的实现。因为这个层没有要学习的参数，所以根据本书的代码规范，此处为 `self.params = []`。其他应该没有特别难的地方，我们继续往下看。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68f6c56",
   "metadata": {},
   "source": [
    "## 解码器的改进2\n",
    "有了表示各个单词重要度的权重 $\\boldsymbol{a}$，就可以通过加权和获得上下文向量。那么，怎么求这个 $\\boldsymbol{a}$ 呢？当然不需要我们手动指定，我们只需要做好让模型从数据中自动学习它的准备工作。\n",
    "\n",
    "下面我们来看一下各个单词的权重 $\\boldsymbol{a}$ 的求解方法。首先，从编码器的处理开始到解码器第一个 LSTM 层输出隐藏状态向量的处理为止的流程如下图所示。\n",
    "\n",
    "<img src=\"./fig/decoder_first_LSTM.png\" alt=\"decoder_first_LSTM\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在图中，用 $\\boldsymbol{h}$ 表示解码器的 LSTM 层的隐藏状态向量。此时，我们的目标是用数值表示这个 $\\boldsymbol{h}$ 在多大程度上和 $\\boldsymbol{h}\\boldsymbol{s}$ 的各个单词向量 “相似”。有几种方法可以做到这一点，这里我们使用最简单的向量内积。顺便说一下，向量 $\\boldsymbol{a} = (a_1, a_2, \\cdots, a_n)$ 和向量 $\\boldsymbol{b} = (b_1, b_2, \\cdots, b_n)$ 的内积为：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{a} \\cdot \\boldsymbol{b} = a_1b_1 + a_2b_2 + \\cdots + a_nb_n \\tag{8.1}\n",
    "$$\n",
    "\n",
    "式 (8.1) 的含义是两个向量在多大程度上指向同一方向，因此使用内积作为两个向量的 “相似度” 是非常自然的选择。\n",
    "\n",
    "计算向量相似度的方法有好几种。除了内积之外，还有使用小型的神经网络输出得分的做法。\n",
    "\n",
    "下面用图表示基于内积计算向量间相似度的处理流程。\n",
    "\n",
    "<img src=\"./fig/dot.png\" alt=\"dot\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，这里通过向量内积算出 $\\boldsymbol{h}$ 和 $\\boldsymbol{h}\\boldsymbol{s}$ 的各个单词向量之间的相似度，并将其结果表示为 $\\boldsymbol{s}$。不过，这个 $\\boldsymbol{s}$ 是正规化之前的值，也称为得分。接下来，使用老一套的 Softmax 函数对 $\\boldsymbol{s}$ 进行正规化。\n",
    "\n",
    "<img src=\"./fig/softmax.png\" alt=\"softmax\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "使用 Softmax 函数之后，输出的 $\\boldsymbol{a}$ 的各个元素的值在 $0.0 \\sim 1.0$，总和为 1，这样就求得了表示各个单词权重的 $\\boldsymbol{a}$。现在我们从代码角度来看一下这些处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25ac262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.77695449 0.37656654 0.48170621 0.75729175]\n",
      "  [0.47520998 0.56609887 0.87964318 0.22298534]\n",
      "  [0.59360664 0.30738468 0.88985671 0.5849681 ]\n",
      "  [0.61770363 0.69425839 0.51242927 0.57657531]\n",
      "  [0.27941957 0.48287317 0.73900374 0.91587326]]\n",
      "\n",
      " [[0.79169181 0.31080077 0.51583566 0.10707723]\n",
      "  [0.29762806 0.25016266 0.07091503 0.69747741]\n",
      "  [0.61309348 0.46432729 0.60393699 0.39676065]\n",
      "  [0.7068785  0.71073355 0.23905607 0.13281973]\n",
      "  [0.71315434 0.58240203 0.69902012 0.14540145]]\n",
      "\n",
      " [[0.40068604 0.23516233 0.63518692 0.55852045]\n",
      "  [0.01397556 0.86549137 0.57917183 0.96483811]\n",
      "  [0.46597886 0.71403315 0.3532388  0.82671019]\n",
      "  [0.08069756 0.97821932 0.57290961 0.58462479]\n",
      "  [0.75509924 0.00599862 0.17637129 0.71280269]]\n",
      "\n",
      " [[0.616761   0.70684457 0.9244903  0.40323676]\n",
      "  [0.48433328 0.31320464 0.39397085 0.30806484]\n",
      "  [0.2963989  0.66071022 0.02993041 0.29550522]\n",
      "  [0.77730622 0.8892604  0.4248498  0.98874528]\n",
      "  [0.66546474 0.5303415  0.85028522 0.66064458]]\n",
      "\n",
      " [[0.85278191 0.68876347 0.79835549 0.63011548]\n",
      "  [0.77894713 0.67494207 0.09868029 0.04432592]\n",
      "  [0.50430429 0.90893129 0.4604724  0.85203252]\n",
      "  [0.58821769 0.96985258 0.74841328 0.67831841]\n",
      "  [0.41626476 0.86147212 0.25898141 0.19135517]]\n",
      "\n",
      " [[0.54335852 0.55265019 0.4550347  0.37177009]\n",
      "  [0.29068071 0.78144589 0.6132552  0.7418571 ]\n",
      "  [0.98443624 0.08690099 0.45065192 0.19119597]\n",
      "  [0.79770841 0.38676358 0.25776844 0.95535852]\n",
      "  [0.00147215 0.71672887 0.09193779 0.77449495]]\n",
      "\n",
      " [[0.82814193 0.11751358 0.24641704 0.59731966]\n",
      "  [0.096947   0.0185466  0.78682937 0.99672835]\n",
      "  [0.21601054 0.43100516 0.31676866 0.08479157]\n",
      "  [0.42819069 0.4668375  0.11155208 0.80679155]\n",
      "  [0.95190103 0.15063249 0.27865218 0.48235791]]\n",
      "\n",
      " [[0.36374544 0.08178104 0.17262471 0.94512532]\n",
      "  [0.52080171 0.5106075  0.77474579 0.78775757]\n",
      "  [0.9324107  0.08765177 0.32945255 0.18679844]\n",
      "  [0.45697934 0.80767618 0.89706457 0.70360105]\n",
      "  [0.09651023 0.82878715 0.86353742 0.31552271]]\n",
      "\n",
      " [[0.57454536 0.28894263 0.06735851 0.00925504]\n",
      "  [0.77354946 0.05852452 0.40140418 0.36551605]\n",
      "  [0.08200863 0.67707059 0.60661762 0.26319867]\n",
      "  [0.07922794 0.9166777  0.60859703 0.08730753]\n",
      "  [0.13702144 0.27776056 0.16390337 0.19656279]]\n",
      "\n",
      " [[0.17326167 0.26024632 0.34214607 0.43325377]\n",
      "  [0.45926087 0.1399537  0.70673286 0.2639766 ]\n",
      "  [0.10747443 0.38630992 0.71616994 0.63876562]\n",
      "  [0.15654276 0.98383512 0.1594421  0.4318127 ]\n",
      "  [0.84220878 0.08405455 0.65213238 0.28653122]]]\n",
      "[[0.4000267  0.4330345  0.56060555 0.01119563]\n",
      " [0.5662094  0.51361889 0.37568638 0.51768042]\n",
      " [0.49123292 0.09709946 0.20940193 0.16478889]\n",
      " [0.86858874 0.02274526 0.10901219 0.99257578]\n",
      " [0.62508923 0.19131569 0.02179536 0.68560159]\n",
      " [0.33553209 0.23637707 0.48126573 0.19635387]\n",
      " [0.96921693 0.37886509 0.4914721  0.41665512]\n",
      " [0.44168723 0.16478973 0.37675395 0.89226276]\n",
      " [0.5489618  0.69674422 0.3512908  0.77429881]\n",
      " [0.98456346 0.58185276 0.7818165  0.22684981]]\n",
      "[[[0.4000267  0.4330345  0.56060555 0.01119563]\n",
      "  [0.4000267  0.4330345  0.56060555 0.01119563]\n",
      "  [0.4000267  0.4330345  0.56060555 0.01119563]\n",
      "  [0.4000267  0.4330345  0.56060555 0.01119563]\n",
      "  [0.4000267  0.4330345  0.56060555 0.01119563]]\n",
      "\n",
      " [[0.5662094  0.51361889 0.37568638 0.51768042]\n",
      "  [0.5662094  0.51361889 0.37568638 0.51768042]\n",
      "  [0.5662094  0.51361889 0.37568638 0.51768042]\n",
      "  [0.5662094  0.51361889 0.37568638 0.51768042]\n",
      "  [0.5662094  0.51361889 0.37568638 0.51768042]]\n",
      "\n",
      " [[0.49123292 0.09709946 0.20940193 0.16478889]\n",
      "  [0.49123292 0.09709946 0.20940193 0.16478889]\n",
      "  [0.49123292 0.09709946 0.20940193 0.16478889]\n",
      "  [0.49123292 0.09709946 0.20940193 0.16478889]\n",
      "  [0.49123292 0.09709946 0.20940193 0.16478889]]\n",
      "\n",
      " [[0.86858874 0.02274526 0.10901219 0.99257578]\n",
      "  [0.86858874 0.02274526 0.10901219 0.99257578]\n",
      "  [0.86858874 0.02274526 0.10901219 0.99257578]\n",
      "  [0.86858874 0.02274526 0.10901219 0.99257578]\n",
      "  [0.86858874 0.02274526 0.10901219 0.99257578]]\n",
      "\n",
      " [[0.62508923 0.19131569 0.02179536 0.68560159]\n",
      "  [0.62508923 0.19131569 0.02179536 0.68560159]\n",
      "  [0.62508923 0.19131569 0.02179536 0.68560159]\n",
      "  [0.62508923 0.19131569 0.02179536 0.68560159]\n",
      "  [0.62508923 0.19131569 0.02179536 0.68560159]]\n",
      "\n",
      " [[0.33553209 0.23637707 0.48126573 0.19635387]\n",
      "  [0.33553209 0.23637707 0.48126573 0.19635387]\n",
      "  [0.33553209 0.23637707 0.48126573 0.19635387]\n",
      "  [0.33553209 0.23637707 0.48126573 0.19635387]\n",
      "  [0.33553209 0.23637707 0.48126573 0.19635387]]\n",
      "\n",
      " [[0.96921693 0.37886509 0.4914721  0.41665512]\n",
      "  [0.96921693 0.37886509 0.4914721  0.41665512]\n",
      "  [0.96921693 0.37886509 0.4914721  0.41665512]\n",
      "  [0.96921693 0.37886509 0.4914721  0.41665512]\n",
      "  [0.96921693 0.37886509 0.4914721  0.41665512]]\n",
      "\n",
      " [[0.44168723 0.16478973 0.37675395 0.89226276]\n",
      "  [0.44168723 0.16478973 0.37675395 0.89226276]\n",
      "  [0.44168723 0.16478973 0.37675395 0.89226276]\n",
      "  [0.44168723 0.16478973 0.37675395 0.89226276]\n",
      "  [0.44168723 0.16478973 0.37675395 0.89226276]]\n",
      "\n",
      " [[0.5489618  0.69674422 0.3512908  0.77429881]\n",
      "  [0.5489618  0.69674422 0.3512908  0.77429881]\n",
      "  [0.5489618  0.69674422 0.3512908  0.77429881]\n",
      "  [0.5489618  0.69674422 0.3512908  0.77429881]\n",
      "  [0.5489618  0.69674422 0.3512908  0.77429881]]\n",
      "\n",
      " [[0.98456346 0.58185276 0.7818165  0.22684981]\n",
      "  [0.98456346 0.58185276 0.7818165  0.22684981]\n",
      "  [0.98456346 0.58185276 0.7818165  0.22684981]\n",
      "  [0.98456346 0.58185276 0.7818165  0.22684981]\n",
      "  [0.98456346 0.58185276 0.7818165  0.22684981]]]\n",
      "[[[3.10802536e-01 1.63066304e-01 2.70047173e-01 8.47835482e-03]\n",
      "  [1.90096679e-01 2.45140344e-01 4.93132847e-01 2.49646043e-03]\n",
      "  [2.37458502e-01 1.33108173e-01 4.98858605e-01 6.54908386e-03]\n",
      "  [2.47097941e-01 3.00637838e-01 2.87270692e-01 6.45512129e-03]\n",
      "  [1.11775287e-01 2.09100742e-01 4.14289597e-01 1.02537741e-02]]\n",
      "\n",
      " [[4.48263346e-01 1.59633147e-01 1.93792434e-01 5.54317838e-02]\n",
      "  [1.68519805e-01 1.28488269e-01 2.66418125e-02 3.61070398e-01]\n",
      "  [3.47139295e-01 2.38487269e-01 2.26890905e-01 2.05395217e-01]\n",
      "  [4.00241254e-01 3.65046178e-01 8.98101119e-02 6.87581748e-02]\n",
      "  [4.03794693e-01 2.99132684e-01 2.62612342e-01 7.52714833e-02]]\n",
      "\n",
      " [[1.96830172e-01 2.28341353e-02 1.33009363e-01 9.20379636e-02]\n",
      "  [6.86525744e-03 8.40387458e-02 1.21279696e-01 1.58994599e-01]\n",
      "  [2.28904157e-01 6.93322335e-02 7.39688858e-02 1.36232653e-01]\n",
      "  [3.96412972e-02 9.49845682e-02 1.19968376e-01 9.63396696e-02]\n",
      "  [3.70929603e-01 5.82462800e-04 3.69324869e-02 1.17461963e-01]]\n",
      "\n",
      " [[5.35711657e-01 1.60773624e-02 1.00780712e-01 4.00243042e-01]\n",
      "  [4.20686431e-01 7.12392056e-03 4.29476247e-02 3.05777701e-01]\n",
      "  [2.57448752e-01 1.50280247e-02 3.26277912e-03 2.93311325e-01]\n",
      "  [6.75159430e-01 2.02264575e-02 4.63138076e-02 9.81404622e-01]\n",
      "  [5.78015177e-01 1.20627544e-02 9.26914534e-02 6.55739815e-01]]\n",
      "\n",
      " [[5.33064783e-01 1.31771260e-01 1.74004483e-02 4.32008175e-01]\n",
      "  [4.86911458e-01 1.29127008e-01 2.15077272e-03 3.03899229e-02]\n",
      "  [3.15235180e-01 1.73892817e-01 1.00361634e-02 5.84154852e-01]\n",
      "  [3.67688540e-01 1.85548017e-01 1.63119397e-02 4.65056179e-01]\n",
      "  [2.60202615e-01 1.64813134e-01 5.64459401e-03 1.31193411e-01]]\n",
      "\n",
      " [[1.82314217e-01 1.30633835e-01 2.18992608e-01 7.29984971e-02]\n",
      "  [9.75327037e-02 1.84715893e-01 2.95138712e-01 1.45666514e-01]\n",
      "  [3.30309945e-01 2.05414019e-02 2.16883324e-01 3.75420687e-02]\n",
      "  [2.67656765e-01 9.14220440e-02 1.24055115e-01 1.87588345e-01]\n",
      "  [4.93953581e-04 1.69418274e-01 4.42465068e-02 1.52075083e-01]]\n",
      "\n",
      " [[8.02649174e-01 4.45217929e-02 1.21107102e-01 2.48876296e-01]\n",
      "  [9.39626775e-02 7.02666089e-03 3.86704684e-01 4.15291975e-01]\n",
      "  [2.09361069e-01 1.63292807e-01 1.55682958e-01 3.53288427e-02]\n",
      "  [4.15009661e-01 1.76868433e-01 5.48247372e-02 3.36153832e-01]\n",
      "  [9.22598590e-01 5.70693923e-02 1.36949773e-01 2.00976895e-01]]\n",
      "\n",
      " [[1.60661715e-01 1.34766750e-02 6.50370402e-02 8.43300124e-01]\n",
      "  [2.30031464e-01 8.41428703e-02 2.91888535e-01 7.02886746e-01]\n",
      "  [4.11833896e-01 1.44441121e-02 1.24122548e-01 1.66673290e-01]\n",
      "  [2.01841938e-01 1.33096737e-01 3.37972620e-01 6.27797011e-01]\n",
      "  [4.26273370e-02 1.36575610e-01 3.25341133e-01 2.81529168e-01]]\n",
      "\n",
      " [[3.15403458e-01 2.01319104e-01 2.36624253e-02 7.16616439e-03]\n",
      "  [4.24649106e-01 4.07766222e-02 1.41009594e-01 2.83018644e-01]\n",
      "  [4.50196056e-02 4.71745015e-01 2.13099185e-01 2.03794414e-01]\n",
      "  [4.34931133e-02 6.38689889e-01 2.13794533e-01 6.76021176e-02]\n",
      "  [7.52195377e-02 1.93528064e-01 5.75777464e-02 1.52198335e-01]]\n",
      "\n",
      " [[1.70587112e-01 1.51425038e-01 2.67495443e-01 9.82835332e-02]\n",
      "  [4.52171469e-01 8.14324476e-02 5.52535415e-01 5.98830397e-02]\n",
      "  [1.05815392e-01 2.24775492e-01 5.59913477e-01 1.44903856e-01]\n",
      "  [1.54126282e-01 5.72447183e-01 1.24654467e-01 9.79566269e-02]\n",
      "  [8.29207993e-01 4.89073744e-02 5.09847859e-01 6.49995520e-02]]]\n",
      "[[0.75239437 0.93086633 0.87597436 0.84146159 0.7454194 ]\n",
      " [0.85712071 0.68472029 1.01791269 0.92385572 1.0408112 ]\n",
      " [0.44471163 0.3711783  0.50843793 0.35093391 0.52590652]\n",
      " [1.05281277 0.77653568 0.56905088 1.72310432 1.3385092 ]\n",
      " [1.11424467 0.64857916 1.08331901 1.03460467 0.56185375]\n",
      " [0.60493916 0.72305382 0.60527674 0.67072227 0.36623382]\n",
      " [1.21715436 0.902986   0.56366568 0.98285666 1.31759465]\n",
      " [1.08247555 1.30894962 0.71707385 1.30070831 0.78607325]\n",
      " [0.54755115 0.88945397 0.93365822 0.96357965 0.47852368]\n",
      " [0.68779113 1.14602237 1.03540822 0.94918456 1.45296278]]\n",
      "[[0.18473589 0.22083132 0.20903614 0.2019448  0.18345185]\n",
      " [0.18915253 0.15919873 0.22214842 0.20220635 0.22729397]\n",
      " [0.20040188 0.18619442 0.21358845 0.18246293 0.21735232]\n",
      " [0.17666102 0.13401553 0.10890445 0.34533823 0.23508078]\n",
      " [0.2441236  0.15324059 0.23668946 0.22543563 0.14051071]\n",
      " [0.20074868 0.22591717 0.20081646 0.2143986  0.15811908]\n",
      " [0.24111447 0.17610906 0.12543443 0.19075199 0.26659005]\n",
      " [0.20261566 0.2541145  0.14059862 0.25202886 0.15064236]\n",
      " [0.15802542 0.22244029 0.23249367 0.23955533 0.14748529]\n",
      " [0.13431713 0.21239228 0.19015143 0.17444284 0.28869632]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..') # 为了导入父目录的common模块\n",
    "from common.layers import Softmax\n",
    "import numpy as np\n",
    "\n",
    "N, T, H = 10, 5, 4 # N: 批量大小, T: 时间步数, H: 隐藏状态向量的元素个数\n",
    "hs = np.random.rand(N, T, H) # 隐藏状态向量\n",
    "print(hs) # (10, 5, 4)\n",
    "\n",
    "h = np.random.rand(N, H) # 查询向量\n",
    "print(h) # (10, 4)\n",
    "\n",
    "hr = h.reshape(N, 1, H).repeat(T, axis=1) # repeat表示沿着指定轴复制元素，这里按照列方向复制T次\n",
    "# hr = h.reshape(N, 1, H) # 也可以不使用repeat，利用numpy的广播机制\n",
    "print(hr) # (10, 1, 4)\n",
    "\n",
    "t = hs * hr # 广播机制，对应元素相乘\n",
    "print(t) # (10, 5, 4)\n",
    "\n",
    "s = np.sum(t, axis=2) # 按列求和\n",
    "print(s) # (10, 5)\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s) # 注意力权重\n",
    "print(a) # (10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ae092",
   "metadata": {},
   "source": [
    "以上就是进行批处理的代码。如前所述，此处我们通过 `reshape()` 和 `repeat()` 方法生成形状合适的 `hr`。在使用 NumPy 的广播的情况下，不需要 `repeat()`。此时的计算图如下图所示。\n",
    "\n",
    "<img src=\"./fig/word_weight_image.png\" alt=\"word_weight_image\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，这里的计算图由 Repeat 节点、表示对应元素的乘积的 “$\\times$” 节点、Sum 节点和 Softmax 层构成。我们将这个计算图表示的处理实现为 `AttentionWeight` 类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb139cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *  # import numpy as np\n",
    "from common.layers import Softmax\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape # N: 批量大小, T: 时间步数, H: 隐藏状态向量的元素个数\n",
    "\n",
    "        hr = h.reshape(N, 1, H).repeat(T, axis=1) # repeat表示沿着指定轴复制元素，这里按照列方向复制T次\n",
    "        t = hs * hr # 广播机制，对应元素相乘\n",
    "        s = np.sum(t, axis=2) # 按列求和\n",
    "        a = self.softmax.forward(s) # 注意力权重\n",
    "\n",
    "        self.cache = (hs, hr) # 反向传播时使用\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache # 从前向传播中取得缓存数据\n",
    "        N, T, H = hs.shape # N: 批量大小, T: 时间步数, H: 隐藏状态向量的元素个数\n",
    "\n",
    "        ds = self.softmax.backward(da) # 形状为(N, T)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2) # 形状为(N, T, H)\n",
    "        dhs = dt * hr # 广播机制，对应元素相乘\n",
    "        dhr = dt * hs # 广播机制，对应元素相乘\n",
    "        dh = np.sum(dhr, axis=1) # 按行求和，形状为(N, H)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333eddd9",
   "metadata": {},
   "source": [
    "类似于之前的 Weight Sum 层，这个实现有 Repeat 和 Sum 运算。只要注意到这两个运算的反向传播，其他应该就没有特别难的地方。下面，我们进行解码器的最后一个改进。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fabbefc",
   "metadata": {},
   "source": [
    "## 解码器的改进3\n",
    "在此之前，我们分两节介绍了解码器的改进方案。之前的小节分别实现了 Weight Sum 层和 Attention Weight 层。现在，我们将这两层组合起来，结果如下图所示。\n",
    "\n",
    "<img src=\"./fig/context_vector.png\" alt=\"context_vector\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "上图显示了用于获取上下文向量 $\\boldsymbol{c}$ 的计算图的全貌。我们已经分为 Weight Sum 层和 Attention Weight 层进行了实现。重申一下，这里进行的计算是：Attention Weight 层关注编码器输出的各个单词向量 $\\boldsymbol{h}\\boldsymbol{s}$，并计算各个单词的权重 $\\boldsymbol{a}$；然后，Weight Sum 层计算 $\\boldsymbol{a}$ 和 $\\boldsymbol{h}\\boldsymbol{s}$ 的加权和，并输出上下文向量 $\\boldsymbol{c}$。我们将进行这一系列计算的层称为 Attention 层。\n",
    "\n",
    "<img src=\"./fig/attention_layer.png\" alt=\"attention_layer\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "以上就是 Attention 技术的核心内容。关注编码器传递的信息 $\\boldsymbol{h}\\boldsymbol{s}$ 中的重要元素，基于它算出上下文向量，再传递给上一层（这里，Affine 层在上一层等待）。下面给出 Attention 层的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4bd80fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight() # 注意力权重层\n",
    "        self.weight_sum_layer = WeightSum() # 加权和层\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h) # 注意力权重\n",
    "        out = self.weight_sum_layer.forward(hs, a) # 上下文向量\n",
    "        self.attention_weight = a # 保存注意力权重，供外部使用\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout) # 反向传播到加权和层\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da) # 反向传播到注意力权重层\n",
    "        dhs = dhs0 + dhs1 # 加权和层和注意力权重层的梯度相加\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f9a64f",
   "metadata": {},
   "source": [
    "以上是 Weight Sum 层和 Attention Weight 层的正向传播和反向传播。为了以后可以访问各个单词的权重，这里设定成员变量 `attention_weight`，如此就完成了 Attention 层的实现。我们将这个 Attention 层放在 LSTM 层和 Affine 层的中间，如图所示。\n",
    "\n",
    "<img src=\"./fig/attention.png\" alt=\"attention\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，编码器的输出 $\\boldsymbol{h}\\boldsymbol{s}$ 被输入到各个时刻的 Attention 层。另外，这里将 LSTM 层的隐藏状态向量输入 Affine 层。根据上一章的解码器的改进，可以说这个扩展非常自然。如下图所示，我们将 Attention 信息 “添加” 到了上一章的解码器上。\n",
    "\n",
    "<img src=\"./fig/attention_LSTM.png\" alt=\"attention_LSTM\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，我们向上一章的解码器 “添加” 基于 Attention 层的上下文向量信息。因此，除了将原先的 LSTM 层的隐藏状态向量传给 Affine 层之外，追加输入 Attention 层的上下文向量。\n",
    "\n",
    "在图中，上下文向量和隐藏状态向量这两个向量被输入 Affine 层。如前所述，这意味着将这两个向量拼接起来，将拼接后的向量输入 Affine 层。\n",
    "\n",
    "最后，我们将时序方向上扩展的多个 Attention 层整体实现为 Time Attention 层，如下图所示。\n",
    "\n",
    "<img src=\"./fig/time_attention.png\" alt=\"time_attention\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "由图可知，Time Attention 层只是组合了多个 Attention 层，其实现如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce7530a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape # N: 批量大小, T: 时间步数, H: 隐藏状态向量的元素个数\n",
    "        out = np.empty_like(hs_dec) # 输出的形状与 hs_dec 相同\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention() # 每个时间步一个 Attention 层\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:]) # 计算 t 时刻的输出\n",
    "            self.layers.append(layer) # 保存层，供反向传播使用\n",
    "            self.attention_weights.append(layer.attention_weight) # 保存注意力权重\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape # N: 批量大小, T: 时间步数, H: 隐藏状态向量的元素个数\n",
    "        dhs_enc = 0 # 编码器隐藏状态的梯度\n",
    "        dhs_dec = np.empty_like(dout) # 解码器隐藏状态的梯度\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27036e3",
   "metadata": {},
   "source": [
    "这里仅创建必要数量的 Attention 层（代码中为 T 个），各自进行正向传播和反向传播。另外，`attention_weights` 列表中保存了各个 Attention 层对各个单词的权重。\n",
    "\n",
    "以上，我们介绍了 Attention 的结构及其实现。下面我们使用 Attention 来实现 seq2seq，并尝试挑战一个真实问题，以确认 Attention 的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47ed47",
   "metadata": {},
   "source": [
    "## 带 Attention 的 seq2seq 的实现\n",
    "上一节实现了 Attention 层（以及 Time Attention 层），现在我们使用这个层来实现 “带 Attention 的 seq2seq”。和上一章实现了 3 个类（`Encoder`、`Decoder` 和 `seq2seq`）一样，这里我们也分别实现 3 个类（`AttentionEncoder`、`AttentionDecoder` 和 `AttentionSeq2seq`）。\n",
    "\n",
    "## 编码器的实现\n",
    "首先实现 `AttentionEncoder` 类。这个类和上一章实现的 `Encoder` 类几乎一样，唯一的区别是，`Encoder` 类的 `forward()` 方法仅返回 LSTM 层的最后的隐藏状态向量，而 `AttentionEncoder` 类则返回所有的隐藏状态向量。因此，这里我们继承上一章的 `Encoder` 类进行实现。`AttentionEncoder` 类的实现如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "545ed801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') # 为了导入父目录的common模块\n",
    "from common.time_layers import *\n",
    "from seq2seq import Encoder, Seq2seq\n",
    "from attention_layer import TimeAttention\n",
    "\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced1d33",
   "metadata": {},
   "source": [
    "## 解码器的实现\n",
    "接着实现使用了 Attention 层的解码器。使用了 Attention 的解码器的层结构如图所示。\n",
    "\n",
    "<img src=\"./fig/decoder_layer.png\" alt=\"decoder_layer\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "从图中可以看出，和上一章的实现一样，Softmax 层（更确切地说，是 Time Softmax with Loss 层）之前的层都作为解码器。另外，和上一章一样，除了正向传播 `forward()` 方法和反向传播 `backward()` 方法之外，还实现了生成新单词序列（字符序列）的 `generate()` 方法。这里仅给出 Attention Decoder 层的初始化方法和 `forward()` 方法的实现，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756c9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size # 词汇表大小, 词向量维度, 隐藏状态向量的元素个数\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f') # 词向量\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f') # LSTM的输入权重\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f') # LSTM的隐藏状态权重\n",
    "        lstm_b = np.zeros(4 * H).astype('f') # LSTM的偏置\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f') # 输出层的权重\n",
    "        affine_b = np.zeros(V).astype('f') # 输出层的偏置\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W) # 词嵌入层\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True) # LSTM层\n",
    "        self.attention = TimeAttention() # 注意力层\n",
    "        self.affine = TimeAffine(affine_W, affine_b) # 全连接层\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine] # 顺序存储各层\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs): # xs: 解码器输入，维度为(N, T), enc_hs: 编码器隐藏状态，维度为(N, T_enc, H)\n",
    "        h = enc_hs[:,-1] # 取编码器隐藏状态的最后一个时间步\n",
    "        self.lstm.set_state(h) # 设置LSTM的初始隐藏状态\n",
    "\n",
    "        out = self.embed.forward(xs) # 词嵌入\n",
    "        dec_hs = self.lstm.forward(out) # 解码器隐藏状态\n",
    "        c = self.attention.forward(enc_hs, dec_hs) # 上下文向量\n",
    "        out = np.concatenate((c, dec_hs), axis=2) # 连接上下文向量和解码器隐藏状态\n",
    "        score = self.affine.forward(out) # 计算得分\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore) # 反向传播到全连接层\n",
    "        N, T, H2 = dout.shape # N: 批量大小, T: 时间步数, H2: 2*隐藏状态向量的元素个数\n",
    "        H = H2 // 2 # 隐藏状态向量的元素个数\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:] # 分离上下文向量和解码器隐藏状态的梯度\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc) # 反向传播到注意力层\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1 # 解码器隐藏状态的梯度\n",
    "        dout = self.lstm.backward(ddec_hs) # 反向传播到LSTM层\n",
    "        dh = self.lstm.dh # LSTM的初始隐藏状态的梯度\n",
    "        denc_hs[:, -1] += dh # 编码器隐藏状态的最后一个时间步的梯度加上LSTM初始隐藏状态的梯度\n",
    "        self.embed.backward(dout) # 反向传播到词嵌入层\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id # 初始输入的单词ID\n",
    "        h = enc_hs[:, -1] # 取编码器隐藏状态的最后一个时间步\n",
    "        self.lstm.set_state(h) # 设置LSTM的初始隐藏状态\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1)) # 当前时间步的输入，维度为(1, 1)\n",
    "\n",
    "            out = self.embed.forward(x) # 词嵌入\n",
    "            dec_hs = self.lstm.forward(out) # 解码器隐藏状态\n",
    "            c = self.attention.forward(enc_hs, dec_hs) # 上下文向量\n",
    "            out = np.concatenate((c, dec_hs), axis=2) # 连接上下文向量和解码器隐藏状态\n",
    "            score = self.affine.forward(out) # 计算得分\n",
    "\n",
    "            sample_id = np.argmax(score.flatten()) # 得分最高的单词ID\n",
    "            sampled.append(sample_id) # 保存采样结果\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d72c3",
   "metadata": {},
   "source": [
    "这里的实现除使用了新的 Time Attention 层之外，和上一章的 `Decoder` 类没有什么太大的不同。需要注意的是，`forward()` 方法中拼接了 Time Attention 层的输出和 LSTM 层的输出。在上面的代码中，使用 `np.concatenate()` 方法进行拼接。\n",
    "\n",
    "这里省略对 `AttentionDecoder` 类的 `backward()` 和 `generate()` 方法的说明。最后，我们使用 `AttentionEncoder` 类和 `AttentionDecoder` 类来实现 `AttentionSeq2seq` 类。\n",
    "\n",
    "## seq2seq 的实现\n",
    "`AttentionSeq2seq` 类的实现也和上一章实现的 `seq2seq` 几乎一样。区别仅在于，编码器使用 `AttentionEncoder` 类，解码器使用 `AttentionDecoder` 类。因此，只要继承上一章的 `Seq2seq` 类，并改一下初始化方法，就可以实现 `AttentionSeq2seq` 类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0435f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') # 为了导入父目录的模块\n",
    "from seq2seq import Encoder, Seq2seq\n",
    "\n",
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size # 词汇表大小, 词向量维度, 隐藏状态向量的元素个数\n",
    "        self.encoder = AttentionEncoder(*args) # 编码器\n",
    "        self.decoder = AttentionDecoder(*args) # 解码器\n",
    "        self.softmax = TimeSoftmaxWithLoss() # 损失层\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params # 编码器和解码器的所有参数\n",
    "        self.grads = self.encoder.grads + self.decoder.grads # 编码器和解码器的所有梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c99dbf",
   "metadata": {},
   "source": [
    "## Attention 的评价\n",
    "下面，我们使用上一节实现的 `AttentionSeq2seq` 类来挑战一个实际问题。原本我们应该通过研究翻译问题来确认 Attention 的效果，可惜没能找到大小适中的翻译数据集。因此，我们转而通过研究 “日期格式转换” 问题（本质上属于人为创造的问题，数据量有限），来确认带 Attention 的 seq2seq 的效果。\n",
    "\n",
    "WMT 是一个有名的翻译数据集。这个数据集中提供了英语和法语（或者英语和德语）的平行学习数据。WMT 数据集在许多研究中都被作为基准使用，经常用于评价 seq2seq 的性能，不过它的数据量很大（超过 20 GB），使用起来不是很方便。\n",
    "\n",
    "## 日期格式转换问题\n",
    "这里我们要处理的是日期格式转换问题。这个任务旨在将使用英语的国家和地区所使用的各种各样的日期格式转换为标准格式。例如，将人写的 “september 27, 1994” 这样的日期数据转换为 “1994-09-27” 这样的标准格式，如下图所示。\n",
    "\n",
    "<img src=\"./fig/date_transform.png\" alt=\"date_transform\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "这里采用日期格式转换问题的原因有两个。首先，该问题并不像看上去那么简单。因为输入的日期数据存在各种各样的版本，所以转换规则也相应地复杂。如果尝试将这些转换规则全部写出来，那将非常费力。\n",
    "\n",
    "其次，该问题的输入（问句）和输出（回答）存在明显的对应关系。具体而言，存在年月日的对应关系。因此，我们可以确认 Attention 有没有正确地关注各自的对应元素。\n",
    "\n",
    "我们事先在 `dataset/date.txt` 中准备好了要处理的日期转换数据。如图所示，这个文本文件包含 50 000 个日期转换用的学习数据。\n",
    "\n",
    "<img src=\"./fig/date_learning.png\" alt=\"date_learning\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "为了对齐输入语句的长度，本书提供的日期数据集填充了空格，并将 “_”（下划线）设置为输入和输出的分隔符。另外，因为这个问题输出的字符数是恒定的，所以无须使用分隔符来指示输出的结束。\n",
    "\n",
    "如上一章所述，本书提供了一个可以轻松处理上述 seq2seq 用的学习数据的 Python 模块，这个模块位于 <code>dataset/sequence.py</code> 中。\n",
    "\n",
    "## 带 Attention 的 seq2seq 的学习\n",
    "下面，我们在日期转换用的数据集上进行 `AttentionSeq2seq` 的学习，学习用的代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6401629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n",
      "| epoch 1 |  iter 21 / 351 | time 6[s] | loss 3.09\n",
      "| epoch 1 |  iter 41 / 351 | time 13[s] | loss 1.90\n",
      "| epoch 1 |  iter 61 / 351 | time 20[s] | loss 1.72\n",
      "| epoch 1 |  iter 81 / 351 | time 26[s] | loss 1.46\n",
      "| epoch 1 |  iter 101 / 351 | time 33[s] | loss 1.19\n",
      "| epoch 1 |  iter 121 / 351 | time 39[s] | loss 1.14\n",
      "| epoch 1 |  iter 141 / 351 | time 45[s] | loss 1.09\n",
      "| epoch 1 |  iter 161 / 351 | time 52[s] | loss 1.06\n",
      "| epoch 1 |  iter 181 / 351 | time 58[s] | loss 1.04\n",
      "| epoch 1 |  iter 201 / 351 | time 65[s] | loss 1.03\n",
      "| epoch 1 |  iter 221 / 351 | time 73[s] | loss 1.02\n",
      "| epoch 1 |  iter 241 / 351 | time 81[s] | loss 1.02\n",
      "| epoch 1 |  iter 261 / 351 | time 88[s] | loss 1.01\n",
      "| epoch 1 |  iter 281 / 351 | time 96[s] | loss 1.00\n",
      "| epoch 1 |  iter 301 / 351 | time 104[s] | loss 1.00\n",
      "| epoch 1 |  iter 321 / 351 | time 111[s] | loss 1.00\n",
      "| epoch 1 |  iter 341 / 351 | time 119[s] | loss 1.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1978-08-11\n",
      "---\n",
      "验证集准确率 0.000%\n",
      "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.00\n",
      "| epoch 2 |  iter 21 / 351 | time 7[s] | loss 1.00\n",
      "| epoch 2 |  iter 41 / 351 | time 15[s] | loss 0.99\n",
      "| epoch 2 |  iter 61 / 351 | time 24[s] | loss 0.99\n",
      "| epoch 2 |  iter 81 / 351 | time 32[s] | loss 0.99\n",
      "| epoch 2 |  iter 101 / 351 | time 40[s] | loss 0.99\n",
      "| epoch 2 |  iter 121 / 351 | time 49[s] | loss 0.99\n",
      "| epoch 2 |  iter 141 / 351 | time 58[s] | loss 0.98\n",
      "| epoch 2 |  iter 161 / 351 | time 66[s] | loss 0.98\n",
      "| epoch 2 |  iter 181 / 351 | time 75[s] | loss 0.97\n",
      "| epoch 2 |  iter 201 / 351 | time 83[s] | loss 0.95\n",
      "| epoch 2 |  iter 221 / 351 | time 92[s] | loss 0.94\n",
      "| epoch 2 |  iter 241 / 351 | time 100[s] | loss 0.90\n",
      "| epoch 2 |  iter 261 / 351 | time 108[s] | loss 0.83\n",
      "| epoch 2 |  iter 281 / 351 | time 116[s] | loss 0.74\n",
      "| epoch 2 |  iter 301 / 351 | time 124[s] | loss 0.66\n",
      "| epoch 2 |  iter 321 / 351 | time 133[s] | loss 0.58\n",
      "| epoch 2 |  iter 341 / 351 | time 141[s] | loss 0.47\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2016-11-08\n",
      "---\n",
      "验证集准确率 51.180%\n",
      "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 0.35\n",
      "| epoch 3 |  iter 21 / 351 | time 8[s] | loss 0.30\n",
      "| epoch 3 |  iter 41 / 351 | time 17[s] | loss 0.21\n",
      "| epoch 3 |  iter 61 / 351 | time 25[s] | loss 0.14\n",
      "| epoch 3 |  iter 81 / 351 | time 33[s] | loss 0.09\n",
      "| epoch 3 |  iter 101 / 351 | time 41[s] | loss 0.07\n",
      "| epoch 3 |  iter 121 / 351 | time 50[s] | loss 0.05\n",
      "| epoch 3 |  iter 141 / 351 | time 58[s] | loss 0.04\n",
      "| epoch 3 |  iter 161 / 351 | time 66[s] | loss 0.03\n",
      "| epoch 3 |  iter 181 / 351 | time 74[s] | loss 0.03\n",
      "| epoch 3 |  iter 201 / 351 | time 82[s] | loss 0.02\n",
      "| epoch 3 |  iter 221 / 351 | time 91[s] | loss 0.02\n",
      "| epoch 3 |  iter 241 / 351 | time 99[s] | loss 0.02\n",
      "| epoch 3 |  iter 261 / 351 | time 107[s] | loss 0.01\n",
      "| epoch 3 |  iter 281 / 351 | time 116[s] | loss 0.01\n",
      "| epoch 3 |  iter 301 / 351 | time 124[s] | loss 0.01\n",
      "| epoch 3 |  iter 321 / 351 | time 132[s] | loss 0.01\n",
      "| epoch 3 |  iter 341 / 351 | time 140[s] | loss 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "验证集准确率 99.900%\n",
      "| epoch 4 |  iter 1 / 351 | time 0[s] | loss 0.01\n",
      "| epoch 4 |  iter 21 / 351 | time 8[s] | loss 0.01\n",
      "| epoch 4 |  iter 41 / 351 | time 15[s] | loss 0.01\n",
      "| epoch 4 |  iter 61 / 351 | time 23[s] | loss 0.01\n",
      "| epoch 4 |  iter 81 / 351 | time 30[s] | loss 0.01\n",
      "| epoch 4 |  iter 101 / 351 | time 38[s] | loss 0.01\n",
      "| epoch 4 |  iter 121 / 351 | time 45[s] | loss 0.00\n",
      "| epoch 4 |  iter 141 / 351 | time 53[s] | loss 0.01\n",
      "| epoch 4 |  iter 161 / 351 | time 60[s] | loss 0.00\n",
      "| epoch 4 |  iter 181 / 351 | time 67[s] | loss 0.00\n",
      "| epoch 4 |  iter 201 / 351 | time 75[s] | loss 0.00\n",
      "| epoch 4 |  iter 221 / 351 | time 83[s] | loss 0.00\n",
      "| epoch 4 |  iter 241 / 351 | time 90[s] | loss 0.00\n",
      "| epoch 4 |  iter 261 / 351 | time 98[s] | loss 0.00\n",
      "| epoch 4 |  iter 281 / 351 | time 105[s] | loss 0.00\n",
      "| epoch 4 |  iter 301 / 351 | time 112[s] | loss 0.00\n",
      "| epoch 4 |  iter 321 / 351 | time 119[s] | loss 0.00\n",
      "| epoch 4 |  iter 341 / 351 | time 127[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "验证集准确率 99.960%\n",
      "| epoch 5 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 5 |  iter 21 / 351 | time 7[s] | loss 0.00\n",
      "| epoch 5 |  iter 41 / 351 | time 15[s] | loss 0.00\n",
      "| epoch 5 |  iter 61 / 351 | time 22[s] | loss 0.00\n",
      "| epoch 5 |  iter 81 / 351 | time 30[s] | loss 0.00\n",
      "| epoch 5 |  iter 101 / 351 | time 38[s] | loss 0.00\n",
      "| epoch 5 |  iter 121 / 351 | time 45[s] | loss 0.00\n",
      "| epoch 5 |  iter 141 / 351 | time 53[s] | loss 0.00\n",
      "| epoch 5 |  iter 161 / 351 | time 60[s] | loss 0.00\n",
      "| epoch 5 |  iter 181 / 351 | time 67[s] | loss 0.00\n",
      "| epoch 5 |  iter 201 / 351 | time 75[s] | loss 0.00\n",
      "| epoch 5 |  iter 221 / 351 | time 82[s] | loss 0.00\n",
      "| epoch 5 |  iter 241 / 351 | time 89[s] | loss 0.00\n",
      "| epoch 5 |  iter 261 / 351 | time 97[s] | loss 0.00\n",
      "| epoch 5 |  iter 281 / 351 | time 104[s] | loss 0.00\n",
      "| epoch 5 |  iter 301 / 351 | time 111[s] | loss 0.00\n",
      "| epoch 5 |  iter 321 / 351 | time 118[s] | loss 0.00\n",
      "| epoch 5 |  iter 341 / 351 | time 126[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "验证集准确率 99.940%\n",
      "| epoch 6 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 6 |  iter 21 / 351 | time 8[s] | loss 0.00\n",
      "| epoch 6 |  iter 41 / 351 | time 16[s] | loss 0.00\n",
      "| epoch 6 |  iter 61 / 351 | time 24[s] | loss 0.00\n",
      "| epoch 6 |  iter 81 / 351 | time 32[s] | loss 0.00\n",
      "| epoch 6 |  iter 101 / 351 | time 41[s] | loss 0.00\n",
      "| epoch 6 |  iter 121 / 351 | time 49[s] | loss 0.00\n",
      "| epoch 6 |  iter 141 / 351 | time 57[s] | loss 0.00\n",
      "| epoch 6 |  iter 161 / 351 | time 66[s] | loss 0.00\n",
      "| epoch 6 |  iter 181 / 351 | time 73[s] | loss 0.00\n",
      "| epoch 6 |  iter 201 / 351 | time 82[s] | loss 0.00\n",
      "| epoch 6 |  iter 221 / 351 | time 89[s] | loss 0.00\n",
      "| epoch 6 |  iter 241 / 351 | time 97[s] | loss 0.00\n",
      "| epoch 6 |  iter 261 / 351 | time 105[s] | loss 0.00\n",
      "| epoch 6 |  iter 281 / 351 | time 114[s] | loss 0.00\n",
      "| epoch 6 |  iter 301 / 351 | time 122[s] | loss 0.00\n",
      "| epoch 6 |  iter 321 / 351 | time 129[s] | loss 0.00\n",
      "| epoch 6 |  iter 341 / 351 | time 137[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "验证集准确率 99.940%\n",
      "| epoch 7 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 7 |  iter 21 / 351 | time 8[s] | loss 0.00\n",
      "| epoch 7 |  iter 41 / 351 | time 16[s] | loss 0.00\n",
      "| epoch 7 |  iter 61 / 351 | time 24[s] | loss 0.00\n",
      "| epoch 7 |  iter 81 / 351 | time 33[s] | loss 0.00\n",
      "| epoch 7 |  iter 101 / 351 | time 41[s] | loss 0.00\n",
      "| epoch 7 |  iter 121 / 351 | time 49[s] | loss 0.00\n",
      "| epoch 7 |  iter 141 / 351 | time 57[s] | loss 0.00\n",
      "| epoch 7 |  iter 161 / 351 | time 65[s] | loss 0.00\n",
      "| epoch 7 |  iter 181 / 351 | time 72[s] | loss 0.00\n",
      "| epoch 7 |  iter 201 / 351 | time 80[s] | loss 0.00\n",
      "| epoch 7 |  iter 221 / 351 | time 88[s] | loss 0.00\n",
      "| epoch 7 |  iter 241 / 351 | time 96[s] | loss 0.00\n",
      "| epoch 7 |  iter 261 / 351 | time 104[s] | loss 0.00\n",
      "| epoch 7 |  iter 281 / 351 | time 112[s] | loss 0.00\n",
      "| epoch 7 |  iter 301 / 351 | time 120[s] | loss 0.00\n",
      "| epoch 7 |  iter 321 / 351 | time 129[s] | loss 0.01\n",
      "| epoch 7 |  iter 341 / 351 | time 137[s] | loss 0.03\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "验证集准确率 99.620%\n",
      "| epoch 8 |  iter 1 / 351 | time 0[s] | loss 0.01\n",
      "| epoch 8 |  iter 21 / 351 | time 8[s] | loss 0.00\n",
      "| epoch 8 |  iter 41 / 351 | time 15[s] | loss 0.00\n",
      "| epoch 8 |  iter 61 / 351 | time 23[s] | loss 0.00\n",
      "| epoch 8 |  iter 81 / 351 | time 30[s] | loss 0.00\n",
      "| epoch 8 |  iter 101 / 351 | time 37[s] | loss 0.00\n",
      "| epoch 8 |  iter 121 / 351 | time 44[s] | loss 0.00\n",
      "| epoch 8 |  iter 141 / 351 | time 52[s] | loss 0.00\n",
      "| epoch 8 |  iter 161 / 351 | time 59[s] | loss 0.00\n",
      "| epoch 8 |  iter 181 / 351 | time 67[s] | loss 0.00\n",
      "| epoch 8 |  iter 201 / 351 | time 74[s] | loss 0.00\n",
      "| epoch 8 |  iter 221 / 351 | time 81[s] | loss 0.00\n",
      "| epoch 8 |  iter 241 / 351 | time 88[s] | loss 0.00\n",
      "| epoch 8 |  iter 261 / 351 | time 96[s] | loss 0.00\n",
      "| epoch 8 |  iter 281 / 351 | time 103[s] | loss 0.00\n",
      "| epoch 8 |  iter 301 / 351 | time 110[s] | loss 0.00\n",
      "| epoch 8 |  iter 321 / 351 | time 118[s] | loss 0.00\n",
      "| epoch 8 |  iter 341 / 351 | time 125[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "验证集准确率 100.000%\n",
      "| epoch 9 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 9 |  iter 21 / 351 | time 8[s] | loss 0.00\n",
      "| epoch 9 |  iter 41 / 351 | time 16[s] | loss 0.00\n",
      "| epoch 9 |  iter 61 / 351 | time 24[s] | loss 0.00\n",
      "| epoch 9 |  iter 81 / 351 | time 33[s] | loss 0.00\n",
      "| epoch 9 |  iter 101 / 351 | time 42[s] | loss 0.00\n",
      "| epoch 9 |  iter 121 / 351 | time 50[s] | loss 0.00\n",
      "| epoch 9 |  iter 141 / 351 | time 58[s] | loss 0.00\n",
      "| epoch 9 |  iter 161 / 351 | time 66[s] | loss 0.00\n",
      "| epoch 9 |  iter 181 / 351 | time 74[s] | loss 0.00\n",
      "| epoch 9 |  iter 201 / 351 | time 83[s] | loss 0.00\n",
      "| epoch 9 |  iter 221 / 351 | time 91[s] | loss 0.00\n",
      "| epoch 9 |  iter 241 / 351 | time 99[s] | loss 0.00\n",
      "| epoch 9 |  iter 261 / 351 | time 107[s] | loss 0.00\n",
      "| epoch 9 |  iter 281 / 351 | time 115[s] | loss 0.00\n",
      "| epoch 9 |  iter 301 / 351 | time 123[s] | loss 0.00\n",
      "| epoch 9 |  iter 321 / 351 | time 137[s] | loss 0.00\n",
      "| epoch 9 |  iter 341 / 351 | time 149[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "验证集准确率 100.000%\n",
      "| epoch 10 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 10 |  iter 21 / 351 | time 14[s] | loss 0.00\n",
      "| epoch 10 |  iter 41 / 351 | time 27[s] | loss 0.00\n",
      "| epoch 10 |  iter 61 / 351 | time 42[s] | loss 0.00\n",
      "| epoch 10 |  iter 81 / 351 | time 56[s] | loss 0.00\n",
      "| epoch 10 |  iter 101 / 351 | time 71[s] | loss 0.00\n",
      "| epoch 10 |  iter 121 / 351 | time 87[s] | loss 0.00\n",
      "| epoch 10 |  iter 141 / 351 | time 104[s] | loss 0.00\n",
      "| epoch 10 |  iter 161 / 351 | time 119[s] | loss 0.00\n",
      "| epoch 10 |  iter 181 / 351 | time 128[s] | loss 0.00\n",
      "| epoch 10 |  iter 201 / 351 | time 135[s] | loss 0.00\n",
      "| epoch 10 |  iter 221 / 351 | time 143[s] | loss 0.00\n",
      "| epoch 10 |  iter 241 / 351 | time 151[s] | loss 0.00\n",
      "| epoch 10 |  iter 261 / 351 | time 159[s] | loss 0.00\n",
      "| epoch 10 |  iter 281 / 351 | time 172[s] | loss 0.00\n",
      "| epoch 10 |  iter 301 / 351 | time 190[s] | loss 0.00\n",
      "| epoch 10 |  iter 321 / 351 | time 207[s] | loss 0.00\n",
      "| epoch 10 |  iter 341 / 351 | time 222[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "验证集准确率 100.000%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..') # 为了导入父目录的common模块\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "from seq2seq import Seq2seq\n",
    "from peeky_seq2seq import PeekySeq2seq\n",
    "import warnings\n",
    "warnings.simplefilter('ignore') # 忽略警告信息\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt') # x: 输入语句, t: 目标语句\n",
    "char_to_id, id_to_char = sequence.get_vocab() # 字符与ID的映射\n",
    "\n",
    "# 反转输入语句\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1] # 反转输入语句有助于提升性能\n",
    "\n",
    "# 设定超参数\n",
    "vocab_size = len(char_to_id) # 词汇表大小\n",
    "wordvec_size = 16 # 词向量维度\n",
    "hidden_size = 256 # 隐藏状态向量的元素个数\n",
    "batch_size = 128 # 批量大小\n",
    "max_epoch = 10 # 最大训练轮数\n",
    "max_grad = 5.0 # 用于梯度裁剪\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam() # Adam优化器\n",
    "trainer = Trainer(model, optimizer) # 训练器\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad) # 训练1轮\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]] # 取出一个样本\n",
    "        verbose = i < 10 # 前10个样本打印推理结果\n",
    "        correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose, is_reverse=True) # 计算正确数量\n",
    "\n",
    "    acc = float(correct_num) / len(x_test) # 计算准确率\n",
    "    acc_list.append(acc)\n",
    "    print('验证集准确率 %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params() # 保存模型参数\n",
    "\n",
    "# 绘制图形\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('训练轮数')\n",
    "plt.ylabel('准确率')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd26aa9",
   "metadata": {},
   "source": [
    "这里显示的代码和上一章的加法问题的学习用代码几乎一样。区别在于，它读入日期数据作为学习数据，使用 `AttentionSeq2seq` 作为模型。另外，这里还使用了反转输入语句的技巧（Reverse）。之后，在学习的同时，每个 `epoch` 使用测试数据计算正确率。为了查看结果，我们将前 10 个问题的问句和回答输出到终端。\n",
    "\n",
    "现在我们运行一下上面的代码。随着学习的进行，结果如图所示。\n",
    "\n",
    "<img src=\"./fig/result.png\" alt=\"result\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，随着学习的深入，带 Attention 的 seq2seq 变聪明了。实际上，没过多久，它就对大多数问题给出了正确答案。此时，测试数据的正确率（代码中的 `acc_list`）如下图所示。\n",
    "\n",
    "<img src=\"./fig/correct_change.png\" alt=\"correct_change\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，从第 1 个 `epoch` 开始，正确率迅速上升，到第 2 个 `epoch` 时，几乎可以正确回答所有问题。这可以说是一个很好的结果。我们将这个结果与上一章的模型比较一下，如下图所示。\n",
    "\n",
    "<img src=\"./fig/compare.png\" alt=\"compare\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "从上图的结果可知，简单的 seq2seq（图中的 baseline）完全没法用。即使经过了 10 个 `epoch`，大多数问题还是不能回答正确。而使用了 “偷窥” 技术的 Peeky 给出了良好的结果，从第 3 个 `epoch` 开始，模型的正确率开始上升，在第 4 个 `epoch` 时，正确率达到了 100%。但是，就学习速度而言，Attention 稍微有些优势。\n",
    "\n",
    "在这次的实验中，就最终精度来看，Attention 和 Peeky 取得了差不多的结果。但是，随着时序数据变长、变复杂，除了学习速度之外，Attention 在精度上也会变得更有优势。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee8dda",
   "metadata": {},
   "source": [
    "## Attention 的可视化\n",
    "接下来，我们对 Attention 进行可视化。在进行时序转换时，实际观察 Attention 在注意哪个元素。因为在 Attention 层中，各个时刻的 Attention 权重均保存到了成员变量中，所以我们可以轻松地进行可视化。\n",
    "\n",
    "在我们的实现中，Time Attention 层中的成员变量 `attention_weights` 保存了各个时刻的 Attention 权重，据此可以将输入语句和输出语句的各个单词的对应关系绘制成一张二维地图。这里，我们针对学习好的 `AttentionSeq2seq`，对进行日期格式转换时的 Attention 权重进行可视化。此处，我们不给出代码，仅将结果显示在图上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36ced391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from dataset import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from attention_seq2seq import AttentionSeq2seq\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# Reverse input\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "model.load_params()\n",
    "\n",
    "_idx = 0\n",
    "def visualize(attention_map, row_labels, column_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    ax.patch.set_facecolor('black')\n",
    "    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_xticklabels(row_labels, minor=False)\n",
    "    ax.set_yticklabels(column_labels, minor=False)\n",
    "\n",
    "    global _idx\n",
    "    _idx += 1\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "np.random.seed(1984)\n",
    "for _ in range(5):\n",
    "    idx = [np.random.randint(0, len(x_test))]\n",
    "    x = x_test[idx]\n",
    "    t = t_test[idx]\n",
    "\n",
    "    model.forward(x, t)\n",
    "    d = model.decoder.attention.attention_weights\n",
    "    d = np.array(d)\n",
    "    attention_map = d.reshape(d.shape[0], d.shape[2])\n",
    "\n",
    "    # reverse for print\n",
    "    attention_map = attention_map[:,::-1]\n",
    "    x = x[:,::-1]\n",
    "\n",
    "    row_labels = [id_to_char[i] for i in x[0]]\n",
    "    column_labels = [id_to_char[i] for i in t[0]]\n",
    "    column_labels = column_labels[1:]\n",
    "\n",
    "    visualize(attention_map, row_labels, column_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95018d36",
   "metadata": {},
   "source": [
    "<img src=\"./fig/attention_model.png\" alt=\"attention_model\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "上图是 seq2seq 进行时序转换时的 Attention 权重的可视化结果。例如，我们可以看到，当 seq2seq 输出第 1 个 “1” 时，注意力集中在输入语句的 “1” 上。这里需要特别注意年月日的对应关系。仔细观察图中的结果，纵轴（输出）的 “1983” 和 “26” 恰好对应于横轴（输入）的 “1983” 和 “26”。另外，输入语句的 “AUGUST” 对应于表示月份的 “08”，这一点也很令人惊讶。这表明 seq2seq 从数据中学习到了 “August” 和 “8 月” 的对应关系。下图中给出了其他一些例子，从中也可以很清楚地看到年月日的对应关系。\n",
    "\n",
    "<img src=\"./fig/attention_weight_example.png\" alt=\"attention_weight_example\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "像这样，使用 Attention，seq2seq 能像我们人一样将注意力集中在必要的信息上。换言之，借助 Attention，我们理解了模型是如何工作的。\n",
    "\n",
    "我们没有办法理解神经网络内部进行了什么工作（基于何种逻辑工作），而 Attention 赋予了模型 “人类可以理解的结构和意义”。在上面的例子中，通过 Attention，我们看到了单词和单词之间的关联性。由此，我们可以判断模型的工作逻辑是否符合人类的逻辑。\n",
    "\n",
    "以上就是关于 Attention 的评价的内容。通过这里的实验，我们体验了 Attention 的奇妙效果。至此，Attention 的核心话题就要告一段落了，但是关于 Attention 的其他内容还有不少。下一节我们继续围绕 Attention，介绍它的几个高级技巧。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de3e3f",
   "metadata": {},
   "source": [
    "## 关于 Attention 的其他话题\n",
    "到目前为止，我们研究了 Attention（正确地说，是带 Attention 的 seq2seq），本节我们介绍几个之前未涉及的话题。\n",
    "\n",
    "## 双向 RNN\n",
    "这里我们关注 seq2seq 的编码器。首先复习一下，上一节之前的编码器如图所示。\n",
    "\n",
    "<img src=\"./fig/LSTM_hs.png\" alt=\"LSTM_hs\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，LSTM 中各个时刻的隐藏状态向量被整合为 $\\boldsymbol{h}\\boldsymbol{s}$。这里，编码器输出的 $\\boldsymbol{h}\\boldsymbol{s}$ 的各行中含有许多对应单词的成分。\n",
    "\n",
    "需要注意的是，我们是从左向右阅读句子的。因此，在图中，单词 “猫” 的对应向量编码了 “吾輩”“は”“猫” 这 3 个单词的信息。如果考虑整体的平衡性，我们希望向量能更均衡地包含单词 “猫” 周围的信息。\n",
    "\n",
    "在这次的翻译问题中，我们获得了所有的时序数据（需要翻译的文本）。因此，我们既可以从左向右读取文本，也可以从右向左读取文本。\n",
    "\n",
    "为此，可以让 LSTM 从两个方向进行处理，这就是名为 **双向 LSTM** 的技术，如下图所示。\n",
    "\n",
    "<img src=\"./fig/biLSTM.png\" alt=\"biLSTM\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，双向 LSTM 在之前的 LSTM 层上添加了一个反方向处理的 LSTM 层。然后，拼接各个时刻的两个 LSTM 层的隐藏状态，将其作为最后的隐藏状态向量（除了拼接之外，也可以 “求和” 或者 “取平均” 等）。\n",
    "\n",
    "通过这样的双向处理，各个单词对应的隐藏状态向量可以从左右两个方向聚集信息。这样一来，这些向量就编码了更均衡的信息。\n",
    "\n",
    "双向 LSTM 的实现非常简单。一种实现方式是准备两个 LSTM 层（本章中是 Time LSTM 层），并调整输入各个层的单词的排列。具体而言，其中一个层的输入语句与之前相同，这相当于从左向右处理输入语句的常规的 LSTM 层。而另一个 LSTM 层的输入语句则按照从右到左的顺序输入。如果原文是 “A B C D”，就改为 “D C B A”。通过输入改变了顺序的输入语句，另一个 LSTM 层从右向左处理输入语句。之后，只需要拼接这两个 LSTM 层的输出，就可以创建双向 LSTM 层。\n",
    "\n",
    "为了便于理解，本章使用了单向 LSTM 作为编码器。不过，显然也可以将此处介绍的双向 LSTM 用作编码器。感兴趣的读者可以尝试实现使用双向 LSTM 的带 Attention 的 seq2seq。另外，<code>common/time_layers.py</code> 的 <code>TimeBiLSTM</code> 类中有双向 LSTM 的实现，感兴趣的读者可以参考一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530e5034",
   "metadata": {},
   "source": [
    "## Attention 层的使用方法\n",
    "接下来，我们思考 Attention 层的使用方法。截止到目前，我们使用的 Attention 层的层结构如图所示。\n",
    "\n",
    "<img src=\"./fig/attention_seq2seq.png\" alt=\"attention_seq2seq\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，我们将 Attention 层插入了 LSTM 层和 Affine 层之间，不过使用 Attention 层的方式并不一定非得像上图那样。实际上，使用 Attention 的模型还有其他好几种方式。比如，文献中以下图的结构使用了 Attention。\n",
    "\n",
    "<img src=\"./fig/attention_example.png\" alt=\"attention_example\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在图中，Attention 层的输出（上下文向量）被连接到了下一时刻的 LSTM 层的输入处。通过这种结构，LSTM 层得以使用上下文向量的信息。相对地，我们实现的模型则是 Affine 层使用了上下文向量。\n",
    "\n",
    "那么，Attention 层的位置的不同对最终精度有何影响呢？答案要试一下才知道。实际上，这只能使用真实数据来验证。不过，在上面的两个模型中，上下文向量都得到了很好的应用。因此，在这两个模型之间，我们可能看不到太大的精度差异。\n",
    "\n",
    "从实现的角度来看，前者的结构（在 LSTM 层和 Affine 层之间插入 Attention 层）更加简单。这是因为在前者的结构中，解码器中的数据是从下往上单向流动的，所以 Attention 层的模块化会更加简单。实际上，我们轻松地将其模块化为了 Time Attention 层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9513ec0",
   "metadata": {},
   "source": [
    "## seq2seq 的深层化和 skip connection\n",
    "在诸如翻译这样的实际应用中，需要解决的问题更加复杂。在这种情况下，我们希望带 Attention 的 seq2seq 具有更强的表现力。此时，首先可以考虑到的是加深 RNN 层（LSTM 层）。通过加深层，可以创建表现力更强的模型，带 Attention 的 seq2seq 也是如此。那么，如果我们加深带 Attention 的 seq2seq，结果会怎样呢？下图给出了一个例子。\n",
    "\n",
    "<img src=\"./fig/3_attention_LSTM.png\" alt=\"3_attention_LSTM\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在上图的模型中，编码器和解码器使用了 3 层 LSTM 层。如本例所示，编码器和解码器中通常使用层数相同的 LSTM 层。另外，Attention 层的使用方法有许多变体。这里将解码器 LSTM 层的隐藏状态输入 Attention 层，然后将上下文向量（Attention 层的输出）传给解码器的多个层（LSTM 层和 Affine 层）。\n",
    "\n",
    "上图的模型只是一个例子。除了这个例子之外，还有很多方式，比如使用多个 Attention 层，或者将 Attention 的输出输入给下一个时刻的 LSTM 层等。另外，如上一章所述，在加深层时，避免泛化性能的下降非常重要。此时，Dropout、权重共享等技术可以发挥作用。\n",
    "\n",
    "另外，在加深层时使用到的另一个重要技巧是**残差连接**（skip connection，也称为 residual connection 或 shortcut）。如图所示，这是一种 “跨层连接” 的简单技巧。\n",
    "\n",
    "<img src=\"./fig/skip_connection.png\" alt=\"skip_connection\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，所谓残差连接，就是指 “跨层连接”。此时，在残差连接的连接处，有两个输出被相加。请注意这个加法（确切地说，是对应元素的加法）非常重要。因为加法在反向传播时 “按原样” 传播梯度，所以残差连接中的梯度可以不受任何影响地传播到前一个层。这样一来，即便加深了层，梯度也能正常传播，而不会发生梯度消失（或者梯度爆炸），学习可以顺利进行。\n",
    "\n",
    "在时间方向上，RNN 层的反向传播会出现梯度消失或梯度爆炸的问题。梯度消失可以通过 LSTM、GRU 等 Gated RNN 应对，梯度爆炸可以通过梯度裁剪应对。而对于深度方向上的梯度消失，这里介绍的残差连接很有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb3e57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "| 对比维度                | 传统多层LSTM网络（无残差）                                                                                     | 残差连接+多层LSTM网络（间隔1层跳连）                                                                                             |\n",
    "|-------------------------|-------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n",
    "| **网络结构示意图**      | <div style=\"text-align:center;\">输入序列 → LSTM层1 → LSTM层2 → ... → LSTM层N → 输出</div> <br>（注：层间无跨连，梯度需经所有层链式传递） | <div style=\"text-align:center;\">输入序列 → LSTM层1 → LSTM层2 → [LSTM层3 + 层1] → ... → [LSTM层N + 层N-2] → 输出</div> <br>（注：每间隔1层添加残差跳连，形成梯度分段传递通道） |\n",
    "| **核心计算逻辑**        | 第n层输出仅依赖前一层：<br> $ h_n = \\text{LSTM}(h_{n-1}, x_t) \\quad (\\forall n=1,2,...,N) $ <br>（$ h_0 $为初始隐藏状态） | 第n层输出 = 本层计算 + 间隔1层的浅层输出：<br> $ h_n = \\begin{cases} \\text{LSTM}(h_{n-1}, x_t) & (n=1,2) \\\\ \\text{LSTM}(h_{n-1}, x_t) + h_{n-2} & (n \\geq 3) \\end{cases} $ |\n",
    "| **梯度传递公式**        | 深度方向梯度（对$ h_1 $）：<br> $ \\frac{\\partial \\text{Loss}}{\\partial h_1} = \\frac{\\partial \\text{Loss}}{\\partial h_N} \\cdot \\prod_{k=2}^N \\frac{\\partial h_k}{\\partial h_{k-1}} $ <br>（说明：N-1个单层梯度连乘，层数N越大，衰减越显著，呈指数级下降） | 深度方向梯度（对$ h_1 $，通用N阶形式）：<br> $ \\frac{\\partial \\text{Loss}}{\\partial h_1} = \\frac{\\partial \\text{Loss}}{\\partial h_N} \\cdot \\left( \\prod_{\\substack{k=2 \\\\ k \\equiv 0 \\mod 2}}^N \\frac{\\partial h_k}{\\partial h_{k-1}} \\right) \\cdot \\left( \\left( \\prod_{\\substack{k=2 \\\\ k \\equiv 1 \\mod 2}}^N \\frac{\\partial h_k}{\\partial h_{k-1}} \\right) + 1 \\right) $ <br>（说明：<br>1. 按“偶数层/奇数层”拆分连乘项，$ \\prod_{\\substack{k=2 \\\\ k \\equiv 0 \\mod 2}}^N $表示偶数层梯度连乘，$ \\prod_{\\substack{k=2 \\\\ k \\equiv 1 \\mod 2}}^N $表示奇数层梯度连乘；<br>2. “+1”来自残差跳连的恒等映射，与奇数层连乘项组合，形成梯度保护机制，适用于任意N≥3） |\n",
    "| **时间维度梯度特性**    | LSTM门控机制确保时间方向（同一层不同时刻）梯度稳定，但深度方向（层间）梯度随N增大呈指数级衰减 | 同时优化两个维度：<br>1. 时间方向：保留LSTM门控的梯度稳定优势；<br>2. 深度方向：通过“分段连乘+残差+1”结构，使梯度随N增大无显著衰减，支持超深网络（N≥10）训练 |\n",
    "\n",
    "\n",
    "### 关键差异解析（LSTM场景下）\n",
    "1. **LSTM的“时间优势”与“深度劣势”**：  \n",
    "   LSTM通过门控机制确保时间方向梯度稳定（解决RNN的时间梯度消失），但堆叠多层时，深度方向的梯度仍需通过每层的非线性变换（tanh/sigmoid）传递，导致衰减。\n",
    "\n",
    "2. **残差连接的“补位作用”**：  \n",
    "   残差跳连为深度方向的梯度提供“直达通道”，其梯度公式中的“+1”项打破了传统网络对多层导数乘积的依赖，即使深层LSTM的梯度衰减，仍能通过残差路径将误差信号传递到浅层，确保所有LSTM层参数有效更新。\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12182538",
   "metadata": {},
   "source": [
    "## Attention 的应用\n",
    "到目前为止，我们仅将 Attention 应用在了 seq2seq 上，但是 Attention 这一想法本身是通用的，在应用上还有更多的可能性。实际上，在近些年的深度学习研究中，作为一种重要技巧，Attention 出现在了各种各样的场景中。本节我们将介绍 3 个使用了 Attention 的前沿研究，以使读者感受到 Attention 的重要性和可能性。\n",
    "\n",
    "## GNMT\n",
    "回看机器翻译的历史，我们可以发现主流方法随着时代的变迁而演变。具体来说，就是从 “基于规则的翻译” 到 “基于用例的翻译”，再到 “基于统计的翻译”。现在，**神经机器翻译**（Neural Machine Translation）取代了这些过往的技术，获得了广泛关注。\n",
    "\n",
    "神经机器翻译这个术语是出于与之前的基于统计的翻译进行对比而使用的，现在已经成为使用了 seq2seq 的机器翻译的统称。\n",
    "\n",
    "从 2016 年开始，谷歌翻译就开始将神经机器翻译用于实际的服务，其机器翻译系统称为 **GNMT**（Google Neural Machine Translation，谷歌神经机器翻译系统）。关于 GNMT 的技术细节，文献中有具体介绍。这里，我们以层结构为中心来看一下 GNMT 的架构，如图所示。\n",
    "\n",
    "<img src=\"./fig/GNMT.png\" alt=\"GNMT\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "GNMT 和本章实现的带 Attention 的 seq2seq 一样，由编码器、解码器和 Attention 构成。不过，与我们的简单模型不同，这里可以看到许多为了提高翻译精度而做的改进，比如 LSTM 层的多层化、双向 LSTM（仅编码器的第 1 层）和 skip connection 等。另外，为了提高学习速度，还进行了多个 GPU 上的分布式学习。\n",
    "\n",
    "除了上述在架构上下的功夫之外，GNMT 还进行了低频词处理、用于加速推理的量化（quantization）等工作。利用这些技巧，GNMT 获得了非常好的结果，实际报告出来的结果如图所示。\n",
    "\n",
    "<img src=\"./fig/GNMT_remark.png\" alt=\"GNMT_remark\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，与基于短语的机器翻译（基于统计的机器翻译的一种）这种传统方法相比，GNMT 成功地提高了翻译精度，其精度进一步接近了人工翻译的精度。像这样，GNMT 给出了出色的结果，充分展示了神经翻译的实用性和可能性。不过，但凡用过谷歌翻译的人都知道，它仍存在许多不自然的翻译以及人绝对不会犯的错误。机器翻译的研究仍在继续。实际上，GNMT 只是一个开始，目前围绕神经翻译的研究非常活跃。\n",
    "\n",
    "实现 GNMT 需要大量的数据和计算资源。根据文献 ，GNMT 使用了大量的训练数据，（1 个模型）在将近 100 个 GPU 上学习了 6 天。另外，GNMT 也在设法基于可以并行学习 8 个模型的集成学习和强化学习等技术进一步提高精度。虽然这些事情不是一个人可以完成的，但是我们已经学习了需要用到的技术的核心部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce441660",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "到目前为止，我们在各种地方使用了 RNN（LSTM）。从语言模型到文本生成，从 seq2seq 到带 Attention 的 seq2seq 及其组成部分，RNN 都会出现。使用 RNN 可以很好地处理可变长度的时序数据，（在大多数情况下）能够获得良好的结果。但是，RNN 也有缺点，比如并行处理的问题。\n",
    "\n",
    "RNN 需要基于上一个时刻的计算结果逐步进行计算，因此（基本）不可能在时间方向上并行计算 RNN。在使用了 GPU 的并行计算环境下进行深度学习时，这一点会成为很大的瓶颈，于是我们就有了避开 RNN 的动机。\n",
    "\n",
    "---\n",
    "\n",
    "### 核心原因：时间步的顺序依赖\n",
    "RNN（包括LSTM）的设计初衷是处理序列数据，其核心机制决定了计算必须按时间顺序进行。\n",
    "1. **隐藏状态的递推计算**：每个时间步t的隐藏状态$h_t$，必须依赖前一个时间步t-1的隐藏状态$h_{t-1}$和当前时间步的输入$x_t$。公式可简化为 $ h_t = f(h_{t-1}, x_t) $，其中f是激活函数（LSTM中为更复杂的门控计算）。\n",
    "2. **计算链条的不可拆分**：由于$h_t$的计算必须等待$h_{t-1}$完成，整个序列的计算形成了一条“链式依赖”。例如，要计算t=3的隐藏状态，必须先算完t=2，而t=2又依赖t=1，无法跳过前面的步骤直接并行计算后续时间步。\n",
    "\n",
    "### 对比：与CNN的并行性差异\n",
    "为了更清晰理解，可将RNN与天然支持并行的CNN对比，二者在处理数据时的依赖关系完全不同。\n",
    "\n",
    "| 对比维度 | RNN（LSTM） | CNN |\n",
    "| :--- | :--- | :--- |\n",
    "| **计算依赖** | 时间步间强依赖（t依赖t-1） | 无时间依赖，仅依赖局部空间邻域 |\n",
    "| **并行方向** | 时间方向无法并行，仅输入特征维度可并行 | 空间维度（如图片的宽、高）可完全并行 |\n",
    "| **核心原因** | 隐藏状态递推更新，需顺序执行 | 卷积核滑动时，各位置计算相互独立 |\n",
    "\n",
    "### 解决方案：用“非递推”架构突破并行限制\n",
    "为解决RNN的并行性问题，后续出现了以Transformer为代表的架构，其核心思路是**打破时间步依赖**。\n",
    "- Transformer通过“自注意力机制”（Self-Attention），让每个时间步的输入可以直接与所有其他时间步的输入建立关联，无需等待前序计算。\n",
    "- 这种“全局依赖”的设计，使得所有时间步的注意力计算和特征转换可以完全并行，大幅提升了训练效率，也成为当前大语言模型（LLM）的基础架构。\n",
    "\n",
    "---\n",
    "\n",
    "在这样的背景下，现在关于去除 RNN 的研究（可以并行计算的 RNN 的研究）很活跃，其中一个著名的模型是 Transformer 模型。Transformer 是在 “Attention is all you need” 这篇论文中提出来的方法。如论文标题所示，Transformer 不用 RNN，而用 Attention 进行处理。这里，我们简单地看一下这个 Transformer。\n",
    "\n",
    "除了 Transformer 之外，还有多个研究致力于去除 RNN，比如用卷积层（Convolution 层）代替 RNN 的研究。这里我们不去探讨该研究的细节，基本上就是用卷积层代替 RNN 来构成 seq2seq，并据此实现并行计算。\n",
    "\n",
    "Transformer 是基于 Attention 构成的，其中使用了 **Self-Attention** 技巧，这一点很重要。Self-Attention 直译为 “自己对自己的 Attention”，也就是说，这是以一个时序数据为对象的 Attention，**旨在观察一个时序数据中每个元素与其他元素的关系**。用 Time Attention 层来说明的话，Self-Attention 如下图所示。\n",
    "\n",
    "<img src=\"./fig/self_attention.png\" alt=\"self_attention\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在此之前，我们用 Attention 求解了翻译这种两个时序数据之间的对应关系。如左图所示，Time Attention 层的两个输入中输入的是不同的时序数据。与之相对，如右图所示，Self-Attention 的两个输入中输入的是同一个时序数据。像这样，可以求得一个时序数据内各个元素之间的对应关系。\n",
    "\n",
    "至此，对 Self-Attention 的说明就结束了，下面我们看一下 Transformer 的层结构，如下图所示。\n",
    "\n",
    "<img src=\"./fig/transformer.png\" alt=\"transformer\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "Transformer 中用 Attention 代替了 RNN。实际上，由图可知，编码器和解码器两者都使用了 Self-Attention。图中的 Feed Forward 层表示前馈神经网络（在时间方向上独立的网络）。具体而言，使用具有一个隐藏层、激活函数为 ReLU 的全连接的神经网络。另外，图中的 $\\boldsymbol{Nx}$ 表示灰色背景包围的元素被堆叠了 $N$ 次。\n",
    "\n",
    "---\n",
    "\n",
    "# 自注意力与普通注意力（编解码注意力）核心对比表格\n",
    "| 对比维度                | 自注意力（Self-Attention）                          | 普通注意力（以编解码注意力为例）              |\n",
    "|-------------------------|-----------------------------------------------------|-----------------------------------------------|\n",
    "| **依赖序列数量**        | 仅需1个序列（如单句、单段时间序列）                 | 需2个序列（如编码器输出序列+解码器输入序列）  |\n",
    "| **注意力计算对象**      | 序列内部任意两个元素（自身与自身元素交互）          | 两个序列间的元素（外部序列与当前序列元素交互）|\n",
    "| **核心目标**            | 梳理单个序列的内部逻辑，捕捉元素间依赖关系          | 匹配两个序列信息，实现跨序列信息对齐          |\n",
    "| **长距离依赖处理能力**  | 强，可直接计算序列首尾元素关联，无信息衰减          | 弱，依赖两个序列交互范围，易受距离限制        |\n",
    "| **计算并行性**          | 高，序列内所有元素注意力权重可同时生成              | 低，需按序列顺序逐步计算，依赖前序结果        |\n",
    "| **典型应用场景**        | 句子语义理解、文本分类、时间序列预测                | 机器翻译（对齐源语言与目标语言）、文本摘要    |\n",
    "| **信息来源**            | 仅依赖自身序列数据                                  | 依赖外部序列数据引导                          |\n",
    "\n",
    "---\n",
    "\n",
    "上图显示的是简化了的 Transformer。实际上，除了这个架构外，Skip Connection、Layer Normalization 等技巧也会被用到。其他常见的技巧还有，（并行）使用多个 Attention、编码时序数据的位置信息（Positional Encoding，位置编码）等。\n",
    "\n",
    "使用 Transformer 可以控制计算量，充分利用 GPU 并行计算带来的好处。其结果是，与 GNMT 相比，Transformer 的学习时间得以大幅减少。在翻译精度方面，如下图所示，也实现了精度提升。\n",
    "\n",
    "<img src=\"./fig/transformer_correct.png\" alt=\"transformer_correct\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "上图比较了 3 种方法。结果是，使用卷积层的 seq2seq（图中记为 ConvS2S）比 GNMT 精度高，而 Transformer 比使用卷积层的 seq2seq 还要高。如此，不仅仅是计算量，从精度的角度来看，Attention 也是很有前途的技术。\n",
    "\n",
    "我们之前组合使用了 Attention 和 RNN，但是由这个研究可知，Attention 其实可以用来替换 RNN。这样一来，利用 Attention 的机会可能会进一步增加。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab6a11",
   "metadata": {},
   "source": [
    "## NTM\n",
    "我们在解决复杂问题时，经常使用纸和笔。从另一个角度来看，这可以解释为基于纸和笔这样的 “外部存储装置”，我们的能力获得了延伸。同样地，利用外部存储装置，神经网络也可以获得额外的能力。本节我们讨论的主题就是 “基于外部存储装置的扩展”。\n",
    "\n",
    "RNN 和 LSTM 能够使用内部状态来存储时序数据，但是它们的内部状态长度固定，能塞入其中的信息量有限。因此，可以考虑在 RNN 的外部配置存储装置（内存），适当地记录必要信息。\n",
    "\n",
    "可见计算机的内存操作可以通过神经网络复现。我们可以立刻想到一个方法：在 RNN 的外部配置一个存储信息的存储装置，并使用 Attention 向这个存储装置读写必要的信息。实际上，这样的研究有好几个，**NTM**（Neural Turing Machine，神经图灵机） 就是其中比较有名的一个。\n",
    "\n",
    "NTM 是 DeepMind 团队进行的一项研究，后来被改进成名为 DNC（Differentiable Neural Computers，可微分神经计算机） 的方法。关于 DNC 的论文发表在了学术期刊《自然》上。DNC 可以认为是强化了内存操作的 NTM，但它们的核心技术是一样的。\n",
    "\n",
    "在解释 NTM 的内容之前，我们先来看一下 NTM 的整体框架。下图这张有趣的图片非常适合用于这一目的。这是 NTM 所进行的处理的概念表示，很好地总结了 NTM 的精髓（准确地说，这是发展了 NTM 的 DNC 的一篇解说文章中用到的图）。\n",
    "\n",
    "<img src=\"./fig/NTM.png\" alt=\"NTM\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "现在我们看一下上图。这里需要注意的是图中间的一个被称为 “控制器” 的模块。这是处理信息的模块，我们假定它使用神经网络（或者 RNN）。从图中可以看出，数据 “0” 和 “1” 一个接一个地流入这个控制器，控制器对其进行处理并输出新的数据。\n",
    "\n",
    "这里重要的是，在这个控制器的外侧有一张 “大纸”（内存）。基于这个内存，控制器获得了计算机（图灵机）的能力。具体来说，这个能力是指，在这张 “大纸” 上写入必要的信息、擦除不必要的信息，以及读取必要信息的能力。顺便说一下，因为上图的 “大纸” 是卷式的，所以各个节点可以在需要的地方读写数据。换句话说，就是可以移动到目标地点。\n",
    "\n",
    "像这样，NTM 在读写外部存储装置的同时处理时序数据。NTM 的有趣之处在于使用 “可微分” 的计算构建了这些内存操作。因此，它可以从数据中学习内存操作的顺序。\n",
    "\n",
    "计算机根据人编写的程序进行动作。与此相对，NTM 从数据中学习程序。也就是说，这意味着它可以从 “算法的输入和输出” 中学习 “算法自身”（逻辑）。\n",
    "\n",
    "NTM 像计算机一样读写外部存储装置，其层结构可以简单地绘制成下图。\n",
    "\n",
    "<img src=\"./fig/NTM_layer.png\" alt=\"NTM_layer\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "上图是简化版的 NTM 的层结构。这里 LSTM 层是控制器，执行 NTM 的主要处理。Write Head 层接收 LSTM 层各个时刻的隐藏状态，将必要的信息写入内存。Read Head 层从内存中读取重要信息，并传递给下一个时刻的 LSTM 层。\n",
    "\n",
    "那么，上图的 Write Head 层和 Read Head 层如何进行内存操作呢？当然是使用 Attention。\n",
    "\n",
    "重申一下，在读取（或者写入）内存中某个地址上的数据时，我们需要先 “选择” 数据。这个选择操作自身是不能微分的，因此先使用 Attention 选择所有地址上的数据，再利用权重表示每个数据的贡献，这样就能够利用可微分的计算替代选择这个操作。\n",
    "\n",
    "为了模仿计算机的内存操作，NTM 的内存操作使用了两个 Attention，分别是 “基于内容的 Attention” 和 “基于位置的 Attention”。基于内容的 Attention 和我们之前介绍的 Attention 一样，用于从内存中找到某个向量（查询向量）的相似向量。\n",
    "\n",
    "而基于位置的 Attention 用于从上一个时刻关注的内存地址（内存的各个位置的权重）前后移动。这里我们省略对其技术细节的探讨，具体可以通过一维卷积运算实现。基于内存位置的移动功能，可以再现 “一边前进（一个内存地址）一边读取” 这种计算机特有的活动。\n",
    "\n",
    "NTM 的内存操作比较复杂。除了上面说到的操作以外，还包括锐化 Attention 权重的处理、加上上一个时刻的 Attention 权重的处理等。\n",
    "\n",
    "通过自由地使用外部存储装置，NTM 获得了强大的能力。实际上，对于 seq2seq 无法解决的复杂问题，NTM 取得了惊人的成绩。具体而言，NTM 成功解决了长时序的记忆问题、排序问题（从大到小排列数字）等。\n",
    "\n",
    "如此，NTM 借助外部存储装置获得了学习算法的能力，其中 Attention 作为一项重要技术而得到了应用。基于外部存储装置的扩展技术和 Attention 会越来越重要，今后将被应用在各种地方。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b9125",
   "metadata": {},
   "source": [
    "## 小结\n",
    "- 在翻译、语音识别等将一个时序数据转换为另一个时序数据的任务中，时序数据之间常常存在对应关系\n",
    "- Attention 从数据中学习两个时序数据之间的对应关系\n",
    "- Attention 使用向量内积（方法之一）计算向量之间的相似度，并输出这个相似度的加权和向量\n",
    "- 因为 Attention 中使用的运算是可微分的，所以可以基于误差反向传播法进行学习\n",
    "- 通过将 Attention 计算出的权重（概率）可视化，可以观察输入与输出之间的对应关系\n",
    "- 在基于外部存储装置扩展神经网络的研究示例中，Attention 被用来读写内存"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
