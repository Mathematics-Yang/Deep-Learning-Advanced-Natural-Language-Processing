{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bd5f7d",
   "metadata": {},
   "source": [
    "# 第六章 &nbsp; &nbsp; Gated RNN \n",
    "上一章的 RNN 存在环路，可以记忆过去的信息，其结构非常简单，易于实现。不过，遗憾的是，这个 RNN 的效果并不好。原因在于，许多情况下它都无法很好地学习到时序数据的长期依赖关系。\n",
    "\n",
    "现在，上一章的简单 RNN 经常被名为 LSTM 或 GRU 的层所代替。实际上，当我们说 RNN 时，更多的是指 LSTM 层，而不是上一章的 RNN。顺便说一句，当需要明确指上一章的 RNN 时，我们会说“简单 RNN”或 “Elman”。\n",
    "\n",
    "LSTM 和 GRU 中增加了一种名为“门”的结构。基于这个门，可以学习到时序数据的长期依赖关系。本章我们将指出上一章的 RNN 的问题，介绍代替它的 LSTM 和 GRU 等 “Gated RNN”。特别是我们将花很多时间研究 LSTM 的结构，并揭示它实现 “长期记忆” 的机制。此外，我们将使用 LSTM 创建语言模型，并展示它可以在实际数据上很好地学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7764fe85",
   "metadata": {},
   "source": [
    "## RNN的问题\n",
    "上一章介绍的 RNN 之所以不擅长学习时序数据的长期依赖关系，是因为 BPTT 会发生梯度消失和梯度爆炸的问题。本节我们将首先回顾一下上一章介绍的 RNN 层，并通过一个实际的例子来说明为什么 RNN 层不擅长长期记忆。\n",
    "\n",
    "---\n",
    "\n",
    "RNN（循环神经网络）的\"记忆是短期的\"主要源于其结构设计导致的**梯度消失或梯度爆炸问题**，这使得它难以捕捉长序列中的长期依赖关系，具体原因如下：\n",
    "\n",
    "1. **信息传递的衰减特性**  \n",
    "   RNN通过循环结构将前一时刻的隐藏状态传递到当前时刻，公式可简化为：  \n",
    "   $$\n",
    "    h_t = \\sigma(W_h h_{t-1} + W_x x_t + b) \n",
    "   $$ \n",
    "   其中 $ h_t $ 是当前隐藏状态，$ h_{t-1} $ 是前一时刻的状态。  \n",
    "   当序列较长时，早期信息需要通过多个时间步的矩阵乘法传递，类似于\"链式相乘\"。若权重矩阵的特征值小于1，多次相乘后会导致梯度逐渐趋近于0（梯度消失）；若大于1，则会导致梯度急剧增大（梯度爆炸）。  \n",
    "   这两种情况都会使模型难以学习到早期信息与后期输出的关联，仿佛\"记不住信息随着时间衰减\"。\n",
    "\n",
    "2. **无法主动保留重要信息**  \n",
    "   RNN没有机制主动筛选和保留关键信息，所有历史信息都会被同等处理并逐步传递。随着序列长度增加，早期信息会被不断涌入的新信息\"稀释\"，最终几乎无法影响后期的预测。  \n",
    "   例如，在处理一句话时，RNN可能很快忘记句首的主语，导致后续动词的单复数预测错误。\n",
    "\n",
    "3. **实际表现的局限性**  \n",
    "   在实践中，RNN能有效捕捉的依赖关系通常局限在几十个时间步内。对于更长的序列（如段落级文本、长视频帧），早期信息几乎会完全丢失，因此被认为只能保留\"短期记忆\"。\n",
    "\n",
    "相比之下，LSTM通过门控机制（遗忘门、输入门、输出门）主动控制信息的保留与丢弃，解决了梯度消失问题，从而能够学习到更长序列的依赖关系，具备\"长期记忆\"能力。\n",
    "\n",
    "---\n",
    "\n",
    "### RNN的复习\n",
    "RNN 层存在环路。如果展开它的循环，它将变成一个在水平方向上延伸的网络，如下图所示。\n",
    "\n",
    "<img src=\"./fig/RNN_unfold.png\" alt=\"RNN_unfold\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在图中，当输入时序数据 $\\boldsymbol{x}_t$ 时，RNN 层输出 $\\boldsymbol{h}_t$。这个 $\\boldsymbol{h}_t$ 也称为 RNN 层的隐藏状态，它记录过去的信息。\n",
    "\n",
    "RNN 的特点在于使用了上一时刻的隐藏状态，由此，RNN 可以继承过去的信息。顺便说一下，如果用计算图来表示此时 RNN 层进行的处理，则有下图。\n",
    "\n",
    "<img src=\"./fig/RNN_calculation.png\" alt=\"RNN_calculation\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，RNN 层的正向传播进行的计算由矩阵乘积、矩阵加法和基于激活函数 $\\tanh$ 的变换构成，这就是我们上一章看到的 RNN 层。下面，我们看一下这个 RNN 层存在的问题（关于长期记忆的问题）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2a662",
   "metadata": {},
   "source": [
    "## 梯度消失和梯度爆炸\n",
    "语言模型的任务是根据已经出现的单词预测下一个将要出现的单词。上一章我们实现了基于 RNN 的语言模型 RNNLM，这里借着探讨 RNNLM 问题的机会，我们再来考虑一下图所示的任务。\n",
    "\n",
    "<img src=\"./fig/long_memory.png\" alt=\"long_memory\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如前所述，填入 “?” 中的单词应该是 Tom。要正确回答这个问题，RNNLM 需要记住 “Tom 在房间看电视，Mary 进了房间” 这些信息。这些信息必须被编码并保存在 RNN 层的隐藏状态中。\n",
    "\n",
    "现在让我们站在 RNNLM 进行学习的角度来考虑上述问题。在正确解标签为 Tom 时，RNNLM 中的梯度是如何传播的呢？这里我们使用 BPTT 进行学习，因此梯度将从正确解标签 Tom 出现的地方向过去的方向传播，如下图所示。\n",
    "\n",
    "<img src=\"./fig/flow.png\" alt=\"flow\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在学习正确解标签 Tom 时，重要的是 RNN 层的存在。RNN 层通过向过去传递“有意义的梯度”，能够学习时间方向上的依赖关系。此时梯度（理论上）包含了那些应该学到的有意义的信息，通过将这些信息向过去传递，RNN 层学习长期的依赖关系。但是，如果这个梯度在中途变弱（甚至没有包含任何信息），则权重参数将不会被更新。也就是说，RNN 层无法学习长期的依赖关系。不幸的是，随着时间的回溯，这个简单 RNN 未能避免梯度变小（梯度消失）或者梯度变大（梯度爆炸）的命运。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e94da58",
   "metadata": {},
   "source": [
    "## 梯度消失和梯度爆炸的原因\n",
    "现在，我们深挖一下 RNN 层中梯度消失（或者梯度爆炸）的起因。如下图所示，这里仅关注 RNN 层在时间方向上的梯度传播。\n",
    "\n",
    "<img src=\"./fig/RNN_time_gradient.png\" alt=\"RNN_time_gradient\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，这里考虑长度为 $T$ 的时序数据，关注从第 $T$ 个正确解标签传递出的梯度如何变化。就上面的问题来说，这相当于第 $T$ 个正确解标签是 Tom 的情形。此时，关注时间方向上的梯度，可知反向传播的梯度流经 $\\tanh$、“$+$”和 MatMul（矩阵乘积）运算。\n",
    "\n",
    "“$+$”的反向传播将上游传来的梯度原样传给下游，因此梯度的值不变。那么，剩下的 $\\tanh$ 和 MatMul 运算会怎样变化呢？我们先来看一下 $\\tanh$。\n",
    "\n",
    "当 $y = \\tanh(x)$ 时，它的导数是 $\\frac{\\mathrm{d}y}{\\mathrm{d}x} = 1 - y^2$。此时，将 $y = \\tanh(x)$ 的值及其导数的值分别画在图上，如图所示。\n",
    "\n",
    "<img src=\"./fig/tanhx.png\" alt=\"tanhx\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "图中的虚线是 $y = \\tanh(x)$ 的导数。从图中可以看出，它的值小于 1.0，并且随着 $x$ 远离 0，它的值在变小。这意味着，当反向传播的梯度经过 $\\tanh$ 节点时，它的值会越来越小。因此，如果经过 $\\tanh$ 函数 $T$ 次，则梯度也会减小 $T$ 次。\n",
    "\n",
    "RNN 层的激活函数一般使用 $\\tanh$ 函数，但是如果改为 ReLU 函数，则有希望抑制梯度消失的问题（当 ReLU 的输入为 $x$ 时，它的输出是 $\\max(0, x)$）。这是因为，在 ReLU 的情况下，当 $x$ 大于 0 时，反向传播将上游的梯度原样传递到下游，梯度不会 “退化”。实际上，题为 “Improving performance of recurrent neural network with relu nonlinearity” 的论文就使用 ReLU 实现了性能改善。\n",
    "\n",
    "接下来，我们关注 MatMul（矩阵乘积）节点。简单起见，这里我们忽略 $\\tanh$ 节点。如此一来，如下图所示，RNN 层的反向传播的梯度就仅取决于 MatMul 运算。\n",
    "\n",
    "<img src=\"./fig/RNN_propagation.png\" alt=\"RNN_propagation\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在图中，假定从上游传来梯度 $\\mathrm{d}\\boldsymbol{h}$，此时 MatMul 节点的反向传播通过矩阵乘积 $\\mathrm{d}\\boldsymbol{h}\\boldsymbol{W}_h^\\mathrm{T}$ 计算梯度。之后，根据时序数据的时间步长，将这个计算重复相应次数。这里需要注意的是，每一次矩阵乘积计算都使用相同的权重 $\\boldsymbol{W}_h$。\n",
    "\n",
    "那么，反向传播时梯度的值通过 MatMul 节点时会如何变化呢？一旦有了疑问，最好的方法就是做实验！让我们通过下面的代码，来观察梯度大小的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fba87a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4684068094579303, 3.335704974161037, 4.783279375373183, 6.2795873320876145, 8.080776465019055, 10.25116303229294, 12.9360635066099, 16.276861327786712, 20.454829618345983, 25.688972842084684, 32.25315718048336, 40.48895641683869, 50.824407307019094, 63.79612654485427, 80.07737014308985, 100.51298922051251, 126.16331847536827, 158.3592064825883, 198.77107967611957, 249.495615421267]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False    # 正常显示负号\n",
    "\n",
    "N = 2  # mini-batch的大小\n",
    "H = 3  # 隐藏状态向量的维数\n",
    "T = 20  # 时序数据的长度\n",
    "\n",
    "dh = np.ones((N, H)) # 来自上游的梯度，初始值设为1\n",
    "\n",
    "np.random.seed(3) # 保持结果的可复现性\n",
    "\n",
    "Wh = np.random.randn(H, H) # 权重矩阵\n",
    "#Wh = np.random.randn(H, H) * 0.5\n",
    "\n",
    "norm_list = []\n",
    "for t in range(T):\n",
    "    dh = np.dot(dh, Wh.T)\n",
    "    norm = np.sqrt(np.sum(dh**2)) / N\n",
    "    norm_list.append(norm)\n",
    "\n",
    "print(norm_list) \n",
    "\n",
    "# 绘制图形\n",
    "plt.plot(np.arange(len(norm_list)), norm_list)\n",
    "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20]) # 设置x轴刻度\n",
    "plt.xlabel('时间步')\n",
    "plt.ylabel('范数')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a595ef2",
   "metadata": {},
   "source": [
    "这里用 `np.ones()` 初始化 `dh`（`np.ones()` 是所有元素均为 1 的矩阵）。然后，根据反向传播的 MatMul 节点的数量更新 `dh` 相应次数，并将各步的 `dh` 的大小（范数）添加到 `norm_list` 中。这里，`dh` 的大小是 mini-batch（$N$ 笔）中的平均 “L2 范数”。L2 范数对所有元素的平方和求平方根。\n",
    "\n",
    "下面，我们将上述代码的执行结果（`norm_list`）画在图上，如下图所示。\n",
    "\n",
    "<img src=\"./fig/gradient_grow.png\" alt=\"gradient_grow\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，可知梯度的大小随时间步长呈指数级增加，这就是梯度爆炸（exploding gradients）。如果发生梯度爆炸，最终就会导致溢出，出现 NaN（Not a Number，非数值）之类的值。如此一来，神经网络的学习将无法正确运行。\n",
    "\n",
    "现在做第 2 个实验，将 `Wh` 的初始值改为下面的值。\n",
    "\n",
    "```python\n",
    "# Wh = np.random.randn(H, H)    # 之前\n",
    "Wh = np.random.randn(H, H) * 0.5  # 现在\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1a88362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.2342034047289652, 0.8339262435402592, 0.5979099219216478, 0.3924742082554759, 0.25252426453184545, 0.16017442237957719, 0.10106299614538984, 0.06358148956166684, 0.039950839098332, 0.025086887541098325, 0.015748611904532892, 0.009884999125204758, 0.006204151282595104, 0.003893806551809953, 0.002443767399386287, 0.0015337065005571367, 0.0009625497320203268, 0.0006040924319556743, 0.00037912574706291117, 0.00023793756048323344]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False    # 正常显示负号\n",
    "\n",
    "N = 2  # mini-batch的大小\n",
    "H = 3  # 隐藏状态向量的维数\n",
    "T = 20  # 时序数据的长度\n",
    "\n",
    "dh = np.ones((N, H)) # 来自上游的梯度，初始值设为1\n",
    "\n",
    "np.random.seed(3) # 保持结果的可复现性\n",
    "\n",
    "# Wh = np.random.randn(H, H) \n",
    "Wh = np.random.randn(H, H) * 0.5 # 权重矩阵\n",
    "\n",
    "norm_list = []\n",
    "for t in range(T):\n",
    "    dh = np.dot(dh, Wh.T)\n",
    "    norm = np.sqrt(np.sum(dh**2)) / N\n",
    "    norm_list.append(norm)\n",
    "\n",
    "print(norm_list) \n",
    "\n",
    "# 绘制图形\n",
    "plt.plot(np.arange(len(norm_list)), norm_list)\n",
    "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20]) # 设置x轴刻度\n",
    "plt.xlabel('时间步')\n",
    "plt.ylabel('范数')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a32452",
   "metadata": {},
   "source": [
    "使用这个初始值，进行与上面相同的实验，结果如下图所示。\n",
    "\n",
    "<img src=\"./fig/gradient_descent.png\" alt=\"gradient_descent\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "从图中可以看出，这次梯度呈指数级减小，这就是**梯度消失**（vanishing gradients）。如果发生梯度消失，梯度将迅速变小。一旦梯度变小，权重梯度不能被更新，模型就会无法学习长期的依赖关系。\n",
    "\n",
    "在这里进行的实验中，梯度的大小或者呈指数级增加，或者呈指数级减小。为什么会出现这样的指数级变化呢？因为矩阵 `Wh` 被反复乘了 $T$ 次。如果 `Wh` 是标量，则问题将很简单：当 `Wh` 大于 1 时，梯度呈指数级增加；当 `Wh` 小于 1 时，梯度呈指数级减小。\n",
    "\n",
    "那么，如果 `Wh` 不是标量，而是矩阵呢？此时，矩阵的奇异值将成为指标。简单而言，矩阵的奇异值表示数据的离散程度。根据这个奇异值（更准确地说是多个奇异值中的最大值）是否大于 1，可以预测梯度大小的变化。\n",
    "\n",
    "如果奇异值的最大值大于1，则可以预测梯度很有可能会呈指数级增加；而如果奇异值的最大值小于1，则可以判断梯度会呈指数级减小。但是，并不是说奇异值比1大就一定会出现梯度爆炸。也就是说，这是必要条件，并非充分条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f985765",
   "metadata": {},
   "source": [
    "## 梯度爆炸的对策\n",
    "至此，我们探讨了 RNN 的梯度爆炸和梯度消失问题，现在我们继续讨论解决方案。首先来看一下梯度爆炸。\n",
    "\n",
    "解决梯度爆炸有既定的方法，称为**梯度裁剪**（gradients clipping）。这是一个非常简单的方法，它的伪代码如下所示：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{if } &\\|\\hat{\\boldsymbol{g}}\\| \\geqslant \\text{threshold}: \\\\\n",
    "&\\hat{\\boldsymbol{g}} = \\frac{\\text{threshold}}{\\|\\hat{\\boldsymbol{g}}\\|} \\hat{\\boldsymbol{g}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "这里假设可以将神经网络用到的所有参数的梯度整合成一个，并用符号 $\\hat{\\boldsymbol{g}}$ 表示。另外，将阈值设置为 $\\text{threshold}$。此时，如果梯度的 L2 范数 $\\|\\hat{\\boldsymbol{g}}\\|$ 大于或等于阈值，就按上述方法修正梯度，这就是梯度裁剪。如你所见，虽然这个方法很简单，但是在许多情况下效果都不错。\n",
    "\n",
    "$\\hat{\\boldsymbol{g}}$ 整合了神经网络中用到的所有参数的梯度。比如，当某个模型有 $\\boldsymbol{W}_1$ 和 $\\boldsymbol{W}_2$ 两个参数时，$\\hat{\\boldsymbol{g}}$ 就是这两个参数对应的梯度 $\\mathrm{d}\\boldsymbol{W}_1$ 和 $\\mathrm{d}\\boldsymbol{W}_2$ 的组合。\n",
    "\n",
    "现在，我们用 Python 来实现梯度裁剪，将其实现为 `clip_grads(grads, max_norm)` 函数。参数 `grads` 是梯度的列表，`max_norm` 是阈值，此时梯度裁剪可以如下实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac6f7699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "之前: [6.49144048 2.78487283 6.76254902 5.90862817 0.23981882 5.58854088\n",
      " 2.59252447 4.15101197 2.83525082]\n",
      "现在: [1.49503731 0.64138134 1.55747605 1.36081038 0.05523244 1.28709139\n",
      " 0.59708178 0.95601551 0.65298384]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "dW1 = np.random.rand(3, 3) * 10 # 生成一个3x3的随机矩阵，值在0到10之间\n",
    "dW2 = np.random.rand(3, 3) * 10 # 生成另一个3x3的随机矩阵，值在0到10之间\n",
    "grads = [dW1, dW2] # 假设有两个参数的梯度\n",
    "max_norm = 5.0 # 设置梯度裁剪的阈值\n",
    "\n",
    "# 定义梯度裁剪函数\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm) # 计算总的L2范数\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6) # 计算裁剪比例\n",
    "    if rate < 1: # 只有当总范数超过阈值时才进行裁剪\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "\n",
    "\n",
    "print('之前:', dW1.flatten()) \n",
    "clip_grads(grads, max_norm)\n",
    "print('现在:', dW1.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb2f5c",
   "metadata": {},
   "source": [
    "这就是梯度裁剪的实现，并没有什么特别难的地方。因为将来还会用到 `clip_grads(grads, max_norm)`，所以我们在 `common/util.py` 中也放了一份相同的代码。\n",
    "\n",
    "本书提供了用于RNNLM学习的<code>RnnlmTrainer</code>类（<code>common/trainer.py</code>），它的内部利用了上述梯度裁剪以防止梯度爆炸。我们会在之后的小节再次说明<code>RnnlmTrainer</code>类中的梯度裁剪。\n",
    "\n",
    "以上就是对梯度裁剪的说明。下面，我们看一下防止梯度消失的对策。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19044a3e",
   "metadata": {},
   "source": [
    "## 梯度消失和LSTM\n",
    "在 RNN 的学习中，梯度消失也是一个大问题。为了解决这个问题，需要从根本上改变 RNN 层的结构，这里本章的主题 Gated RNN 就要登场了。\n",
    "\n",
    "人们已经提出了诸多 Gated RNN 框架（网络结构），其中具有代表性的有 LSTM 和 GRU。本节我们将关注 LSTM，仔细研究它的结构，并阐明为何它不会（难以）引起梯度消失。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f6771",
   "metadata": {},
   "source": [
    "## LSTM的接口\n",
    "接下来，我们仔细看一下 LSTM 层。在此之前，为了将来方便，我们在计算图中引入“简略图示法”。如下图所示，这种图示法将矩阵计算等整理为一个长方形节点。\n",
    "\n",
    "<img src=\"./fig/simple_RNN.png\" alt=\"simple_RNN\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，这里将 $\\tanh(\\boldsymbol{h}_{t-1}\\boldsymbol{W}_h + \\boldsymbol{x}_t\\boldsymbol{W}_x + \\boldsymbol{b})$ 这个计算表示为一个长方形节点 $\\tanh$（$\\boldsymbol{h}_{t-1}$ 和 $\\boldsymbol{x}_t$ 是行向量），这个长方形节点中包含了矩阵乘积、偏置的和以及基于 $\\tanh$ 函数的变换。\n",
    "\n",
    "现在我们已经做好了准备。首先，我们来比较一下 LSTM 与 RNN 的接口（输入和输出）。\n",
    "\n",
    "<img src=\"./fig/RNN_vs_LSTM.png\" alt=\"RNN_vs_LSTM\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，LSTM 与 RNN 的接口的不同之处在于，LSTM 还有路径 $\\boldsymbol{c}$。这个 $\\boldsymbol{c}$ 称为记忆单元（或者简称为“单元”），相当于 LSTM 专用的记忆部门。\n",
    "\n",
    "记忆单元的特点是，仅在 LSTM 层内部接收和传递数据。也就是说，记忆单元在 LSTM 层内部结束工作，不向其他层输出。而 LSTM 的隐藏状态 $\\boldsymbol{h}$ 和 RNN 层相同，会被（向上）输出到其他层。\n",
    "\n",
    "从接收LSTM的输出的一侧来看，LSTM的输出仅有隐藏状态向量 $\\boldsymbol{h}$。记忆单元 $\\boldsymbol{c}$ 对外部不可见，我们甚至不用考虑它的存在。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a20a0fe",
   "metadata": {},
   "source": [
    "## LSTM层的结构\n",
    "现在，我们来看一下 LSTM 层的内部结构。这里，我想一个一个地组装 LSTM 的部件，并仔细研究它们的结构。以下内容参考了 “colah’s blog: Understanding LSTM Networks” 这篇优秀的文章。\n",
    "\n",
    "如前所述，LSTM 有记忆单元 $\\boldsymbol{c}_t$。这个 $\\boldsymbol{c}_t$ 存储了时刻 $t$ 时 LSTM 的记忆，可以认为其中保存了从过去到时刻 $t$ 的所有必要信息（或者以此为目的进行了学习）。然后，基于这个充满必要信息的记忆，向外部的层（和下一时刻的 LSTM）输出隐藏状态 $\\boldsymbol{h}_t$。如图所示，LSTM 输出经 $\\tanh$ 函数变换后的记忆单元。\n",
    "\n",
    "<img src=\"./fig/LSTM_memory.png\" alt=\"LSTM_memory\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，当前的记忆单元 $\\boldsymbol{c}_t$ 是基于 3 个输入 $\\boldsymbol{c}_{t-1}$、$\\boldsymbol{h}_{t-1}$ 和 $\\boldsymbol{x}_t$，经过“某种计算”（后述）算出来的。这里的重点是隐藏状态 $\\boldsymbol{h}_t$ 要使用更新后的 $\\boldsymbol{c}_t$ 来计算。另外，这个计算是 $\\boldsymbol{h}_t = \\tanh(\\boldsymbol{c}_t)$，表示对 $\\boldsymbol{c}_t$ 的各个元素应用 $\\tanh$ 函数。\n",
    "\n",
    "到目前为止，记忆单元 $\\boldsymbol{c}_t$ 和隐藏状态 $\\boldsymbol{h}_t$ 的关系只是按元素应用 $\\tanh$ 函数。这意味着，记忆单元 $\\boldsymbol{c}_t$ 和隐藏状态 $\\boldsymbol{h}_t$ 的元素个数相同。如果记忆单元 $\\boldsymbol{c}_t$ 的元素个数是 100，则隐藏状态 $\\boldsymbol{h}_t$ 的元素个数也是 100。\n",
    "\n",
    "在进入下一项之前，我们先简单说明一下 Gate 的功能。Gate 是“门”的意思，就像将门打开或合上一样，控制数据的流动。直观上，如下图所示，门的作用就是阻止或者释放水流。\n",
    "\n",
    "<img src=\"./fig/gate.png\" alt=\"gate\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "LSTM 中使用的门并非只能“开或合”，还可以根据将门打开多少来控制水的流量。如下图所示，可以将“开合程度”控制在 0.7（70%）或者 0.2（20%）。\n",
    "\n",
    "<img src=\"./fig/gate_control.png\" alt=\"gate_control\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，门的开合程度由 0.0~1.0 的实数表示（1.0 为全开），通过这个数值控制流出的水量。这里的重点是，门的开合程度也是（自动）从数据中学习到的。\n",
    "\n",
    "有专门的权重参数用于控制门的开合程度，这些权重参数通过学习被更新。另外，sigmoid函数用于求门的开合程度（sigmoid函数的输出范围在0.0~1.0）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165f257a",
   "metadata": {},
   "source": [
    "## 输出门\n",
    "现在，我们将话题转回到 LSTM。在刚才的说明中，隐藏状态 $\\boldsymbol{h}_t$ 对记忆单元 $\\boldsymbol{c}_t$ 仅仅应用了 $\\tanh$ 函数。这里考虑对 $\\tanh(\\boldsymbol{c}_t)$ 施加门。换句话说，针对 $\\tanh(\\boldsymbol{c}_t)$ 的各个元素，调整它们作为下一时刻的隐藏状态的重要程度。由于这个门管理下一个隐藏状态 $\\boldsymbol{h}_t$ 的输出，所以称为**输出门**（output gate）。\n",
    "\n",
    "输出门的开合程度（流出比例）根据输入 $\\boldsymbol{x}_t$ 和上一个状态 $\\boldsymbol{h}_{t-1}$ 求出。此时进行的计算如式 (6.1) 所示。这里在使用的权重参数和偏置的上标上添加了 output 的首字母 o。之后，我们也将使用上标表示门。另外，sigmoid 函数用 $\\sigma()$ 表示。\n",
    "\n",
    "$$\n",
    "\\boldsymbol{o} = \\sigma(\\boldsymbol{x}_t \\boldsymbol{W}_x^{(\\text{o})} + \\boldsymbol{h}_{t-1} \\boldsymbol{W}_h^{(\\text{o})} + \\boldsymbol{b}^{(\\text{o})}) \\tag{6.1}\n",
    "$$\n",
    "\n",
    "如式 (6.1) 所示，输入 $\\boldsymbol{x}_t$ 有权重 $\\boldsymbol{W}_x^{(\\text{o})}$，上一时刻的状态 $\\boldsymbol{h}_{t-1}$ 有权重 $\\boldsymbol{W}_h^{(\\text{o})}$（$\\boldsymbol{x}_t$ 和 $\\boldsymbol{h}_{t-1}$ 是行向量）。将它们的矩阵乘积和偏置 $\\boldsymbol{b}^{(\\text{o})}$ 之和传给 sigmoid 函数，结果就是输出门的输出 $\\boldsymbol{o}$。最后，将这个 $\\boldsymbol{o}$ 和 $\\tanh(\\boldsymbol{c}_t)$ 的对应元素的乘积作为 $\\boldsymbol{h}_t$ 输出。将这些计算绘制成计算图，结果如图所示。\n",
    "\n",
    "<img src=\"./fig/output_gate.png\" alt=\"output_gate\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在图中，将输出门进行的式 (6.1) 的计算表示为 $\\sigma$。然后，将它的输出表示为 $\\boldsymbol{o}$，则 $\\boldsymbol{h}_t$ 可由 $\\boldsymbol{o}$ 和 $\\tanh(\\boldsymbol{c}_t)$ 的乘积计算出来。这里说的“乘积”是对应元素的乘积，也称为阿达玛乘积。如果用 $\\odot$ 表示阿达玛乘积，则此处的计算如下所示：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{h}_t = \\boldsymbol{o} \\odot \\tanh(\\boldsymbol{c}_t) \\tag{6.2}\n",
    "$$\n",
    "\n",
    "以上就是 LSTM 的输出门。这样一来，LSTM 的输出部分就完成了，接着我们再来看一下记忆单元的更新部分。\n",
    "\n",
    "$\\tanh$ 的输出是 $-1.0 \\sim 1.0$ 的实数。我们可以认为这个 $-1.0 \\sim 1.0$ 的数值表示某种被编码的 “信息” 的强弱（程度）。而 sigmoid 函数的输出是 $0.0 \\sim 1.0$ 的实数，表示数据流出的比例。因此，在大多数情况下，门使用 sigmoid 函数作为激活函数，而包含实质信息的数据则使用 $\\tanh$ 函数作为激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aadc4a",
   "metadata": {},
   "source": [
    "## 遗忘门\n",
    "只有放下包袱，才能轻装上阵。接下来，我们要做的就是明确告诉记忆单元需要“忘记什么”。这里，我们使用门来实现这一目标。\n",
    "\n",
    "现在，我们在记忆单元 $\\boldsymbol{c}_{t-1}$ 上添加一个忘记不必要记忆的门，这里称为**遗忘门**（forget gate）。将遗忘门添加到 LSTM 层，计算图如图所示。\n",
    "\n",
    "<img src=\"./fig/forget_gate.png\" alt=\"forget_gate\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在图中，将遗忘门进行的一系列计算表示为 $\\sigma$，其中有遗忘门专用的权重参数，此时的计算如下：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{f} = \\sigma(\\boldsymbol{x}_t \\boldsymbol{W}_x^{(\\text{f})} + \\boldsymbol{h}_{t-1} \\boldsymbol{W}_h^{(\\text{f})} + \\boldsymbol{b}^{(\\text{f})}) \\tag{6.3}\n",
    "$$\n",
    "\n",
    "遗忘门的输出 $\\boldsymbol{f}$ 可以由式 (6.3) 求得。然后，$\\boldsymbol{c}_t$ 由这个 $\\boldsymbol{f}$ 和上一个记忆单元 $\\boldsymbol{c}_{t-1}$ 的对应元素的乘积求得（$\\boldsymbol{c}_t = \\boldsymbol{f} \\odot \\boldsymbol{c}_{t-1}$）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b8e58",
   "metadata": {},
   "source": [
    "## 新的记忆单元\n",
    "遗忘门从上一时刻的记忆单元中删除了应该忘记的东西，但是这样一来，记忆单元只会忘记信息。现在我们还想向这个记忆单元添加一些应当记住的新信息，为此我们添加新的 $\\tanh$ 节点。\n",
    "\n",
    "<img src=\"./fig/new_memory.png\" alt=\"new_memory\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，基于 $\\tanh$ 节点计算出的结果被加到上一时刻的记忆单元 $\\boldsymbol{c}_{t-1}$ 上。这样一来，新的信息就被添加到了记忆单元中。这个 $\\tanh$ 节点的作用不是门，而是将新的信息添加到记忆单元中。因此，它不用 sigmoid 函数作为激活函数，而是使用 $\\tanh$ 函数。$\\tanh$ 节点进行的计算如下所示：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{g} = \\tanh(\\boldsymbol{x}_t \\boldsymbol{W}_x^{(\\text{g})} + \\boldsymbol{h}_{t-1} \\boldsymbol{W}_h^{(\\text{g})} + \\boldsymbol{b}^{(\\text{g})}) \\tag{6.4}\n",
    "$$\n",
    "\n",
    "这里用 $\\boldsymbol{g}$ 表示向记忆单元添加的新信息。通过将这个 $\\boldsymbol{g}$ 加到上一时刻的 $\\boldsymbol{c}_{t-1}$ 上，从而形成新的记忆。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe239bfc",
   "metadata": {},
   "source": [
    "## 输入门\n",
    "最后，我们给上图的 $\\boldsymbol{g}$ 添加门，这里将这个新添加的门称为**输入门**（input gate）。添加输入门后，计算图如下图所示。\n",
    "\n",
    "<img src=\"./fig/input_gate.png\" alt=\"input_gate\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "输入门判断新增信息 $\\boldsymbol{g}$ 的各个元素的价值有多大。输入门不会不经考虑就添加新信息，而是会对要添加的信息进行取舍。换句话说，输入门会添加加权后的新信息。\n",
    "\n",
    "在图中，用 $\\sigma$ 表示输入门，用 $\\boldsymbol{i}$ 表示输出，此时进行的计算如下所示：\n",
    "\n",
    "$$\n",
    "\\boldsymbol{i} = \\sigma(\\boldsymbol{x}_t \\boldsymbol{W}_x^{(\\text{i})} + \\boldsymbol{h}_{t-1} \\boldsymbol{W}_h^{(\\text{i})} + \\boldsymbol{b}^{(\\text{i})}) \\tag{6.5}\n",
    "$$\n",
    "\n",
    "然后，将 $\\boldsymbol{i}$ 和 $\\boldsymbol{g}$ 的对应元素的乘积添加到记忆单元中。以上就是对 LSTM 内部处理的说明。\n",
    "\n",
    "LSTM 有多个“变体”。这里说明的 LSTM 是最有代表性的 LSTM，也有许多在门的连接方式上稍微不同的其他 LSTM。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb27e46",
   "metadata": {},
   "source": [
    "## LSTM的梯度的流动\n",
    "上面我们介绍了 LSTM 的结构，那么，为什么它不会引起梯度消失呢？其原因可以通过观察记忆单元 $\\boldsymbol{c}$ 的反向传播来了解。\n",
    "\n",
    "<img src=\"./fig/memory_init_backpropagation.png\" alt=\"memory_init_backpropagation\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在图中，我们仅关注记忆单元，绘制了它的反向传播。此时，记忆单元的反向传播仅流过“+”和“×”节点。“+”节点将上游传来的梯度原样流出，所以梯度没有变化（退化）。\n",
    "\n",
    "而“×”节点的计算并不是矩阵乘积，而是对应元素的乘积（阿达玛积）。顺便说一下，在之前的 RNN 的反向传播中，我们使用相同的权重矩阵重复了多次矩阵乘积计算，由此导致了梯度消失（或梯度爆炸）。而这里的 LSTM 的反向传播进行的不是矩阵乘积计算，而是对应元素的乘积计算，而且每次都会基于不同的门值进行对应元素的乘积计算。这就是它不会发生梯度消失（或梯度爆炸）的原因。\n",
    "\n",
    "图中的“×”节点的计算由遗忘门控制（每次输出不同的门值）。遗忘门认为“应该忘记”的记忆单元的元素，其梯度会变小；而遗忘门认为“不能忘记”的元素，其梯度在向过去的方向流动时不会退化。因此，可以期待记忆单元的梯度（应该长期记住的信息）能在不发生梯度消失的情况下传播。\n",
    "\n",
    "从以上讨论可知，LSTM 的记忆单元不会（难以）发生梯度消失。因此，可以期待记忆单元能够保存（学习）长期的依赖关系。\n",
    "\n",
    "LSTM 是 Long Short-Term Memory（长短期记忆）的缩写，意思是可以长（Long）时间维持短期记忆（Short-Term Memory）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb435566",
   "metadata": {},
   "source": [
    "## LSTM的实现\n",
    "下面，我们来实现 LSTM。这里将进行单步处理的类实现为 `LSTM` 类，将整体处理 $T$ 步的类实现为 `TimeLSTM` 类。现在我们先来整理一下 LSTM 中进行的计算，如下所示：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{f} &= \\sigma(\\boldsymbol{x}_t \\boldsymbol{W}_x^{(\\text{f})} + \\boldsymbol{h}_{t-1} \\boldsymbol{W}_h^{(\\text{f})} + \\boldsymbol{b}^{(\\text{f})}) \\\\\n",
    "\\boldsymbol{g} &= \\tanh(\\boldsymbol{x}_t \\boldsymbol{W}_x^{(\\text{g})} + \\boldsymbol{h}_{t-1} \\boldsymbol{W}_h^{(\\text{g})} + \\boldsymbol{b}^{(\\text{g})}) \\\\\n",
    "\\boldsymbol{i} &= \\sigma(\\boldsymbol{x}_t \\boldsymbol{W}_x^{(\\text{i})} + \\boldsymbol{h}_{t-1} \\boldsymbol{W}_h^{(\\text{i})} + \\boldsymbol{b}^{(\\text{i})}) \\\\\n",
    "\\boldsymbol{o} &= \\sigma(\\boldsymbol{x}_t \\boldsymbol{W}_x^{(\\text{o})} + \\boldsymbol{h}_{t-1} \\boldsymbol{W}_h^{(\\text{o})} + \\boldsymbol{b}^{(\\text{o})}) \\\\\n",
    "\\boldsymbol{c}_t &= \\boldsymbol{f} \\odot \\boldsymbol{c}_{t-1} + \\boldsymbol{g} \\odot \\boldsymbol{i} \\\\\n",
    "\\boldsymbol{h}_t &= \\boldsymbol{o} \\odot \\tanh(\\boldsymbol{c}_t)\n",
    "\\end{align*} \\tag{6.6, 6.7, 6.8}\n",
    "$$\n",
    "\n",
    "以上就是 LSTM 进行的计算。这里需要注意式 (6.6) 中的 4 个仿射变换。这里的仿射变换是指 $\\boldsymbol{x}\\boldsymbol{W}_x + \\boldsymbol{h}\\boldsymbol{W}_h + \\boldsymbol{b}$ 这样的式子。式 (6.6) 中通过 4 个式子分别进行仿射变换，但其实可以整合为通过 1 个式子进行，如下图所示。\n",
    "\n",
    "<img src=\"./fig/four_weight.png\" alt=\"four_weight\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在图中，4 个权重（或偏置）被整合为了 1 个。如此，原本单独执行 4 次的仿射变换通过 1 次计算即可完成，可以加快计算速度。这是因为矩阵库计算“大矩阵”时通常会更快，而且通过将权重整合到一起管理，源代码也会更简洁。\n",
    "\n",
    "假设 $\\boldsymbol{W}_x$、$\\boldsymbol{W}_h$ 和 $\\boldsymbol{b}$ 分别包含 4 个权重（或偏置），此时 LSTM 的计算图如下图所示。\n",
    "\n",
    "<img src=\"./fig/four_weight_LSTM.png\" alt=\"four_weight_LSTM\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，先一起执行 4 个仿射变换。然后，基于 slice 节点，取出 4 个结果。这个 slice 节点很简单，它将仿射变换的结果（矩阵）均等地分成 4 份，然后取出内容。在 slice 节点之后，数据流过激活函数（sigmoid 函数或 $\\tanh$ 函数），进行上一节介绍的计算。\n",
    "\n",
    "现在，参考上图，我们来实现 LSTM 类。首先来看一下 LSTM 类的初始化代码。\n",
    "\n",
    "初始化的参数有权重参数 `Wx`、`Wh` 和偏置 `b`。如前所述，这些权重（或偏置）整合了 4 个权重。把这些参数获得的权重参数设定给成员变量 `params`，并初始化形状与之对应的梯度。另外，成员变量 `cache` 保存正向传播的中间结果，它们将在反向传播的计算中使用。\n",
    "\n",
    "接下来实现正向传播的 `forward(x, h_prev, c_prev)` 方法。它的参数接收当前时刻的输入 `x`、上一时刻的隐藏状态 `h_prev`，以及上一时刻的记忆单元 `c_prev`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81d297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys \n",
    "sys.path.append(os.pardir) # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "from common.functions import *\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape # N是mini-batch的大小，H是隐藏状态向量的维数\n",
    "\n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b # 仿射变换\n",
    "\n",
    "        f = sigmoid(A[:, :H]) # 遗忘门\n",
    "        g = np.tanh(A[:, H:2*H]) # 输入门的候选值\n",
    "        i = sigmoid(A[:, 2*H:3*H]) # 输入门\n",
    "        o = sigmoid(A[:, 3*H:]) # 输出门\n",
    "\n",
    "        c_next = f * c_prev + i * g # 记忆单元的更新\n",
    "        h_next = o * np.tanh(c_next) # 隐藏状态的更新 \n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next) # 保存中间结果以备反向传播使用\n",
    "        return h_next, c_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ff45a",
   "metadata": {},
   "source": [
    "首先进行仿射变换。重复一下，此时的成员变量 `Wx`、`Wh` 和 `b` 保存的是 4 个权重，矩阵的形状将变为如图所示的样子。\n",
    "\n",
    "<img src=\"./fig/affine_transformation.png\" alt=\"affine_transformation\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在图中，批大小是 $N$，输入数据的维数是 $D$，记忆单元和隐藏状态的维数都是 $H$。另外，计算结果 $\\boldsymbol{A}$ 中保存了 4 个仿射变换的结果。因此，通过 $\\boldsymbol{A}[:, :H]$、$\\boldsymbol{A}[:, H:2*H]$ 这样的切片取出数据，并分配给之后的运算节点。参考 LSTM 的数学式和计算图，剩余的实现应该不难。\n",
    "\n",
    "LSTM 层中保存了 4 个权重。这样一来，LSTM 层只需管理 $Wx$、$Wh$ 和 $b$ 这 3 个参数。顺便说一下，RNN 层中也保存着 $Wx$、$Wh$ 和 $b$ 这 3 个参数。LSTM 层和 RNN 层的参数数量虽然相同，但是它们的形状不一样。\n",
    "\n",
    "LSTM 的反向传播可以通过将计算图反方向传播而求得。基于前面介绍的知识，这并不困难。不过，因为 slice 节点是第一次见到，所以我们简要说明一下它的反向传播。\n",
    "\n",
    "slice 节点将矩阵分成了 4 份，因此它的反向传播需要整合 4 个梯度，如下图所示。\n",
    "\n",
    "<img src=\"./fig/slice_backpropagation.png\" alt=\"slice_backpropagation\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "由图可知，在 slice 节点的反向传播中，拼接 4 个矩阵。图中有 4 个梯度 $\\text{df}$、$\\text{dg}$、$\\text{di}$ 和 $\\text{do}$，将它们拼接成 $\\text{dA}$。如果通过 NumPy 进行，则可以使用 `np.hstack()`。`np.hstack()` 在水平方向上将参数中给定的数组拼接起来（垂直方向上的拼接使用 `np.vstack()`）。因此，上述处理可以用下面 1 行代码完成。\n",
    "\n",
    "```python\n",
    "dA = np.hstack((df, dg, di, do))\n",
    "```\n",
    "\n",
    "以上就是对 slice 节点的反向传播的说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfdef1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dh_next, dc_next):\n",
    "    Wx, Wh, b = self.params\n",
    "    x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "    N, H = dh_next.shape # N是mini-batch的大小，H是隐藏状态向量的维数\n",
    "\n",
    "    tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "    dc_next += dh_next * o * (1 - tanh_c_next ** 2) # 记忆单元的梯度\n",
    "    dc_prev = dc_next * f # 上一时刻记忆单元的梯度\n",
    "\n",
    "    di = dc_next * g # 输入门的梯度\n",
    "    df = dc_next * c_prev # 遗忘门的梯度\n",
    "    do = dh_next * tanh_c_next # 输出门的梯度\n",
    "    dg = dc_next * i # 输入门的候选值的梯度\n",
    "\n",
    "    di *= i * (1 - i) # sigmoid函数的导数\n",
    "    df *= f * (1 - f) # sigmoid函数的导数\n",
    "    do *= o * (1 - o) # sigmoid函数的导数\n",
    "    dg *= (1 - g ** 2) # tanh函数的导数\n",
    "\n",
    "    dA = np.hstack((df, dg, di, do)) # 将四个部分的梯度合并\n",
    "\n",
    "    db = np.sum(dA, axis=0) # 偏置的梯度\n",
    "    dWh = np.dot(h_prev.T, dA) # Wh的梯度\n",
    "    dWx = np.dot(x.T, dA) # Wx的梯度\n",
    "    dx = np.dot(dA, Wx.T) # 输入x的梯度\n",
    "    dh_prev = np.dot(dA, Wh.T) # 上一时刻隐藏状态h_prev的梯度\n",
    "\n",
    "    self.grads[0][...] = dWx\n",
    "    self.grads[1][...] = dWh\n",
    "    self.grads[2][...] = db\n",
    "\n",
    "    return dx, dh_prev, dc_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bb4d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from common.functions import *\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape # N是mini-batch的大小，H是隐藏状态向量的维数\n",
    "\n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b # 仿射变换\n",
    "\n",
    "        f = sigmoid(A[:, :H]) # 遗忘门\n",
    "        g = np.tanh(A[:, H:2*H]) # 输入门的候选值\n",
    "        i = sigmoid(A[:, 2*H:3*H]) # 输入门\n",
    "        o = sigmoid(A[:, 3*H:]) # 输出门\n",
    "\n",
    "        c_next = f * c_prev + i * g # 记忆单元的更新\n",
    "        h_next = o * np.tanh(c_next) # 隐藏状态的更新 \n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next) # 保存中间结果以备反向传播使用\n",
    "        return h_next, c_next\n",
    "    \n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "        N, H = dh_next.shape # N是mini-batch的大小，H是隐藏状态向量的维数\n",
    "\n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2) # 记忆单元的梯度\n",
    "        dc_prev = ds * f # 传递给上一时刻的记忆单元的梯度\n",
    "\n",
    "        di = ds * g # 输入门的梯度\n",
    "        dg = ds * i # 输入门的候选值的梯度\n",
    "        df = ds * c_prev # 遗忘门的梯度\n",
    "        do = dh_next * tanh_c_next # 输出门的梯度\n",
    "\n",
    "        di *= i * (1 - i) # sigmoid函数的导数\n",
    "        df *= f * (1 - f) # sigmoid函数的导数\n",
    "        do *= o * (1 - o) # sigmoid函数的导数\n",
    "        dg *= (1 - g ** 2) # tanh函数的导数\n",
    "\n",
    "        dA = np.hstack((df, dg, di, do)) # 将四个部分的梯度合并\n",
    "\n",
    "        db = np.sum(dA, axis=0) # 偏置的梯度\n",
    "        dWh = np.dot(h_prev.T, dA) # Wh的梯度\n",
    "        dWx = np.dot(x.T, dA) # Wx的梯度\n",
    "        dx = np.dot(dA, Wx.T) # 传递给输入数据x的梯度\n",
    "        dh_prev = np.dot(dA, Wh.T) # 传递给上一时刻隐藏状态h_prev的梯度\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        return dx, dh_prev, dc_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e0d41",
   "metadata": {},
   "source": [
    "## Time LSTM层的实现\n",
    "现在我们继续 `TimeLSTM` 的实现。`Time LSTM` 层是整体处理 $T$ 个时序数据的层，由 $T$ 个 LSTM 层构成，如图所示。\n",
    "\n",
    "<img src=\"./fig/time_LSTM.png\" alt=\"time_LSTM\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如前所述，RNN 中使用 Truncated BPTT 进行学习。Truncated BPTT 以适当的长度截断反向传播的连接，但是需要维持正向传播的数据流。为此，如下图所示，将隐藏状态和记忆单元保存在成员变量中。这样一来，在调用下一个 `forward()` 函数时，就可以继承上一时刻的隐藏状态（和记忆单元）。\n",
    "\n",
    "<img src=\"./fig/time_LSTM_backpropagation.png\" alt=\"time_LSTM_backpropagation\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "我们已经实现了 `Time RNN` 层，这里也以同样的方式实现 `Time LSTM` 层。`TimeLSTM` 可以像下面这样实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35906d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b] # 参数\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape # N是mini-batch的大小，T是时序数据的长度，D是输入数据的维数\n",
    "        H = Wh.shape[0] # H是隐藏状态向量的维数\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params) # 生成一个LSTM层\n",
    "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c) # 前向传播\n",
    "            hs[:, t, :] = self.h # 保存隐藏状态\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc) # 反向传播\n",
    "            dxs[:, t, :] = dx # 保存输入数据的梯度\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h, c=None):\n",
    "        self.h, self.c = h, c # 设置隐藏状态和记忆单元\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None # 重置隐藏状态和记忆单元"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeaffa7",
   "metadata": {},
   "source": [
    "在 LSTM 中，除了隐藏状态 $\\boldsymbol{h}$ 外，还使用记忆单元 $\\boldsymbol{c}$。`TimeLSTM` 类的实现和 `TimeRNN` 类几乎一样。这里仍通过参数 `stateful` 指定是否维持状态。接下来，我们使用这个 `TimeLSTM` 创建语言模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00dda9b",
   "metadata": {},
   "source": [
    "## 使用LSTM的语言模型\n",
    "`Time LSTM` 层的实现完成了，现在我们来实现正题——语言模型。这里实现的语言模型和上一章几乎是一样的，唯一的区别是，上一章使用 `Time RNN` 层的地方这次使用 `Time LSTM` 层，如下图所示。\n",
    "\n",
    "<img src=\"./fig/language_model_network.png\" alt=\"language_model_network\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "由图可知，这里和上一章实现的语言模型的差别在于使用了 LSTM。我们将右图中的神经网络实现为 `Rnnlm` 类。`Rnnlm` 类和上一章介绍的 `SimpleRnnlm` 类几乎相同，但是增加了一些新方法。下面给出使用 LSTM 层实现的 `Rnnlm` 类的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc26b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') # 为了导入上一级目录的common模块\n",
    "from common.time_layers import *\n",
    "from common.base_model import BaseModel \n",
    "\n",
    "\n",
    "class Rnnlm(BaseModel):\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size # 词汇表大小、词向量维数、隐藏状态向量维数\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 初始化权重\n",
    "        embed_W = (rn(V, D) / 100).astype('f') # 词向量矩阵\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f') # 输入到隐藏层的权重\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f') # 隐藏层到隐藏层的权重\n",
    "        lstm_b = np.zeros(4 * H).astype('f') # LSTM的偏置\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f') # 输出层的权重\n",
    "        affine_b = np.zeros(V).astype('f') # 输出层的偏置\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W), # 词嵌入层\n",
    "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True), # LSTM层\n",
    "            TimeAffine(affine_W, affine_b) # 输出层\n",
    "        ] \n",
    "        self.loss_layer = TimeSoftmaxWithLoss() # 损失函数层\n",
    "        self.lstm_layer = self.layers[1] # 方便后面重置状态\n",
    "\n",
    "        # 将所有的权重和梯度整理到列表中\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs) # 预测值\n",
    "        loss = self.loss_layer.forward(score, ts) # 计算损失\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers): # 反向传播\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state() # 重置LSTM的状态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e7c0d",
   "metadata": {},
   "source": [
    "`Rnnlm` 类将到 Softmax 层为止的处理实现为 `predict()` 方法，这个方法在第 7 章进行文本生成时还会用到。此外，该类还添加了用于读写参数的 `save_params()` 和 `load_params()` 方法。剩下的实现与上一章的 `SimpleRnnlm` 类相同。\n",
    "\n",
    "common/base_model.py 中有一个 BaseModel 类，该类实现了 save_params() 和 load_params() 方法。因此，通过继承 BaseModel 类，也能获得数据读写的功能。另外，BaseModel 类的实现还进行了优化，以支持 GPU 和进行缩位（使用 16 位浮点数存储）。\n",
    "\n",
    "下面，我们在 PTB 数据集上学习这个网络。这次我们使用 PTB 数据集的所有训练数据进行学习（上一章中只使用了 PTB 数据集的一部分），代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9de93cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 1327 | time 0[s] | perplexity 9999.69\n",
      "| epoch 1 |  iter 21 / 1327 | time 3[s] | perplexity 2965.94\n",
      "| epoch 1 |  iter 41 / 1327 | time 7[s] | perplexity 1249.77\n",
      "| epoch 1 |  iter 61 / 1327 | time 11[s] | perplexity 973.69\n",
      "| epoch 1 |  iter 81 / 1327 | time 15[s] | perplexity 797.85\n",
      "| epoch 1 |  iter 101 / 1327 | time 19[s] | perplexity 673.13\n",
      "| epoch 1 |  iter 121 / 1327 | time 23[s] | perplexity 661.38\n",
      "| epoch 1 |  iter 141 / 1327 | time 28[s] | perplexity 578.21\n",
      "| epoch 1 |  iter 161 / 1327 | time 32[s] | perplexity 596.38\n",
      "| epoch 1 |  iter 181 / 1327 | time 36[s] | perplexity 577.70\n",
      "| epoch 1 |  iter 201 / 1327 | time 40[s] | perplexity 515.73\n",
      "| epoch 1 |  iter 221 / 1327 | time 43[s] | perplexity 503.64\n",
      "| epoch 1 |  iter 241 / 1327 | time 48[s] | perplexity 442.97\n",
      "| epoch 1 |  iter 261 / 1327 | time 52[s] | perplexity 472.80\n",
      "| epoch 1 |  iter 281 / 1327 | time 56[s] | perplexity 454.90\n",
      "| epoch 1 |  iter 301 / 1327 | time 60[s] | perplexity 400.31\n",
      "| epoch 1 |  iter 321 / 1327 | time 64[s] | perplexity 335.20\n",
      "| epoch 1 |  iter 341 / 1327 | time 68[s] | perplexity 398.22\n",
      "| epoch 1 |  iter 361 / 1327 | time 72[s] | perplexity 408.77\n",
      "| epoch 1 |  iter 381 / 1327 | time 76[s] | perplexity 340.89\n",
      "| epoch 1 |  iter 401 / 1327 | time 80[s] | perplexity 354.99\n",
      "| epoch 1 |  iter 421 / 1327 | time 84[s] | perplexity 344.60\n",
      "| epoch 1 |  iter 441 / 1327 | time 88[s] | perplexity 336.79\n",
      "| epoch 1 |  iter 461 / 1327 | time 92[s] | perplexity 325.02\n",
      "| epoch 1 |  iter 481 / 1327 | time 96[s] | perplexity 307.72\n",
      "| epoch 1 |  iter 501 / 1327 | time 100[s] | perplexity 312.56\n",
      "| epoch 1 |  iter 521 / 1327 | time 104[s] | perplexity 301.02\n",
      "| epoch 1 |  iter 541 / 1327 | time 108[s] | perplexity 315.23\n",
      "| epoch 1 |  iter 561 / 1327 | time 112[s] | perplexity 288.71\n",
      "| epoch 1 |  iter 581 / 1327 | time 116[s] | perplexity 257.20\n",
      "| epoch 1 |  iter 601 / 1327 | time 120[s] | perplexity 335.17\n",
      "| epoch 1 |  iter 621 / 1327 | time 124[s] | perplexity 306.68\n",
      "| epoch 1 |  iter 641 / 1327 | time 128[s] | perplexity 280.72\n",
      "| epoch 1 |  iter 661 / 1327 | time 132[s] | perplexity 269.80\n",
      "| epoch 1 |  iter 681 / 1327 | time 136[s] | perplexity 226.88\n",
      "| epoch 1 |  iter 701 / 1327 | time 140[s] | perplexity 249.71\n",
      "| epoch 1 |  iter 721 / 1327 | time 144[s] | perplexity 259.13\n",
      "| epoch 1 |  iter 741 / 1327 | time 148[s] | perplexity 220.67\n",
      "| epoch 1 |  iter 761 / 1327 | time 152[s] | perplexity 236.23\n",
      "| epoch 1 |  iter 781 / 1327 | time 156[s] | perplexity 216.40\n",
      "| epoch 1 |  iter 801 / 1327 | time 160[s] | perplexity 239.98\n",
      "| epoch 1 |  iter 821 / 1327 | time 164[s] | perplexity 224.10\n",
      "| epoch 1 |  iter 841 / 1327 | time 168[s] | perplexity 225.57\n",
      "| epoch 1 |  iter 861 / 1327 | time 172[s] | perplexity 221.04\n",
      "| epoch 1 |  iter 881 / 1327 | time 176[s] | perplexity 206.11\n",
      "| epoch 1 |  iter 901 / 1327 | time 180[s] | perplexity 253.05\n",
      "| epoch 1 |  iter 921 / 1327 | time 184[s] | perplexity 226.48\n",
      "| epoch 1 |  iter 941 / 1327 | time 189[s] | perplexity 229.16\n",
      "| epoch 1 |  iter 961 / 1327 | time 193[s] | perplexity 242.72\n",
      "| epoch 1 |  iter 981 / 1327 | time 197[s] | perplexity 227.46\n",
      "| epoch 1 |  iter 1001 / 1327 | time 201[s] | perplexity 192.24\n",
      "| epoch 1 |  iter 1021 / 1327 | time 204[s] | perplexity 225.92\n",
      "| epoch 1 |  iter 1041 / 1327 | time 208[s] | perplexity 206.02\n",
      "| epoch 1 |  iter 1061 / 1327 | time 212[s] | perplexity 196.43\n",
      "| epoch 1 |  iter 1081 / 1327 | time 216[s] | perplexity 167.43\n",
      "| epoch 1 |  iter 1101 / 1327 | time 220[s] | perplexity 192.20\n",
      "| epoch 1 |  iter 1121 / 1327 | time 224[s] | perplexity 227.11\n",
      "| epoch 1 |  iter 1141 / 1327 | time 228[s] | perplexity 208.04\n",
      "| epoch 1 |  iter 1161 / 1327 | time 232[s] | perplexity 196.65\n",
      "| epoch 1 |  iter 1181 / 1327 | time 236[s] | perplexity 189.19\n",
      "| epoch 1 |  iter 1201 / 1327 | time 240[s] | perplexity 161.94\n",
      "| epoch 1 |  iter 1221 / 1327 | time 244[s] | perplexity 157.78\n",
      "| epoch 1 |  iter 1241 / 1327 | time 248[s] | perplexity 185.98\n",
      "| epoch 1 |  iter 1261 / 1327 | time 252[s] | perplexity 171.11\n",
      "| epoch 1 |  iter 1281 / 1327 | time 255[s] | perplexity 176.26\n",
      "| epoch 1 |  iter 1301 / 1327 | time 259[s] | perplexity 220.50\n",
      "| epoch 1 |  iter 1321 / 1327 | time 263[s] | perplexity 209.06\n",
      "| epoch 2 |  iter 1 / 1327 | time 265[s] | perplexity 223.85\n",
      "| epoch 2 |  iter 21 / 1327 | time 269[s] | perplexity 204.29\n",
      "| epoch 2 |  iter 41 / 1327 | time 273[s] | perplexity 189.27\n",
      "| epoch 2 |  iter 61 / 1327 | time 277[s] | perplexity 175.56\n",
      "| epoch 2 |  iter 81 / 1327 | time 281[s] | perplexity 159.42\n",
      "| epoch 2 |  iter 101 / 1327 | time 285[s] | perplexity 150.45\n",
      "| epoch 2 |  iter 121 / 1327 | time 288[s] | perplexity 159.07\n",
      "| epoch 2 |  iter 141 / 1327 | time 292[s] | perplexity 176.79\n",
      "| epoch 2 |  iter 161 / 1327 | time 296[s] | perplexity 192.62\n",
      "| epoch 2 |  iter 181 / 1327 | time 300[s] | perplexity 200.29\n",
      "| epoch 2 |  iter 201 / 1327 | time 304[s] | perplexity 184.50\n",
      "| epoch 2 |  iter 221 / 1327 | time 308[s] | perplexity 183.51\n",
      "| epoch 2 |  iter 241 / 1327 | time 312[s] | perplexity 175.89\n",
      "| epoch 2 |  iter 261 / 1327 | time 316[s] | perplexity 185.23\n",
      "| epoch 2 |  iter 281 / 1327 | time 320[s] | perplexity 184.21\n",
      "| epoch 2 |  iter 301 / 1327 | time 324[s] | perplexity 165.51\n",
      "| epoch 2 |  iter 321 / 1327 | time 328[s] | perplexity 138.99\n",
      "| epoch 2 |  iter 341 / 1327 | time 332[s] | perplexity 168.92\n",
      "| epoch 2 |  iter 361 / 1327 | time 336[s] | perplexity 196.26\n",
      "| epoch 2 |  iter 381 / 1327 | time 340[s] | perplexity 153.40\n",
      "| epoch 2 |  iter 401 / 1327 | time 344[s] | perplexity 167.64\n",
      "| epoch 2 |  iter 421 / 1327 | time 348[s] | perplexity 153.79\n",
      "| epoch 2 |  iter 441 / 1327 | time 352[s] | perplexity 160.37\n",
      "| epoch 2 |  iter 461 / 1327 | time 356[s] | perplexity 155.88\n",
      "| epoch 2 |  iter 481 / 1327 | time 359[s] | perplexity 155.94\n",
      "| epoch 2 |  iter 501 / 1327 | time 363[s] | perplexity 169.34\n",
      "| epoch 2 |  iter 521 / 1327 | time 367[s] | perplexity 172.35\n",
      "| epoch 2 |  iter 541 / 1327 | time 372[s] | perplexity 174.17\n",
      "| epoch 2 |  iter 561 / 1327 | time 376[s] | perplexity 155.95\n",
      "| epoch 2 |  iter 581 / 1327 | time 380[s] | perplexity 136.62\n",
      "| epoch 2 |  iter 601 / 1327 | time 384[s] | perplexity 188.58\n",
      "| epoch 2 |  iter 621 / 1327 | time 388[s] | perplexity 180.54\n",
      "| epoch 2 |  iter 641 / 1327 | time 392[s] | perplexity 163.84\n",
      "| epoch 2 |  iter 661 / 1327 | time 396[s] | perplexity 154.91\n",
      "| epoch 2 |  iter 681 / 1327 | time 400[s] | perplexity 129.52\n",
      "| epoch 2 |  iter 701 / 1327 | time 404[s] | perplexity 149.70\n",
      "| epoch 2 |  iter 721 / 1327 | time 408[s] | perplexity 160.23\n",
      "| epoch 2 |  iter 741 / 1327 | time 412[s] | perplexity 132.56\n",
      "| epoch 2 |  iter 761 / 1327 | time 416[s] | perplexity 129.48\n",
      "| epoch 2 |  iter 781 / 1327 | time 420[s] | perplexity 135.23\n",
      "| epoch 2 |  iter 801 / 1327 | time 424[s] | perplexity 146.67\n",
      "| epoch 2 |  iter 821 / 1327 | time 428[s] | perplexity 143.18\n",
      "| epoch 2 |  iter 841 / 1327 | time 432[s] | perplexity 144.89\n",
      "| epoch 2 |  iter 861 / 1327 | time 435[s] | perplexity 145.09\n",
      "| epoch 2 |  iter 881 / 1327 | time 439[s] | perplexity 129.40\n",
      "| epoch 2 |  iter 901 / 1327 | time 443[s] | perplexity 166.46\n",
      "| epoch 2 |  iter 921 / 1327 | time 447[s] | perplexity 146.27\n",
      "| epoch 2 |  iter 941 / 1327 | time 451[s] | perplexity 152.63\n",
      "| epoch 2 |  iter 961 / 1327 | time 455[s] | perplexity 162.67\n",
      "| epoch 2 |  iter 981 / 1327 | time 459[s] | perplexity 154.69\n",
      "| epoch 2 |  iter 1001 / 1327 | time 463[s] | perplexity 131.59\n",
      "| epoch 2 |  iter 1021 / 1327 | time 467[s] | perplexity 154.98\n",
      "| epoch 2 |  iter 1041 / 1327 | time 471[s] | perplexity 142.22\n",
      "| epoch 2 |  iter 1061 / 1327 | time 475[s] | perplexity 128.20\n",
      "| epoch 2 |  iter 1081 / 1327 | time 479[s] | perplexity 109.77\n",
      "| epoch 2 |  iter 1101 / 1327 | time 483[s] | perplexity 118.45\n",
      "| epoch 2 |  iter 1121 / 1327 | time 487[s] | perplexity 153.35\n",
      "| epoch 2 |  iter 1141 / 1327 | time 492[s] | perplexity 142.98\n",
      "| epoch 2 |  iter 1161 / 1327 | time 496[s] | perplexity 131.42\n",
      "| epoch 2 |  iter 1181 / 1327 | time 500[s] | perplexity 133.50\n",
      "| epoch 2 |  iter 1201 / 1327 | time 504[s] | perplexity 111.64\n",
      "| epoch 2 |  iter 1221 / 1327 | time 509[s] | perplexity 108.57\n",
      "| epoch 2 |  iter 1241 / 1327 | time 513[s] | perplexity 129.47\n",
      "| epoch 2 |  iter 1261 / 1327 | time 517[s] | perplexity 123.12\n",
      "| epoch 2 |  iter 1281 / 1327 | time 522[s] | perplexity 121.68\n",
      "| epoch 2 |  iter 1301 / 1327 | time 526[s] | perplexity 156.13\n",
      "| epoch 2 |  iter 1321 / 1327 | time 530[s] | perplexity 152.79\n",
      "| epoch 3 |  iter 1 / 1327 | time 532[s] | perplexity 161.12\n",
      "| epoch 3 |  iter 21 / 1327 | time 536[s] | perplexity 142.43\n",
      "| epoch 3 |  iter 41 / 1327 | time 540[s] | perplexity 135.80\n",
      "| epoch 3 |  iter 61 / 1327 | time 544[s] | perplexity 127.40\n",
      "| epoch 3 |  iter 81 / 1327 | time 548[s] | perplexity 117.40\n",
      "| epoch 3 |  iter 101 / 1327 | time 553[s] | perplexity 105.01\n",
      "| epoch 3 |  iter 121 / 1327 | time 557[s] | perplexity 115.84\n",
      "| epoch 3 |  iter 141 / 1327 | time 561[s] | perplexity 126.19\n",
      "| epoch 3 |  iter 161 / 1327 | time 565[s] | perplexity 140.38\n",
      "| epoch 3 |  iter 181 / 1327 | time 570[s] | perplexity 151.33\n",
      "| epoch 3 |  iter 201 / 1327 | time 574[s] | perplexity 140.25\n",
      "| epoch 3 |  iter 221 / 1327 | time 578[s] | perplexity 141.33\n",
      "| epoch 3 |  iter 241 / 1327 | time 582[s] | perplexity 135.13\n",
      "| epoch 3 |  iter 261 / 1327 | time 587[s] | perplexity 139.07\n",
      "| epoch 3 |  iter 281 / 1327 | time 591[s] | perplexity 140.41\n",
      "| epoch 3 |  iter 301 / 1327 | time 595[s] | perplexity 123.60\n",
      "| epoch 3 |  iter 321 / 1327 | time 599[s] | perplexity 102.52\n",
      "| epoch 3 |  iter 341 / 1327 | time 603[s] | perplexity 122.63\n",
      "| epoch 3 |  iter 361 / 1327 | time 608[s] | perplexity 152.33\n",
      "| epoch 3 |  iter 381 / 1327 | time 612[s] | perplexity 115.84\n",
      "| epoch 3 |  iter 401 / 1327 | time 616[s] | perplexity 129.95\n",
      "| epoch 3 |  iter 421 / 1327 | time 620[s] | perplexity 112.32\n",
      "| epoch 3 |  iter 441 / 1327 | time 624[s] | perplexity 123.11\n",
      "| epoch 3 |  iter 461 / 1327 | time 629[s] | perplexity 117.45\n",
      "| epoch 3 |  iter 481 / 1327 | time 633[s] | perplexity 120.18\n",
      "| epoch 3 |  iter 501 / 1327 | time 637[s] | perplexity 128.59\n",
      "| epoch 3 |  iter 521 / 1327 | time 641[s] | perplexity 138.17\n",
      "| epoch 3 |  iter 541 / 1327 | time 646[s] | perplexity 136.46\n",
      "| epoch 3 |  iter 561 / 1327 | time 650[s] | perplexity 118.65\n",
      "| epoch 3 |  iter 581 / 1327 | time 654[s] | perplexity 103.82\n",
      "| epoch 3 |  iter 601 / 1327 | time 659[s] | perplexity 147.68\n",
      "| epoch 3 |  iter 621 / 1327 | time 663[s] | perplexity 142.95\n",
      "| epoch 3 |  iter 641 / 1327 | time 667[s] | perplexity 129.57\n",
      "| epoch 3 |  iter 661 / 1327 | time 671[s] | perplexity 120.79\n",
      "| epoch 3 |  iter 681 / 1327 | time 675[s] | perplexity 100.25\n",
      "| epoch 3 |  iter 701 / 1327 | time 680[s] | perplexity 119.06\n",
      "| epoch 3 |  iter 721 / 1327 | time 684[s] | perplexity 125.68\n",
      "| epoch 3 |  iter 741 / 1327 | time 688[s] | perplexity 107.08\n",
      "| epoch 3 |  iter 761 / 1327 | time 692[s] | perplexity 103.16\n",
      "| epoch 3 |  iter 781 / 1327 | time 696[s] | perplexity 103.85\n",
      "| epoch 3 |  iter 801 / 1327 | time 700[s] | perplexity 116.31\n",
      "| epoch 3 |  iter 821 / 1327 | time 704[s] | perplexity 116.28\n",
      "| epoch 3 |  iter 841 / 1327 | time 708[s] | perplexity 114.49\n",
      "| epoch 3 |  iter 861 / 1327 | time 713[s] | perplexity 120.22\n",
      "| epoch 3 |  iter 881 / 1327 | time 717[s] | perplexity 106.08\n",
      "| epoch 3 |  iter 901 / 1327 | time 721[s] | perplexity 132.80\n",
      "| epoch 3 |  iter 921 / 1327 | time 725[s] | perplexity 117.96\n",
      "| epoch 3 |  iter 941 / 1327 | time 729[s] | perplexity 126.80\n",
      "| epoch 3 |  iter 961 / 1327 | time 733[s] | perplexity 130.89\n",
      "| epoch 3 |  iter 981 / 1327 | time 738[s] | perplexity 125.21\n",
      "| epoch 3 |  iter 1001 / 1327 | time 742[s] | perplexity 109.08\n",
      "| epoch 3 |  iter 1021 / 1327 | time 746[s] | perplexity 127.92\n",
      "| epoch 3 |  iter 1041 / 1327 | time 750[s] | perplexity 118.71\n",
      "| epoch 3 |  iter 1061 / 1327 | time 755[s] | perplexity 103.64\n",
      "| epoch 3 |  iter 1081 / 1327 | time 759[s] | perplexity 88.15\n",
      "| epoch 3 |  iter 1101 / 1327 | time 763[s] | perplexity 94.32\n",
      "| epoch 3 |  iter 1121 / 1327 | time 767[s] | perplexity 122.40\n",
      "| epoch 3 |  iter 1141 / 1327 | time 771[s] | perplexity 116.15\n",
      "| epoch 3 |  iter 1161 / 1327 | time 775[s] | perplexity 105.76\n",
      "| epoch 3 |  iter 1181 / 1327 | time 780[s] | perplexity 111.20\n",
      "| epoch 3 |  iter 1201 / 1327 | time 784[s] | perplexity 94.40\n",
      "| epoch 3 |  iter 1221 / 1327 | time 788[s] | perplexity 88.43\n",
      "| epoch 3 |  iter 1241 / 1327 | time 792[s] | perplexity 104.24\n",
      "| epoch 3 |  iter 1261 / 1327 | time 797[s] | perplexity 105.05\n",
      "| epoch 3 |  iter 1281 / 1327 | time 801[s] | perplexity 100.41\n",
      "| epoch 3 |  iter 1301 / 1327 | time 805[s] | perplexity 129.42\n",
      "| epoch 3 |  iter 1321 / 1327 | time 809[s] | perplexity 127.09\n",
      "| epoch 4 |  iter 1 / 1327 | time 811[s] | perplexity 132.52\n",
      "| epoch 4 |  iter 21 / 1327 | time 815[s] | perplexity 122.48\n",
      "| epoch 4 |  iter 41 / 1327 | time 819[s] | perplexity 106.83\n",
      "| epoch 4 |  iter 61 / 1327 | time 824[s] | perplexity 107.52\n",
      "| epoch 4 |  iter 81 / 1327 | time 828[s] | perplexity 96.97\n",
      "| epoch 4 |  iter 101 / 1327 | time 832[s] | perplexity 86.73\n",
      "| epoch 4 |  iter 121 / 1327 | time 836[s] | perplexity 96.14\n",
      "| epoch 4 |  iter 141 / 1327 | time 841[s] | perplexity 103.38\n",
      "| epoch 4 |  iter 161 / 1327 | time 845[s] | perplexity 116.85\n",
      "| epoch 4 |  iter 181 / 1327 | time 849[s] | perplexity 129.38\n",
      "| epoch 4 |  iter 201 / 1327 | time 853[s] | perplexity 120.21\n",
      "| epoch 4 |  iter 221 / 1327 | time 857[s] | perplexity 122.03\n",
      "| epoch 4 |  iter 241 / 1327 | time 862[s] | perplexity 116.31\n",
      "| epoch 4 |  iter 261 / 1327 | time 866[s] | perplexity 115.54\n",
      "| epoch 4 |  iter 281 / 1327 | time 871[s] | perplexity 120.65\n",
      "| epoch 4 |  iter 301 / 1327 | time 875[s] | perplexity 104.38\n",
      "| epoch 4 |  iter 321 / 1327 | time 879[s] | perplexity 84.78\n",
      "| epoch 4 |  iter 341 / 1327 | time 884[s] | perplexity 99.76\n",
      "| epoch 4 |  iter 361 / 1327 | time 888[s] | perplexity 129.02\n",
      "| epoch 4 |  iter 381 / 1327 | time 892[s] | perplexity 98.66\n",
      "| epoch 4 |  iter 401 / 1327 | time 897[s] | perplexity 110.25\n",
      "| epoch 4 |  iter 421 / 1327 | time 901[s] | perplexity 94.12\n",
      "| epoch 4 |  iter 441 / 1327 | time 905[s] | perplexity 104.13\n",
      "| epoch 4 |  iter 461 / 1327 | time 909[s] | perplexity 99.62\n",
      "| epoch 4 |  iter 481 / 1327 | time 913[s] | perplexity 103.84\n",
      "| epoch 4 |  iter 501 / 1327 | time 917[s] | perplexity 107.76\n",
      "| epoch 4 |  iter 521 / 1327 | time 921[s] | perplexity 117.14\n",
      "| epoch 4 |  iter 541 / 1327 | time 925[s] | perplexity 113.86\n",
      "| epoch 4 |  iter 561 / 1327 | time 929[s] | perplexity 104.11\n",
      "| epoch 4 |  iter 581 / 1327 | time 934[s] | perplexity 88.67\n",
      "| epoch 4 |  iter 601 / 1327 | time 938[s] | perplexity 126.93\n",
      "| epoch 4 |  iter 621 / 1327 | time 942[s] | perplexity 121.48\n",
      "| epoch 4 |  iter 641 / 1327 | time 945[s] | perplexity 111.35\n",
      "| epoch 4 |  iter 661 / 1327 | time 949[s] | perplexity 103.87\n",
      "| epoch 4 |  iter 681 / 1327 | time 953[s] | perplexity 84.74\n",
      "| epoch 4 |  iter 701 / 1327 | time 957[s] | perplexity 102.94\n",
      "| epoch 4 |  iter 721 / 1327 | time 961[s] | perplexity 109.25\n",
      "| epoch 4 |  iter 741 / 1327 | time 965[s] | perplexity 95.80\n",
      "| epoch 4 |  iter 761 / 1327 | time 969[s] | perplexity 88.59\n",
      "| epoch 4 |  iter 781 / 1327 | time 973[s] | perplexity 88.58\n",
      "| epoch 4 |  iter 801 / 1327 | time 977[s] | perplexity 99.22\n",
      "| epoch 4 |  iter 821 / 1327 | time 981[s] | perplexity 102.83\n",
      "| epoch 4 |  iter 841 / 1327 | time 985[s] | perplexity 98.82\n",
      "| epoch 4 |  iter 861 / 1327 | time 988[s] | perplexity 105.01\n",
      "| epoch 4 |  iter 881 / 1327 | time 992[s] | perplexity 92.69\n",
      "| epoch 4 |  iter 901 / 1327 | time 996[s] | perplexity 115.88\n",
      "| epoch 4 |  iter 921 / 1327 | time 1000[s] | perplexity 103.72\n",
      "| epoch 4 |  iter 941 / 1327 | time 1004[s] | perplexity 111.42\n",
      "| epoch 4 |  iter 961 / 1327 | time 1008[s] | perplexity 111.96\n",
      "| epoch 4 |  iter 981 / 1327 | time 1012[s] | perplexity 108.25\n",
      "| epoch 4 |  iter 1001 / 1327 | time 1016[s] | perplexity 96.78\n",
      "| epoch 4 |  iter 1021 / 1327 | time 1020[s] | perplexity 113.16\n",
      "| epoch 4 |  iter 1041 / 1327 | time 1024[s] | perplexity 103.66\n",
      "| epoch 4 |  iter 1061 / 1327 | time 1028[s] | perplexity 89.69\n",
      "| epoch 4 |  iter 1081 / 1327 | time 1032[s] | perplexity 78.01\n",
      "| epoch 4 |  iter 1101 / 1327 | time 1035[s] | perplexity 79.69\n",
      "| epoch 4 |  iter 1121 / 1327 | time 1039[s] | perplexity 103.93\n",
      "| epoch 4 |  iter 1141 / 1327 | time 1043[s] | perplexity 101.96\n",
      "| epoch 4 |  iter 1161 / 1327 | time 1047[s] | perplexity 91.79\n",
      "| epoch 4 |  iter 1181 / 1327 | time 1051[s] | perplexity 96.40\n",
      "| epoch 4 |  iter 1201 / 1327 | time 1055[s] | perplexity 83.67\n",
      "| epoch 4 |  iter 1221 / 1327 | time 1059[s] | perplexity 76.06\n",
      "| epoch 4 |  iter 1241 / 1327 | time 1063[s] | perplexity 91.28\n",
      "| epoch 4 |  iter 1261 / 1327 | time 1067[s] | perplexity 93.92\n",
      "| epoch 4 |  iter 1281 / 1327 | time 1071[s] | perplexity 89.38\n",
      "| epoch 4 |  iter 1301 / 1327 | time 1075[s] | perplexity 112.16\n",
      "| epoch 4 |  iter 1321 / 1327 | time 1079[s] | perplexity 110.20\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating perplexity ...\n",
      "234 / 235\n",
      "test perplexity:  136.42147627671523\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..') # 为了导入上一级目录的common模块\n",
    "from common.optimizer import SGD \n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.util import eval_perplexity\n",
    "from dataset import ptb\n",
    "from rnnlm import Rnnlm\n",
    "\n",
    "\n",
    "# 设定超参数\n",
    "batch_size = 20 # mini-batch的大小\n",
    "wordvec_size = 100 # 词向量的维数\n",
    "hidden_size = 100  # RNN的隐藏状态向量的元素个数\n",
    "time_size = 35  # RNN的展开大小\n",
    "lr = 20.0 # 学习率\n",
    "max_epoch = 4 # 最大迭代轮数\n",
    "max_grad = 0.25 # 用于梯度裁剪的阈值\n",
    "\n",
    "# 读入训练数据\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train') # 训练数据\n",
    "corpus_test, _, _ = ptb.load_data('test') # 测试数据\n",
    "vocab_size = len(word_to_id) # 词汇表大小\n",
    "xs = corpus[:-1] # 输入数据\n",
    "ts = corpus[1:] # 目标数据\n",
    "\n",
    "# 生成模型\n",
    "model = Rnnlm(vocab_size, wordvec_size, hidden_size) # RNN语言模型\n",
    "optimizer = SGD(lr) # 随机梯度下降法\n",
    "trainer = RnnlmTrainer(model, optimizer) # 训练器\n",
    "\n",
    "# step1:应用梯度裁剪进行学习\n",
    "trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\n",
    "            eval_interval=20) # 每20轮评估一次\n",
    "trainer.plot(ylim=(0, 500)) # 绘制学习曲线\n",
    "\n",
    "# step2:基于测试数据进行评价\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print('test perplexity: ', ppl_test)\n",
    "\n",
    "# step3:保存参数\n",
    "model.save_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f23cdd",
   "metadata": {},
   "source": [
    "这里给出的代码和上一章的代码有很多相同的地方，因此这里重点介绍不同的地方。首先，代码 step1 处使用 `RnnlmTrainer` 类进行模型的学习。`RnnlmTrainer` 类的 `fit()` 方法求模型的梯度，更新模型的参数。另外，在方法内部，通过指定 `max_grad` 参数，从而应用梯度裁剪。顺便说一下，`fit()` 方法内部进行的实现如下所示（这里给出的是伪代码）。\n",
    "\n",
    "```python\n",
    "# 求梯度\n",
    "model.forward(...)\n",
    "model.backward(...)\n",
    "params, grads = model.params, model.grads\n",
    "# 梯度裁剪\n",
    "if max_grad is not None:\n",
    "    clip_grads(grads, max_grad)\n",
    "# 更新参数\n",
    "optimizer.update(params, grads)\n",
    "```\n",
    "\n",
    "我们在之前的小节将梯度裁剪实现为了 `clip_grads(grads, max_grad)`，这里使用该方法进行梯度裁剪。\n",
    "\n",
    "另外，通过 step1 处的 `fit()` 方法的参数 `eval_interval=20`，每 20 次迭代对困惑度进行 1 次评价。因为这次的数据量很大，所以没有对每个 epoch 进行评价，而是每 20 次迭代评价 1 次。后面我们会将评价结果用 `plot()` 方法绘制成图。\n",
    "\n",
    "在学习结束后，在代码 step2 处使用测试数据对困惑度进行评价。这里需要注意的是，此时需要先重置模型的状态（LSTM 的隐藏状态和记忆单元）。此外，因为评价困惑度的函数 `eval_perplexity()` 在 `common/util.py` 中已经实现，所以直接使用即可。\n",
    "\n",
    "最后，在代码 step3 处将学习好的参数保存到外部文件。在下一章生成句子时，将会使用这些学习好的权重参数。\n",
    "\n",
    "以上就是 RNNLM 的学习代码。执行代码后，在终端上会输出下图的结果。\n",
    "\n",
    "<img src=\"./fig/output.png\" alt=\"output\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在图中，每 20 次迭代输出 1 次困惑度的值。我们来看一下结果，刚开始的困惑度为 10 000.84，这意味着下一个单词的候选个数能减少到 10 000 个左右。因为这次数据集的词汇量是 10 000 个，所以这是什么也没学习的状态，相当于猜测。但是随着学习的进行，困惑度开始变好。实际上，当迭代超过 300 次时，困惑度已经降到了 400 以下。现在，我们看一下困惑度的演变图，如下图所示。\n",
    "\n",
    "<img src=\"./fig/perplexity_change.png\" alt=\"perplexity_change\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在这次的实验中，一共进行了 4 个 epoch 的学习（按迭代来算，相当于 1327 * 4 次）。如图所示，困惑度顺利下降，最终达到 100 左右。基于最终的测试数据的评价（源代码 step2 处）结果为 136.07…。该结果在每次执行时都不相同，但是都在 135 前后。换句话说，我们的模型成长到了能将下一个单词的候选个数（从 10 000 个）缩小到 136 个左右的水平。\n",
    "\n",
    "那么，136 这样的困惑度在实践中是什么水平呢？说实话，这并不是一个很好的结果。在 2017 年的一个研究中，PTB 数据集上的困惑度已经降到了 60 以下。我们的模型还有很大的改进空间，下面我们就来进一步改进现有的 RNNLM。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb615f54",
   "metadata": {},
   "source": [
    "## 进一步改进RNNLM\n",
    "本节我们先针对当前的 RNNLM 说明 3 点需要改进的地方，然后实施这些改进，并评价最后精度提高了多少。\n",
    "\n",
    "## LSTM层的多层化\n",
    "在使用 RNNLM 创建高精度模型时，加深 LSTM 层（叠加多个 LSTM 层）的方法往往很有效。之前我们只用了一个 LSTM 层，通过叠加多个层，可以提高语言模型的精度。例如，在下图中，RNNLM 使用了两个 LSTM 层。\n",
    "\n",
    "<img src=\"./fig/two_LSTM.png\" alt=\"two_LSTM\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "上图显示了叠加两个 LSTM 层的例子。此时，第一个 LSTM 层的隐藏状态是第二个 LSTM 层的输入。按照同样的方式，我们可以叠加多个 LSTM 层，从而学习更加复杂的模式，这和前馈神经网络时的层加深是一样的。在前作《深度学习入门：基于 Python 的理论与实现》中，我们通过叠加多个 Affine 层和 Convolution 层，创建了表现力更好的模型。\n",
    "\n",
    "那么，应该叠加几个层呢？这其实是一个关于超参数的问题。因为层数是超参数，所以需要根据要解决的问题的复杂程度、能给到的训练数据的规模来确定。顺便说一句，在 PTB 数据集上学习语言模型的情况下，当 LSTM 的层数为 2 ~ 4 时，可以获得比较好的结果。\n",
    "\n",
    "据报道，谷歌翻译中使用的 GNMT 模型是叠加了 8 层 LSTM 的网络。如该例所示，如果待解决的问题很难，又能准备大量的训练数据，就可以通过加深 LSTM 层来提高精度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775df6b0",
   "metadata": {},
   "source": [
    "## 基于Dropout抑制过拟合\n",
    "通过叠加 LSTM 层，可以期待能够学习到时序数据的复杂依赖关系。换句话说，通过加深层，可以创建表现力更强的模型，但是这样的模型往往会发生**过拟合**（overfitting）。更糟糕的是，RNN 比常规的前馈神经网络更容易发生过拟合，因此 RNN 的过拟合对策非常重要。\n",
    "\n",
    "过拟合是指过度学习了训练数据的状态，也就是说，过拟合是一种缺乏泛化能力的状态。我们想要的是一个泛化能力强的模型，因此必须基于训练数据和验证数据的评价差异，判断是否发生了过拟合，并据此来进行模型的设计。\n",
    "\n",
    "抑制过拟合已有既定的方法：一是增加训练数据；二是降低模型的复杂度。我们会优先考虑这两个方法。除此之外，对模型复杂度给予惩罚的**正则化**也很有效。比如，L2 正则化会对过大的权重进行惩罚。\n",
    "\n",
    "此外，像 Dropout 这样，在训练时随机忽略层的一部分（比如 50%）神经元，也可以被视为一种正则化。本节我们将仔细研究 Dropout，并将其应用于 RNN。\n",
    "\n",
    "<img src=\"./fig/dropout.png\" alt=\"dropout\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，Dropout 随机选择一部分神经元，然后忽略它们，停止向前传递信号。这种 “随机忽视” 是一种制约，可以提高神经网络的泛化能力。我们在前作《深度学习入门：基于 Python 的理论与实现》中已经实现了 Dropout。如下图所示，当时我们给出了在激活函数后插入 Dropout 层的示例，并展示了它有助于抑制过拟合。\n",
    "\n",
    "<img src=\"./fig/dropout_example.png\" alt=\"dropout_example\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "那么，在使用 RNN 的模型中，应该将 Dropout 层插入哪里呢？首先可以想到的是插入在 LSTM 层的时序方向上，如下图所示。不过答案是，这并不是一个好的插入方式。\n",
    "\n",
    "<img src=\"./fig/dropout_bad_example.png\" alt=\"dropout_bad_example\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如果在时序方向上插入 Dropout，那么当模型学习时，随着时间的推移，信息会渐渐丢失。也就是说，因 Dropout 产生的噪声会随时间成比例地积累。考虑到噪声的积累，最好不要在时间轴方向上插入 Dropout。因此，如下图所示，我们在深度方向（垂直方向）上插入 Dropout 层。\n",
    "\n",
    "<img src=\"./fig/dropout_good_example.png\" alt=\"dropout_good_example\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "这样一来，无论沿时间方向（水平方向）前进多少，信息都不会丢失。Dropout 与时间轴独立，仅在深度方向（垂直方向）上起作用。\n",
    "\n",
    "如前所述，“常规的 Dropout” 不适合用在时间方向上。但是，最近的研究提出了多种方法来实现时间方向上的 RNN 正则化。比如，有文献中提出的 “变分 Dropout”（variational dropout）就被成功地应用在了时间方向上。\n",
    "\n",
    "除了深度方向，变分 Dropout 也能用在时间方向上，从而进一步提高语言模型的精度。如下图所示，它的机制是同一层的 Dropout 使用相同的 mask。这里所说的 mask 是指决定是否传递数据的随机布尔值。\n",
    "\n",
    "<img src=\"./fig/dropout_mask.png\" alt=\"dropout_mask\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，通过同一层的 Dropout 共用 mask，mask 被 “固定”。如此一来，信息的损失方式也被 “固定”，所以可以避免常规 Dropout 发生的指数级信息损失。\n",
    "\n",
    "据说变分 Dropout 比常规 Dropout 的效果更好。不过，本章并不打算使用变分 Dropout，而是仍使用常规 Dropout。变分 Dropout 的想法很简单，感兴趣的读者可以自己尝试实现一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09471680",
   "metadata": {},
   "source": [
    "## 权重共享\n",
    "改进语言模型有一个非常简单的技巧，那就是**权重共享**（weight tying）。weight tying 可以直译为 “权重绑定”。如下图所示，其含义就是共享权重。\n",
    "\n",
    "<img src=\"./fig/weight_shared.png\" alt=\"weight_shared\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，绑定（共享）Embedding 层和 Affine 层的权重的技巧在于权重共享。通过在这两个层之间共享权重，可以大大减少学习的参数数量。尽管如此，它仍能提高精度。真可谓一石二鸟！\n",
    "\n",
    "现在，我们来考虑一下权重共享的实现。这里，假设词汇量为 $V$，LSTM 的隐藏状态的维数为 $H$，则 Embedding 层的权重形状为 $V \\times H$，Affine 层的权重形状为 $H \\times V$。此时，如果要使用权重共享，只需将 Embedding 层权重的转置设置为 Affine 层的权重。这个非常简单的技巧可以带来出色的结果。\n",
    "\n",
    "为什么说权重共享是有效的呢？直观上，共享权重可以减少需要学习的参数数量，从而促进学习。另外，参数数量减少，还能收获抑制过拟合的好处。有论文从理论上描述了权重共享为什么有用，感兴趣的读者可以参考一下。\n",
    "\n",
    "## 更好的RNNLM的实现\n",
    "至此，我们介绍了 RNNLM 的 3 点有待改进的地方。接下来，我们来看一下这些技巧会在多大程度上有效。这里，将下图的层结构实现为 `BetterRnnlm` 类。\n",
    "\n",
    "<img src=\"./fig/betterRNNLM.png\" alt=\"betterRNNLM\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，改进的 3 点如下：\n",
    "- LSTM 层的多层化（此处为 2 层）\n",
    "- 使用 Dropout（仅应用在深度方向上）\n",
    "- 权重共享（Embedding 层和 Affine 层的权重共享）\n",
    "\n",
    "现在，我们来实现进行了这 3 点改进的 `BetterRnnlm` 类，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c60c65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') # 为了导入上一级目录的common模块\n",
    "from common.time_layers import *\n",
    "from common.np import *  # import numpy as np\n",
    "from common.base_model import BaseModel\n",
    "\n",
    "\n",
    "class BetterRnnlm(BaseModel):\n",
    "    '''\n",
    "     利用2个LSTM层并在各层使用Dropout的模型\n",
    "     基于[1]提出的模型，利用weight tying[2][3]\n",
    "\n",
    "     [1] Recurrent Neural Network Regularization (https://arxiv.org/abs/1409.2329)\n",
    "     [2] Using the Output Embedding to Improve Language Models (https://arxiv.org/abs/1608.05859)\n",
    "     [3] Tying Word Vectors and Word Classifiers (https://arxiv.org/pdf/1611.01462.pdf)\n",
    "    '''\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=650,\n",
    "                 hidden_size=650, dropout_ratio=0.5): \n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size # 词汇表大小、词向量维数、隐藏状态向量维数\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f') # 词向量矩阵\n",
    "        lstm_Wx1 = (rn(D, 4 * H) / np.sqrt(D)).astype('f') # 第1个LSTM层的输入到隐藏层的权重\n",
    "        lstm_Wh1 = (rn(H, 4 * H) / np.sqrt(H)).astype('f') # 第1个LSTM层的隐藏层到隐藏层的权重\n",
    "        lstm_b1 = np.zeros(4 * H).astype('f') # 第1个LSTM层的偏置\n",
    "        lstm_Wx2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f') # 第2个LSTM层的输入到隐藏层的权重\n",
    "        lstm_Wh2 = (rn(H, 4 * H) / np.sqrt(H)).astype('f') # 第2个LSTM层的隐藏层到隐藏层的权重\n",
    "        lstm_b2 = np.zeros(4 * H).astype('f') # 第2个LSTM层的偏置\n",
    "        affine_b = np.zeros(V).astype('f') # 输出层的偏置\n",
    "\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W), # 词嵌入层\n",
    "            TimeDropout(dropout_ratio), # Dropout层\n",
    "            TimeLSTM(lstm_Wx1, lstm_Wh1, lstm_b1, stateful=True), # 第1个LSTM层\n",
    "            TimeDropout(dropout_ratio), # Dropout层\n",
    "            TimeLSTM(lstm_Wx2, lstm_Wh2, lstm_b2, stateful=True), # 第2个LSTM层\n",
    "            TimeDropout(dropout_ratio), # Dropout层\n",
    "            TimeAffine(embed_W.T, affine_b)  # 输出层，权重参数采用weight tying[2][3]\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss() # 损失函数层\n",
    "        self.lstm_layers = [self.layers[2], self.layers[4]] # 方便后面重置状态\n",
    "        self.drop_layers = [self.layers[1], self.layers[3], self.layers[5]] # 方便设置训练标志\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs, train_flg=False): # train_flg用于区分训练和测试，train_flg=True表示训练\n",
    "        for layer in self.drop_layers:\n",
    "            layer.train_flg = train_flg # 设置训练标志\n",
    "\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs) # 前向传播\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts, train_flg=True):\n",
    "        score = self.predict(xs, train_flg) # 预测值\n",
    "        loss = self.loss_layer.forward(score, ts) # 计算损失\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout) # 损失层的反向传播\n",
    "        for layer in reversed(self.layers): # 反向传播\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        for layer in self.lstm_layers:\n",
    "            layer.reset_state() # 重置LSTM的状态"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76471f0",
   "metadata": {},
   "source": [
    "layer层的代码就是刚才所说的改进的地方，具体而言，叠加两个 `Time LSTM` 层，使用 `Time Dropout` 层，并在 `Time Embedding` 层和 `Time Affine` 层之间共享权重。\n",
    "\n",
    "下面进行改进过的 `BetterRnnlm` 类的学习。在这之前，我们稍微改动一下将要执行的学习代码。这个改动是，针对每个 epoch 使用验证数据评价困惑度，在值变差时，降低学习率。这是一种在实践中经常用到的技巧，并且往往能有好的结果。这里的实现参考了 PyTorch 的语言模型的实现示例，学习代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff81757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common import config\n",
    "# 在用GPU运行时，请打开下面的注释（需要cupy）\n",
    "# ==============================================\n",
    "# config.GPU = True\n",
    "# ==============================================\n",
    "from common.optimizer import SGD\n",
    "from common.trainer import RnnlmTrainer\n",
    "from common.util import eval_perplexity, to_gpu\n",
    "from dataset import ptb\n",
    "from better_rnnlm import BetterRnnlm\n",
    "\n",
    "\n",
    "# 设定超参数\n",
    "batch_size = 20 # mini-batch的大小\n",
    "wordvec_size = 650 # 词向量的维数\n",
    "hidden_size = 650 # 隐藏层的维数\n",
    "time_size = 35 # RNN展开的时间步数\n",
    "lr = 20.0 # 学习率\n",
    "max_epoch = 40 # 最大迭代轮数\n",
    "max_grad = 0.25 # 用于梯度裁剪的阈值\n",
    "dropout = 0.5 # Dropout的比例\n",
    "\n",
    "# 读入训练数据\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train') # 训练数据\n",
    "corpus_val, _, _ = ptb.load_data('val') # 验证数据\n",
    "corpus_test, _, _ = ptb.load_data('test') # 测试数据\n",
    "\n",
    "if config.GPU:\n",
    "    corpus = to_gpu(corpus)\n",
    "    corpus_val = to_gpu(corpus_val)\n",
    "    corpus_test = to_gpu(corpus_test)\n",
    "\n",
    "vocab_size = len(word_to_id) # 词汇表大小\n",
    "xs = corpus[:-1] # 输入数据\n",
    "ts = corpus[1:] # 目标数据\n",
    "\n",
    "model = BetterRnnlm(vocab_size, wordvec_size, hidden_size, dropout) # RNN语言模型\n",
    "optimizer = SGD(lr) # 随机梯度下降法\n",
    "trainer = RnnlmTrainer(model, optimizer) # 训练器\n",
    "\n",
    "best_ppl = float('inf') # 最佳困惑度\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(xs, ts, max_epoch=1, batch_size=batch_size,\n",
    "                time_size=time_size, max_grad=max_grad)\n",
    "\n",
    "    model.reset_state() # 在评估困惑度之前需要重置状态\n",
    "    ppl = eval_perplexity(model, corpus_val) # 计算验证数据的困惑度\n",
    "    print('验证数据的困惑度: ', ppl)  # 输出验证数据的困惑度\n",
    "\n",
    "    # 根据困惑度调整学习率\n",
    "    if best_ppl > ppl:\n",
    "        best_ppl = ppl\n",
    "        model.save_params()\n",
    "    else:\n",
    "        lr /= 4.0\n",
    "        optimizer.lr = lr\n",
    "\n",
    "    model.reset_state() # 重置状态\n",
    "    print('-' * 50)\n",
    "\n",
    "\n",
    "# 基于验证数据进行评价\n",
    "model.reset_state() # 重置状态\n",
    "ppl_test = eval_perplexity(model, corpus_test) # 计算测试数据的困惑度\n",
    "print('测试数据的困惑度: ', ppl_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a322c347",
   "metadata": {},
   "source": [
    "这里针对每个 epoch 使用验证数据评价困惑度，当它比之前的困惑度（`best_ppl`）低时，将学习率乘以 1/4。为此，我们用 `for` 循环反复执行以下处理：通过 `RnnlmTrainer` 类的 `fit()` 方法进行一个 epoch 的学习，然后使用验证数据评价困惑度。现在让我们运行一下学习代码。\n",
    "\n",
    "这个学习需要相当长的时间。在用 CPU 运行的情况下，需要 2 天左右；而如果用 GPU 运行，则能在 5 小时左右完成（在用 GPU 运行时，需要去掉文件顶部 import 语句中的 # config.GPU = True 这行注释）。\n",
    "\n",
    "执行上面的代码，困惑度平稳下降，最终在测试数据上获得了困惑度为 75.76 的结果（每次运行结果不同）。考虑到改进前的 RNNLM 的困惑度约为 136，这个结果可以说提升很大。通过 LSTM 的多层化提高表现力，通过 Dropout 提高泛化能力，通过权重共享有效利用权重，从而实现了精度的大幅提高。\n",
    "\n",
    "## 前沿研究\n",
    "至此，我们对 RNNLM 的改进就结束了。通过对 RNNLM 进行若干改造，精度显著提升，在 PTB 数据集的测试数据上达到了 75 左右的困惑度，可以说是一个还算不错的结果。不过，前沿研究走得更远。这里我想简单地介绍一下最新的研究结果，让我们来看一下下图。\n",
    "\n",
    "<img src=\"./fig/result_ptb.png\" alt=\"result_ptb\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "上图摘自文献，该表总结了过去各个阶段最优语言模型在 PTB 数据集上的困惑度结果。由 Test 列可知，随着新方法被提出，困惑度在下降，最后一行的结果是 52.8。实际上，这个 52.8 是一个非常好的结果。在 PTB 数据集上的困惑度接近 50，这在几年前还是无法想象的。\n",
    "\n",
    "这里只展示了最先进的研究结果。当然，我们的模型和它还有相当的距离，但是图中的最先进的模型和我们的模型有很多共同点。比如，最先进的模型使用了多层 LSTM 模型，并进行了基于 Dropout 的正则化（变分 Dropout 和 DropConnect）和权重共享。在此基础上，它进一步使用了最优化和正则化的几个技巧，并严格进行了超参数的调整，最终达成了 52.8 这样惊人的值。\n",
    "\n",
    "图中有一个名为AWD-LSTM 3-layer LSTM (tied) + continuous cache pointer的模型。这个continuous cache pointer技术基于第8章会详细介绍的 Attention。Attention是一项非常重要的技术，被应用在许多地方。在语言模型这个任务中，它也为精度提高做出了重大贡献。让我们期待第8章的 Attention。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150878e4",
   "metadata": {},
   "source": [
    "## 小结 \n",
    "- 在简单 RNN 的学习中，存在梯度消失和梯度爆炸问题\n",
    "- 梯度裁剪对解决梯度爆炸有效，LSTM、GRU 等 Gated RNN 对解决梯度消失有效\n",
    "- LSTM 中有 3 个门：输入门、遗忘门和输出门\n",
    "- 门有专门的权重，并使用 sigmoid 函数输出 0.0 ～ 1.0 的实数\n",
    "- LSTM 的多层化、Dropout 和权重共享等技巧可以有效改进语言模型\n",
    "- RNN 的正则化很重要，人们提出了各种基于 Dropout 的方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
