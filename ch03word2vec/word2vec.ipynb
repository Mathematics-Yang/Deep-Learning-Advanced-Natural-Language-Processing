{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d76a84f",
   "metadata": {},
   "source": [
    "#  第三章 &nbsp; &nbsp; word2vec\n",
    "接着上一章，本章的主题仍是单词的分布式表示。在上一章中，我们使用基于计数的方法得到了单词的分布式表示。本章我们将讨论该方法的替代方法，即基于推理的方法。\n",
    "\n",
    "顾名思义，基于推理的方法使用了推理机制。当然，这里的推理机制用的是神经网络。本章，著名的 word2vec 将会登场。我们将花很多时间考察 word2vec 的结构，并通过代码实现来加深对它的理解。\n",
    "\n",
    "本章的目标是实现一个简单的 word2vec。这个简单的 word2vec 会优先考虑易理解性，从而牺牲一定的处理效率。因此，我们不会用它来处理大规模数据集，但用它处理小数据集毫无问题。下一章我们会对这个简单的 word2vec 进行改进，从而完成一个“真正的” word2vec。现在，让我们一起进入基于推理的方法和 word2vec 的世界吧！\n",
    "\n",
    "## 基于推理的方法和神经网络\n",
    "用向量表示单词的研究最近正在如火如荼地展开，其中比较成功的方法大致可以分为两种：一种是基于计数的方法；另一种是基于推理的方法。虽然两者在获得单词含义的方法上差别很大，但是两者的背景都是分布式假设。\n",
    "\n",
    "本节我们将指出基于计数的方法的问题，并从宏观角度说明它的替代方法——基于推理的方法的优点。另外，为了做好 word2vec 的准备工作，我们会看一个用神经网络处理单词的例子。\n",
    "\n",
    "## 基于计数的方法的问题\n",
    "如上一章所说，基于计数的方法根据一个单词周围的单词的出现频数来表示该单词。具体来说，先生成所有单词的共现矩阵，再对这个矩阵进行 SVD，以获得密集向量（单词的分布式表示）。但是，基于计数的方法在处理大规模语料库时会出现问题。\n",
    "\n",
    "在现实世界中，语料库处理的单词数量非常大。比如，据说英文的词汇量超过 100 万个。如果词汇量超过 100 万个，那么使用基于计数的方法就需要生成一个 100 万 × 100 万的庞大矩阵，但对如此庞大的矩阵执行 SVD 显然是不现实的。\n",
    "\n",
    "对于一个$n \\times n$的矩阵，SVD的复杂度是$O(n^3)$，这表示计算量与$n$的立方成比例增长。如此大的计算成本，即便是超级计算机也无法胜任。实际上，利用近似方法和稀疏矩阵的性质，可以在一定程度上提高处理速度，但还是需要大量的计算资源和时间。\n",
    "\n",
    "基于计数的方法使用整个语料库的统计数据（共现矩阵和 PPMI 等），通过一次处理（SVD 等）获得单词的分布式表示。而基于推理的方法使用神经网络，通常在 mini-batch 数据上进行学习。这意味着神经网络一次只需要看一部分学习数据（mini-batch），并反复更新权重。这种学习机制上的差异如图所示。\n",
    "\n",
    "<img src=\"./fig/svd_vs_network.png\" alt=\"svd_vs_network\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，基于计数的方法一次性处理全部学习数据；反之，基于推理的方法使用部分学习数据逐步学习。这意味着，在词汇量很大的语料库中，即使 SVD 等的计算量太大导致计算机难以处理，神经网络也可以在部分数据上学习。并且，神经网络的学习可以使用多台机器、多个 GPU 并行执行，从而加速整个学习过程。在这方面，基于推理的方法更有优势。\n",
    "\n",
    "基于推理的方法和基于计数的方法相比，还有一些其他的优点。关于这一点，在详细说明基于推理的方法（特别是 word2vec）之后，我们会在之后的章节再次讨论。\n",
    "\n",
    "## 基于推理的方法的概要\n",
    "基于推理的方法的主要操作是“推理”。如下图所示，当给出周围的单词（上下文）时，预测“?”处会出现什么单词，这就是推理。\n",
    "\n",
    "<img src=\"./fig/context_predict.png\" alt=\"context_predict\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "解开图中的推理问题并学习规律，就是基于推理的方法的主要任务。通过反复求解这些推理问题，可以学习到单词的出现模式。从“模型视角”出发，这个推理问题如下图所示。\n",
    "\n",
    "<img src=\"./fig/predict_way.png\" alt=\"predict_way\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，基于推理的方法引入了某种模型，我们将神经网络用于此模型。这个模型接收上下文信息作为输入，并输出（可能出现的）各个单词的出现概率。在这样的框架中，使用语料库来学习模型，使之能做出正确的预测。另外，作为模型学习的产物，我们得到了单词的分布式表示。这就是基于推理的方法的全貌。\n",
    "\n",
    "基于推理的方法和基于计数的方法一样，也基于分布式假设。分布式假设假设“单词含义由其周围的单词构成”。基于推理的方法将这一假设归结为了上面的预测问题。由此可见，不管是哪种方法，如何对基于分布式假设的“单词共现”建模都是最重要的研究主题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28ea57",
   "metadata": {},
   "source": [
    "## 神经网络中单词的处理方法\n",
    "从现在开始，我们将使用神经网络来处理单词。但是，神经网络无法直接处理 you 或 say 这样的单词，要用神经网络处理单词，需要先将单词转化为固定长度的向量。对此，一种方式是将单词转换为 one-hot 表示（ one-hot 向量）。在 one-hot 表示中，只有一个元素是1，其他元素都是0。\n",
    "\n",
    "我们来看一个 one-hot 表示的例子。和上一章一样，我们用“You say goodbye and I say hello.”这个一句话的语料库来说明。在这个语料库中，一共有7个单词（“you”“say”“goodbye”“and”“i”“hello”“.”）。此时，各个单词可以转化为下图所示的 one-hot 表示。\n",
    "\n",
    "<img src=\"./fig/word_one_hot.png\" alt=\"word_one_hot\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，单词可以表示为文本、单词 ID 和 one-hot 表示。此时，要将单词转化为 one-hot 表示，就需要准备元素个数与词汇个数相等的向量，并将单词 ID 对应的元素设为 1，其他元素设为 0。像这样，只要将单词转化为固定长度的向量，神经网络的输入层的神经元个数就可以固定下来。\n",
    "\n",
    "<img src=\"./fig/network_one_hot.png\" alt=\"network_one_hot\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，输入层由 7 个神经元表示，分别对应于 7 个单词（第 1 个神经元对应于 you，第 2 个神经元对应于 say ）。\n",
    "\n",
    "现在事情变得很简单了。因为只要将单词表示为向量，这些向量就可以由构成神经网络的各种“层”来处理。比如，对于one-hot表示的某个单词，使用全连接层对其进行变换的情况如下图所示。\n",
    "\n",
    "<img src=\"./fig/words_network.png\" alt=\"words_network\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，全连接层通过箭头连接所有节点。这些箭头拥有权重（参数），它们和输入层神经元的加权和成为中间层的神经元。另外，本章使用的全连接层将省略偏置（这是为了配合后文对 word2vec 的说明）。\n",
    "\n",
    "没有偏置的全连接层相当于在计算矩阵乘积。在很多深度学习的框架中，在生成全连接层时，都可以选择不使用偏置。在本书中，不使用偏置的全连接层相当于 MatMul 层（该层已经在第 1 章中实现）。\n",
    "\n",
    "在上图中，神经元之间的连接是用箭头表示的。之后，为了明确地显示权重，我们将使用下图所示的方法。\n",
    "\n",
    "<img src=\"./fig/simple_words_network.png\" alt=\"simple_words_network\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "现在，我们看一下代码。这里的全连接层变换可以写成如下的 Python代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4e1772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.47772173 -0.25598657 -1.46795473]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]]) # one-hot 向量\n",
    "W = np.random.randn(7, 3) # 权重矩阵\n",
    "h = np.dot(c, W) # 矩阵乘法，中间节点\n",
    "print(h) # 输出中间节点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1aa143",
   "metadata": {},
   "source": [
    "这段代码将单词 ID 为 0 的单词表示为了 one - hot 表示，并用全连接层对其进行了变换。作为复习，全连接层的计算通过矩阵乘积进行。这可以用 NumPy 的 `np.dot()` 来实现（省略偏置）。\n",
    "\n",
    "这里，输入数据(变量c)的维数(ndim)是2。这是考虑了mini-batch处理，将各个数据保存在了第1维(0维度)中。\n",
    "\n",
    "希望读者注意一下 $c$ 和 $W$ 进行矩阵乘积计算的地方。此处，$c$ 是 one - hot 表示，单词 ID 对应的元素是 1，其他地方都是 0。因此，如图所示，上述代码中的 $c$ 和 $W$ 的矩阵乘积相当于“提取”权重的对应行向量。\n",
    "\n",
    "<img src=\"./fig/matrix_one_hot.png\" alt=\"matrix_one_hot\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "这里，仅为了提取权重的行向量而进行矩阵乘积计算好像不是很有效率。关于这一点，我们将在之后的章节进行改进。另外，上述代码的功能也可以使用第 1 章中实现的 MatMul 层完成，如下所示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "509aabef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.28901725 -0.78022662  0.33600646]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..') # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "from common.layers import MatMul\n",
    "\n",
    "c = np.array([[1, 0, 0, 0, 0, 0, 0]]) # one-hot 向量\n",
    "W = np.random.randn(7, 3) # 权重矩阵\n",
    "layer = MatMul(W) # MatMul层\n",
    "h = layer.forward(c) # 前向传播\n",
    "print(h) # 输出中间节点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94553789",
   "metadata": {},
   "source": [
    "## 简单的 word2vec \n",
    "上一节我们学习了基于推理的方法，并基于代码讨论了神经网络中单词的处理方法，至此准备工作就完成了，现在是时候实现 word2vec 了。\n",
    "\n",
    "我们要做的事情是将神经网络嵌入到模型中。这里，我们使用由原版 word2vec 提出的名为 **continuous bag-of-words（CBOW）** 的模型作为神经网络。\n",
    "\n",
    "word2vec一词最初用来指程序或者工具，但是随着该词的流行，在某些语境下，也指神经网络的模型。正确地说，CBOW模型和skip-gram模型是word2vec中使用的两个神经网络。本节我们将主要讨论CBOW模型。关于这两个模型的差异，我们将在之后的章节详细介绍。\n",
    "\n",
    "## CBOW模型的推理\n",
    "CBOW 模型是根据上下文预测目标词的神经网络（“目标词”是指中间的单词，它周围的单词是“上下文”）。通过训练这个 CBOW 模型，使其能尽可能地进行正确的预测，我们可以获得单词的分布式表示。\n",
    "\n",
    "CBOW 模型的输入是上下文。这个上下文用 `['you', 'goodbye']` 这样的单词列表表示。我们将其转换为 one-hot 表示，以便 CBOW 模型可以进行处理。在此基础上，CBOW 模型的网络可以画成下图这样。\n",
    "\n",
    "<img src=\"./fig/CBOW_network.png\" alt=\"CBOW_network\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "上图是 CBOW 模型的网络。它有两个输入层，经过中间层到达输出层。这里，从输入层到中间层的变换由相同的全连接层（权重为 $\\boldsymbol{W}_{\\text{in}}$）完成，从中间层到输出层神经元的变换由另一个全连接层（权重为 $\\boldsymbol{W}_{\\text{out}}$）完成。\n",
    "\n",
    "这里，因为我们对上下文仅考虑两个单词，所以输入层有两个。如果对上下文考虑$N$个单词，则输入层会有$N$个。\n",
    "\n",
    "现在，我们注意一下上图的中间层。此时，中间层的神经元是各个输入层经全连接层变换后得到的值的“平均”。就上面的例子而言，经全连接层变换后，第 1 个输入层转化为 $\\boldsymbol{h}_1$，第 2 个输入层转化为 $\\boldsymbol{h}_2$，那么中间层的神经元是 $\\frac{1}{2}(\\boldsymbol{h}_1 + \\boldsymbol{h}_2)$。\n",
    "\n",
    "最后是图中的输出层，这个输出层有 7 个神经元。这里重要的是，这些神经元对应于各个单词。输出层的神经元是各个单词的得分，它的值越大，说明对应单词的出现概率就越高。得分是指在被解释为概率之前的值，对这些得分应用 Softmax 函数，就可以得到概率。\n",
    "\n",
    "有时将得分经过Softmax层之后的神经元称为输出层。这里，我们将输出得分的节点称为输出层。\n",
    "\n",
    "如图所示，从输入层到中间层的变换由全连接层（权重是 $\\boldsymbol{W}_{\\text{in}}$）完成。此时，全连接层的权重 $\\boldsymbol{W}_{\\text{in}}$ 是一个 $7 \\times 3$ 的矩阵。提前剧透一下，这个权重就是我们要的单词的分布式表示，如下图所示。\n",
    "\n",
    "<img src=\"./fig/word_weight.png\" alt=\"word_weight\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，权重 $\\boldsymbol{W}_{\\text{in}}$ 的各行保存着各个单词的分布式表示。通过反复学习，不断更新各个单词的分布式表示，以正确地从上下文预测出应当出现的单词。令人惊讶的是，如此获得的向量很好地对单词含义进行了编码。这就是 word2vec 的全貌。\n",
    "\n",
    "中间层的神经元数量比输入层少这一点很重要。中间层需要将预测单词所需的信息压缩保存，从而产生密集的向量表示。这时，中间层被写入了我们人类无法解读的代码，这相当于“编码”工作。而从中间层的信息获得期望结果的过程则称为“解码”。这一过程将被编码的信息复原为我们可以理解的形式。\n",
    "\n",
    "到目前为止，我们从神经元视角图示了 CBOW 模型。下面，我们从层视角图示 CBOW 模型。这样一来，这个神经网络的结构如图所示。\n",
    "\n",
    "<img src=\"./fig/CBOW_layers.png\" alt=\"CBOW_layers\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，CBOW 模型一开始有两个 `MatMul` 层，这两个层的输出被加在一起。然后，对这个相加后得到的值乘以 0.5 求平均，可以得到中间层的神经元。最后，将另一个 `MatMul` 层应用于中间层的神经元，输出得分。\n",
    "\n",
    "不使用偏置的全连接层的处理由MatMul层的正向传播代理。这个层在内部计算矩阵乘积。\n",
    "\n",
    "参考上图，我们来实现 CBOW 模型的推理（即求得分的过程），具体实现如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3929e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18117023 -1.33416728  0.47815604  2.64761802  1.12622634 -3.20466293\n",
      "  -1.48876166]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul\n",
    "\n",
    "\n",
    "# 样本的上下文数据\n",
    "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
    "\n",
    "# 初始化权重\n",
    "W_in = np.random.randn(7, 3)\n",
    "W_out = np.random.randn(3, 7)\n",
    "\n",
    "# 生成层\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# 正向传播\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h)\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad411742",
   "metadata": {},
   "source": [
    "这里，我们首先将必要的权重（`W_in` 和 `W_out`）初始化。然后，生成与上下文单词数量等量（这里是两个）的处理输入层的 `MatMul` 层，输出侧仅生成一个 `MatMul` 层。需要注意的是，输入侧的 `MatMul` 层共享权重 `W_in`。\n",
    "\n",
    "之后，输入侧的 `MatMul` 层（`in_layer0` 和 `in_layer1`）调用 `forward()` 方法，计算中间数据，并通过输出侧的 `MatMul` 层（`out_layer`）计算各个单词的得分。\n",
    "\n",
    "以上就是 CBOW 模型的推理过程。这里我们见到的 CBOW 模型是没有使用激活函数的简单的网络结构。除了多个输入层共享权重外，并没有什么难点。接下来，我们继续看一下 CBOW 模型的学习。\n",
    "\n",
    "## CBOW模型的学习\n",
    "到目前为止，我们介绍的 CBOW 模型在输出层输出了各个单词的得分。通过对这些得分应用 Softmax 函数，可以获得概率。这个概率表示哪个单词会出现在给定的上下文（周围单词）中间。\n",
    "\n",
    "<img src=\"./fig/CBOW_example.png\" alt=\"CBOW_example\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在图所示的例子中，上下文是 `you` 和 `goodbye`，正确解标签（神经网络应该预测出的单词）是 `say`。这时，如果网络具有“良好的权重”，那么在表示概率的神经元中，对应正确解的神经元的得分应该更高。\n",
    "\n",
    "CBOW 模型的学习就是调整权重，以使预测准确。其结果是，权重 $\\boldsymbol{W}_{\\text{in}}$（确切地说 $\\boldsymbol{W}_{\\text{in}}$ 和 $\\boldsymbol{W}_{\\text{out}}$ 两者）学习到蕴含单词出现模式的向量。根据过去的实验，CBOW 模型（和 skip - gram 模型）得到的单词的分布式表示，特别是使用维基百科等大规模语料库学习到的单词的分布式表示，在单词的含义和语法上符合我们直觉的案例有很多。\n",
    "\n",
    "CBOW模型只是学习语料库中单词的出现模式。如果语料库不一样，学习到的单词的分布式表示也不一样。比如，只使用“体育”相关的文章得到的单词的分布式表示，和只使用“音乐”相关的文章得到的单词的分布式表示将有很大不同。\n",
    "\n",
    "现在，我们来考虑一下上述神经网络的学习。其实很简单，这里我们处理的模型是一个进行多类别分类的神经网络。因此，对其进行学习只是使用一下 Softmax 函数和交叉熵误差。首先，使用 Softmax 函数将得分转化为概率，再求这些概率和监督标签之间的交叉熵误差，并将其作为损失进行学习，这一过程可以用下图表示。\n",
    "\n",
    "<img src=\"./fig/CBOW_learning.png\" alt=\"CBOW_learning\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，只需向上一节介绍的进行推理的 CBOW 模型加上 Softmax 层和 Cross Entropy Error 层，就可以得到损失。这就是 CBOW 模型计算损失的流程，对应于神经网络的正向传播。\n",
    "\n",
    "虽然图中使用了 Softmax 层和 Cross Entropy Error 层，但是我们将这两个层实现为了一个 Softmax with Loss 层。因此，我们接下来要实现的网络实际上如下图所示。\n",
    "\n",
    "<img src=\"./fig/CBOW_softmaxwithloss.png\" alt=\"CBOW_softmaxwithloss\" style=\"display: block; margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89f1273",
   "metadata": {},
   "source": [
    "## word2vec的权重和分布式表示\n",
    "如前所述，word2vec 中使用的网络有两个权重，分别是输入侧的全连接层的权重（$\\boldsymbol{W}_{\\text{in}}$）和输出侧的全连接层的权重（$\\boldsymbol{W}_{\\text{out}}$）。一般而言，输入侧的权重 $\\boldsymbol{W}_{\\text{in}}$ 的每一行对应于各个单词的分布式表示。另外，输出侧的权重 $\\boldsymbol{W}_{\\text{out}}$ 也同样保存了对单词含义进行了编码的向量。只是，如下图所示，输出侧的权重在列方向上保存了各个单词的分布式表示。\n",
    "\n",
    "<img src=\"./fig/weight.png\" alt=\"weights\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "那么，我们最终应该使用哪个权重作为单词的分布式表示呢？这里有三个选项。\n",
    "\n",
    "- A. 只使用输入侧的权重\n",
    "- B. 只使用输出侧的权重\n",
    "- C. 同时使用两个权重\n",
    "\n",
    "方案 A 和方案 B 只使用其中一个权重。而在采用方案 C 的情况下，根据如何组合这两个权重，存在多种方式，其中一个方式就是简单地将这两个权重相加。\n",
    "\n",
    "就 word2vec（特别是 skip - gram 模型）而言，最受欢迎的是方案 A。许多研究中也都仅使用输入侧的权重 $\\boldsymbol{W}_{\\text{in}}$ 作为最终的单词的分布式表示。遵循这一思路，我们也使用 $\\boldsymbol{W}_{\\text{in}}$ 作为单词的分布式表示。\n",
    "\n",
    "文献通过实验证明了word2vec的skip-gram模型中$\\boldsymbol{W}_{\\text{in}}$的有效性。另外，在与word2vec相似的GloVe方法中，通过将两个权重相加，也获得了良好的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25eb6ec",
   "metadata": {},
   "source": [
    "## 学习数据的准备\n",
    "在开始 word2vec 的学习之前，我们先来准备学习用的数据。这里我们仍以 “You say goodbye and I say hello.” 这个只有一句话的语料库为例进行说明。\n",
    "\n",
    "## 上下文和目标词\n",
    "word2vec 中使用的神经网络的输入是上下文，它的正确解标签是被这些上下文包围在中间的单词，即目标词。也就是说，我们要做的事情是，当向神经网络输入上下文时，使目标词出现的概率高（为了达成这一目标而进行学习）。\n",
    "\n",
    "下面我们就从语料库生成上下文和目标词，如图所示。\n",
    "\n",
    "<img src=\"./fig/corpus_context.png\" alt=\"corpus_context\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "在图中，将语料库中的目标单词作为目标词，将其周围的单词作为上下文提取出来。我们对语料库中的所有单词都执行该操作（两端的单词除外），可以得到图右侧的 `contexts`（上下文）和 `target`（目标词）。`contexts` 的各行成为神经网络的输入，`target` 的各行成为正确解标签（要预测出的单词）。另外，在各笔样本数据中，上下文有多个单词（这个例子中有两个），而目标词则只有一个，因此只有上下文写成了复数形式 `contexts`。\n",
    "\n",
    "现在，我们来实现从语料库生成上下文和目标词的函数。在此之前，我们先复习一下上一章的内容。首先，将语料库的文本转化成单词 ID。这需要使用第 2 章实现的 `preprocess()` 函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770cc404",
   "metadata": {},
   "source": [
    "然后，从单词ID列表``corpus``生成``contexts``和``target``。具体来说，如图所示，实现一个当给定``corpus``时返回``contexts``和``target``的函数。\n",
    "\n",
    "<img src=\"./fig/wordID_example.png\" alt=\"wordID_example\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，`contexts` 是二维数组。此时，`contexts` 的第 0 维保存的是各个上下文数据。具体来说，`contexts[0]` 保存的是第 0 个上下文，`context[1]` 保存的是第 1 个上下文……同样地，就目标词而言，`target[0]` 保存的是第 0 个目标词，`target[1]` 保存的是第 1 个目标词……\n",
    "\n",
    "现在，我们来实现这个生成上下文和目标词的函数，这里将其称为 `create_contexts_target(corpus, window_size)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee086503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{'you': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'i': 4, 'hello': 5, '.': 6}\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n",
      "[[0 2]\n",
      " [1 3]\n",
      " [2 4]\n",
      " [3 1]\n",
      " [4 5]\n",
      " [1 6]]\n",
      "[1 2 3 4 1 5]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..') # 为了导入父目录的文件而进行的设定\n",
    "from common.util import preprocess\n",
    "\n",
    "text = 'You say goodbye and I say hello.' \n",
    "corpus, word_to_id, id_to_word = preprocess(text) # 预处理文本数据\n",
    "vocab_size = len(word_to_id) # 词汇表大小\n",
    "print(corpus) # 输出语料库\n",
    "print(word_to_id) # 输出单词对应的ID\n",
    "print(id_to_word) # 输出ID对应的单词\n",
    "\n",
    "def create_contexts_target(corpus, window_size=1):\n",
    "    target = corpus[window_size: -window_size] # 目标词，去掉开头和结尾的 window_size 个单词\n",
    "    contexts = [] # 上下文\n",
    "\n",
    "    for idx in range(window_size, len(corpus) - window_size): # 遍历语料库，跳过开头和结尾的 window_size 个单词\n",
    "        cs = [] # 当前上下文\n",
    "        for t in range(-window_size, window_size + 1): # 遍历上下文窗口\n",
    "            if t == 0: # 跳过目标词\n",
    "                continue\n",
    "            cs.append(corpus[idx + t]) # 添加上下文单词\n",
    "        contexts.append(cs) # 添加当前上下文到上下文列表\n",
    "\n",
    "    return np.array(contexts), np.array(target) # 返回上下文和目标词的数组\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size=1) # 生成上下文和目标词\n",
    "print(contexts) # 输出上下文\n",
    "print(target) # 输出目标词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22404177",
   "metadata": {},
   "source": [
    "这样就从语料库生成了上下文和目标词，后面只需将它们赋给 CBOW模型即可。不过，因为这些上下文和目标词的元素还是单词 ID，所以还需要将它们转化为 one-hot 表示。\n",
    "\n",
    "## 转化为one-hot表示 \n",
    "下面，我们将上下文和目标词转化为 one-hot 表示，如图所示。\n",
    "\n",
    "<img src=\"./fig/context_target_example.png\" alt=\"context_target_example\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，上下文和目标词从单词 ID 转化为了 one-hot 表示。这里需要注意各个多维数组的形状。在上面的例子中，使用单词 ID 时的 `contexts` 的形状是 (6,2)，将其转化为 one-hot 表示后，形状变为 (6,2,7)。\n",
    "\n",
    "本书提供了 `convert_one_hot()` 函数以将单词 ID 转化为 one-hot 表示。这个函数的实现不再说明，内容很简单，代码在 `common/util.py` 中。该函数的参数是单词 ID 列表和词汇个数。我们再把到目前为止的数据预处理总结一下，如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d6bc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0 0 0 0 0 0]\n",
      "  [0 0 1 0 0 0 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 1 0 0 0]]\n",
      "\n",
      " [[0 0 1 0 0 0 0]\n",
      "  [0 0 0 0 1 0 0]]\n",
      "\n",
      " [[0 0 0 1 0 0 0]\n",
      "  [0 1 0 0 0 0 0]]\n",
      "\n",
      " [[0 0 0 0 1 0 0]\n",
      "  [0 0 0 0 0 1 0]]\n",
      "\n",
      " [[0 1 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 1]]]\n",
      "[[0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..') # 为了导入父目录的文件而进行的设定\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text) # 预处理文本数据\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size=1) # 生成上下文和目标词\n",
    "vocab_size = len(word_to_id) # 词汇表大小\n",
    "contexts = convert_one_hot(contexts, vocab_size) # 转化为one-hot表示\n",
    "target = convert_one_hot(target, vocab_size) # 转化为one-hot表示\n",
    "print(contexts) # 输出上下文\n",
    "print(target) # 输出目标词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e67661f",
   "metadata": {},
   "source": [
    "## CBOW模型的实现 \n",
    "现在，我们来实现 CBOW 模型。这里要实现的神经网络如图所示。 \n",
    "\n",
    "<img src=\"./fig/CBOW_softmaxwithloss.png\" alt=\"CBOW_softmaxwithloss\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "我们将图中的神经网络实现为 SimpleCBOW 类（下一章将实现对其进行了改进的 CBOW 类）。首先，让我们看一下 SimpleCBOW 类的初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "effd1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') # 为了导入父目录的文件而进行的设定\n",
    "from common.layers import MatMul, SoftmaxWithLoss\n",
    "import numpy as np\n",
    "\n",
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "        # 初始化权重\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f') # 输入层权重\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f') # 输出层权重\n",
    "\n",
    "        # 生成层\n",
    "        self.in_layer0 = MatMul(W_in) \n",
    "        self.in_layer1 = MatMul(W_in) \n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # 将所有的层保存在列表中\n",
    "        self.layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], [] # 初始化参数和梯度的列表\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "        # 将单词的分布式表示设置为成员变量 \n",
    "        self.word_vecs = W_in # 词向量\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0]) # 第一个上下文，contexts[:,0]表示取contexts的所有行的第0列\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1]) # 第二个上下文，contexts[:,1]表示取contexts的所有行的第1列\n",
    "        h = 0.5 * (h0 + h1) # 上下文的平均\n",
    "        score = self.out_layer.forward(h) # 输出层\n",
    "        loss = self.loss_layer.forward(score, target) # 计算损失\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout) # 反向传播损失\n",
    "        da = self.out_layer.backward(ds) # 反向传播输出层\n",
    "        da *= 0.5 # 平均值的梯度\n",
    "        self.in_layer1.backward(da) # 反向传播第二个上下文\n",
    "        self.in_layer0.backward(da) # 反向传播第一个上下文\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05d934",
   "metadata": {},
   "source": [
    "## 学习的实现 \n",
    "CBOW 模型的学习和一般的神经网络的学习完全相同。首先，给神经网络准备好学习数据。然后，求梯度，并逐步更新权重参数。这里，我们使用第 1 章介绍的 Trainer 类来执行学习过程，学习的源代码如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08d8f7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 2 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 3 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 4 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 5 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 6 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 7 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 8 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 9 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 10 |  iter 1 / 2 | time 0[s] | loss 1.95\n",
      "| epoch 11 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 12 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 13 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 14 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 15 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 16 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 17 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 18 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 19 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 20 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 21 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 22 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 23 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 24 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 25 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 26 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 27 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 28 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 29 |  iter 1 / 2 | time 0[s] | loss 1.94\n",
      "| epoch 30 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 31 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 32 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 33 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 34 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 35 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 36 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 37 |  iter 1 / 2 | time 0[s] | loss 1.93\n",
      "| epoch 38 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 39 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 40 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 41 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 42 |  iter 1 / 2 | time 0[s] | loss 1.92\n",
      "| epoch 43 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 44 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 45 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 46 |  iter 1 / 2 | time 0[s] | loss 1.91\n",
      "| epoch 47 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 48 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 49 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 50 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 51 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 52 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 53 |  iter 1 / 2 | time 0[s] | loss 1.90\n",
      "| epoch 54 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 55 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
      "| epoch 56 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
      "| epoch 57 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
      "| epoch 58 |  iter 1 / 2 | time 0[s] | loss 1.89\n",
      "| epoch 59 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
      "| epoch 60 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
      "| epoch 61 |  iter 1 / 2 | time 0[s] | loss 1.87\n",
      "| epoch 62 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 63 |  iter 1 / 2 | time 0[s] | loss 1.88\n",
      "| epoch 64 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 65 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 66 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 67 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
      "| epoch 68 |  iter 1 / 2 | time 0[s] | loss 1.85\n",
      "| epoch 69 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
      "| epoch 70 |  iter 1 / 2 | time 0[s] | loss 1.86\n",
      "| epoch 71 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
      "| epoch 72 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
      "| epoch 73 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
      "| epoch 74 |  iter 1 / 2 | time 0[s] | loss 1.84\n",
      "| epoch 75 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
      "| epoch 76 |  iter 1 / 2 | time 0[s] | loss 1.82\n",
      "| epoch 77 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
      "| epoch 78 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
      "| epoch 79 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
      "| epoch 80 |  iter 1 / 2 | time 0[s] | loss 1.83\n",
      "| epoch 81 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
      "| epoch 82 |  iter 1 / 2 | time 0[s] | loss 1.79\n",
      "| epoch 83 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
      "| epoch 84 |  iter 1 / 2 | time 0[s] | loss 1.78\n",
      "| epoch 85 |  iter 1 / 2 | time 0[s] | loss 1.78\n",
      "| epoch 86 |  iter 1 / 2 | time 0[s] | loss 1.81\n",
      "| epoch 87 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
      "| epoch 88 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
      "| epoch 89 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
      "| epoch 90 |  iter 1 / 2 | time 0[s] | loss 1.78\n",
      "| epoch 91 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
      "| epoch 92 |  iter 1 / 2 | time 0[s] | loss 1.75\n",
      "| epoch 93 |  iter 1 / 2 | time 0[s] | loss 1.77\n",
      "| epoch 94 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
      "| epoch 95 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 96 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
      "| epoch 97 |  iter 1 / 2 | time 0[s] | loss 1.76\n",
      "| epoch 98 |  iter 1 / 2 | time 0[s] | loss 1.73\n",
      "| epoch 99 |  iter 1 / 2 | time 0[s] | loss 1.73\n",
      "| epoch 100 |  iter 1 / 2 | time 0[s] | loss 1.69\n",
      "| epoch 101 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 102 |  iter 1 / 2 | time 0[s] | loss 1.72\n",
      "| epoch 103 |  iter 1 / 2 | time 0[s] | loss 1.74\n",
      "| epoch 104 |  iter 1 / 2 | time 0[s] | loss 1.71\n",
      "| epoch 105 |  iter 1 / 2 | time 0[s] | loss 1.69\n",
      "| epoch 106 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
      "| epoch 107 |  iter 1 / 2 | time 0[s] | loss 1.68\n",
      "| epoch 108 |  iter 1 / 2 | time 0[s] | loss 1.70\n",
      "| epoch 109 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
      "| epoch 110 |  iter 1 / 2 | time 0[s] | loss 1.71\n",
      "| epoch 111 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
      "| epoch 112 |  iter 1 / 2 | time 0[s] | loss 1.66\n",
      "| epoch 113 |  iter 1 / 2 | time 0[s] | loss 1.66\n",
      "| epoch 114 |  iter 1 / 2 | time 0[s] | loss 1.69\n",
      "| epoch 115 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
      "| epoch 116 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
      "| epoch 117 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
      "| epoch 118 |  iter 1 / 2 | time 0[s] | loss 1.62\n",
      "| epoch 119 |  iter 1 / 2 | time 0[s] | loss 1.67\n",
      "| epoch 120 |  iter 1 / 2 | time 0[s] | loss 1.55\n",
      "| epoch 121 |  iter 1 / 2 | time 0[s] | loss 1.65\n",
      "| epoch 122 |  iter 1 / 2 | time 0[s] | loss 1.61\n",
      "| epoch 123 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
      "| epoch 124 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
      "| epoch 125 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
      "| epoch 126 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
      "| epoch 127 |  iter 1 / 2 | time 0[s] | loss 1.62\n",
      "| epoch 128 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
      "| epoch 129 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
      "| epoch 130 |  iter 1 / 2 | time 0[s] | loss 1.59\n",
      "| epoch 131 |  iter 1 / 2 | time 0[s] | loss 1.58\n",
      "| epoch 132 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
      "| epoch 133 |  iter 1 / 2 | time 0[s] | loss 1.64\n",
      "| epoch 134 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
      "| epoch 135 |  iter 1 / 2 | time 0[s] | loss 1.50\n",
      "| epoch 136 |  iter 1 / 2 | time 0[s] | loss 1.54\n",
      "| epoch 137 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
      "| epoch 138 |  iter 1 / 2 | time 0[s] | loss 1.57\n",
      "| epoch 139 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
      "| epoch 140 |  iter 1 / 2 | time 0[s] | loss 1.51\n",
      "| epoch 141 |  iter 1 / 2 | time 0[s] | loss 1.55\n",
      "| epoch 142 |  iter 1 / 2 | time 0[s] | loss 1.50\n",
      "| epoch 143 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
      "| epoch 144 |  iter 1 / 2 | time 0[s] | loss 1.50\n",
      "| epoch 145 |  iter 1 / 2 | time 0[s] | loss 1.52\n",
      "| epoch 146 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
      "| epoch 147 |  iter 1 / 2 | time 0[s] | loss 1.51\n",
      "| epoch 148 |  iter 1 / 2 | time 0[s] | loss 1.46\n",
      "| epoch 149 |  iter 1 / 2 | time 0[s] | loss 1.56\n",
      "| epoch 150 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 151 |  iter 1 / 2 | time 0[s] | loss 1.44\n",
      "| epoch 152 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
      "| epoch 153 |  iter 1 / 2 | time 0[s] | loss 1.48\n",
      "| epoch 154 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
      "| epoch 155 |  iter 1 / 2 | time 0[s] | loss 1.45\n",
      "| epoch 156 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
      "| epoch 157 |  iter 1 / 2 | time 0[s] | loss 1.50\n",
      "| epoch 158 |  iter 1 / 2 | time 0[s] | loss 1.43\n",
      "| epoch 159 |  iter 1 / 2 | time 0[s] | loss 1.41\n",
      "| epoch 160 |  iter 1 / 2 | time 0[s] | loss 1.46\n",
      "| epoch 161 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
      "| epoch 162 |  iter 1 / 2 | time 0[s] | loss 1.40\n",
      "| epoch 163 |  iter 1 / 2 | time 0[s] | loss 1.41\n",
      "| epoch 164 |  iter 1 / 2 | time 0[s] | loss 1.47\n",
      "| epoch 165 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
      "| epoch 166 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
      "| epoch 167 |  iter 1 / 2 | time 0[s] | loss 1.46\n",
      "| epoch 168 |  iter 1 / 2 | time 0[s] | loss 1.38\n",
      "| epoch 169 |  iter 1 / 2 | time 0[s] | loss 1.38\n",
      "| epoch 170 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
      "| epoch 171 |  iter 1 / 2 | time 0[s] | loss 1.49\n",
      "| epoch 172 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
      "| epoch 173 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
      "| epoch 174 |  iter 1 / 2 | time 0[s] | loss 1.36\n",
      "| epoch 175 |  iter 1 / 2 | time 0[s] | loss 1.34\n",
      "| epoch 176 |  iter 1 / 2 | time 0[s] | loss 1.40\n",
      "| epoch 177 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 178 |  iter 1 / 2 | time 0[s] | loss 1.42\n",
      "| epoch 179 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 180 |  iter 1 / 2 | time 0[s] | loss 1.39\n",
      "| epoch 181 |  iter 1 / 2 | time 0[s] | loss 1.39\n",
      "| epoch 182 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
      "| epoch 183 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
      "| epoch 184 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
      "| epoch 185 |  iter 1 / 2 | time 0[s] | loss 1.35\n",
      "| epoch 186 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
      "| epoch 187 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
      "| epoch 188 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 189 |  iter 1 / 2 | time 0[s] | loss 1.39\n",
      "| epoch 190 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 191 |  iter 1 / 2 | time 0[s] | loss 1.37\n",
      "| epoch 192 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 193 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 194 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 195 |  iter 1 / 2 | time 0[s] | loss 1.30\n",
      "| epoch 196 |  iter 1 / 2 | time 0[s] | loss 1.33\n",
      "| epoch 197 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
      "| epoch 198 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 199 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 200 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 201 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 202 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 203 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 204 |  iter 1 / 2 | time 0[s] | loss 1.29\n",
      "| epoch 205 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
      "| epoch 206 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 207 |  iter 1 / 2 | time 0[s] | loss 1.25\n",
      "| epoch 208 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 209 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 210 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 211 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 212 |  iter 1 / 2 | time 0[s] | loss 1.24\n",
      "| epoch 213 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 214 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 215 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 216 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 217 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 218 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 219 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 220 |  iter 1 / 2 | time 0[s] | loss 1.27\n",
      "| epoch 221 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 222 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 223 |  iter 1 / 2 | time 0[s] | loss 1.23\n",
      "| epoch 224 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 225 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 226 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 227 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 228 |  iter 1 / 2 | time 0[s] | loss 1.25\n",
      "| epoch 229 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 230 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 231 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 232 |  iter 1 / 2 | time 0[s] | loss 1.14\n",
      "| epoch 233 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 234 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 235 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 236 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 237 |  iter 1 / 2 | time 0[s] | loss 1.15\n",
      "| epoch 238 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 239 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 240 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 241 |  iter 1 / 2 | time 0[s] | loss 1.21\n",
      "| epoch 242 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 243 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 244 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 245 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 246 |  iter 1 / 2 | time 0[s] | loss 1.20\n",
      "| epoch 247 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 248 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 249 |  iter 1 / 2 | time 0[s] | loss 1.19\n",
      "| epoch 250 |  iter 1 / 2 | time 0[s] | loss 1.12\n",
      "| epoch 251 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 252 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 253 |  iter 1 / 2 | time 0[s] | loss 1.22\n",
      "| epoch 254 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 255 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 256 |  iter 1 / 2 | time 0[s] | loss 1.17\n",
      "| epoch 257 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 258 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 259 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 260 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 261 |  iter 1 / 2 | time 0[s] | loss 1.16\n",
      "| epoch 262 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 263 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 264 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 265 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 266 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 267 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 268 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 269 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 270 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 271 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 272 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 273 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 274 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 275 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 276 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 277 |  iter 1 / 2 | time 0[s] | loss 1.18\n",
      "| epoch 278 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 279 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 280 |  iter 1 / 2 | time 0[s] | loss 1.04\n",
      "| epoch 281 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 282 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 283 |  iter 1 / 2 | time 0[s] | loss 1.08\n",
      "| epoch 284 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 285 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 286 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 287 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 288 |  iter 1 / 2 | time 0[s] | loss 1.09\n",
      "| epoch 289 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 290 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 291 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 292 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 293 |  iter 1 / 2 | time 0[s] | loss 0.99\n",
      "| epoch 294 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 295 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 296 |  iter 1 / 2 | time 0[s] | loss 1.10\n",
      "| epoch 297 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 298 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 299 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 300 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 301 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 302 |  iter 1 / 2 | time 0[s] | loss 1.05\n",
      "| epoch 303 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 304 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 305 |  iter 1 / 2 | time 0[s] | loss 1.13\n",
      "| epoch 306 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 307 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 308 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 309 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 310 |  iter 1 / 2 | time 0[s] | loss 1.06\n",
      "| epoch 311 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 312 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 313 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 314 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 315 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 316 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 317 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 318 |  iter 1 / 2 | time 0[s] | loss 1.07\n",
      "| epoch 319 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 320 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 321 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 322 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 323 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 324 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 325 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 326 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 327 |  iter 1 / 2 | time 0[s] | loss 1.01\n",
      "| epoch 328 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 329 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 330 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 331 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 332 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 333 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 334 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 335 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 336 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 337 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 338 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 339 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 340 |  iter 1 / 2 | time 0[s] | loss 1.11\n",
      "| epoch 341 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 342 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 343 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 344 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 345 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 346 |  iter 1 / 2 | time 0[s] | loss 1.02\n",
      "| epoch 347 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 348 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 349 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 350 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 351 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 352 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 353 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 354 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 355 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 356 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 357 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 358 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 359 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 360 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 361 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 362 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 363 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 364 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 365 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 366 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 367 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 368 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 369 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 370 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 371 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 372 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 373 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 374 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 375 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 376 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 377 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 378 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 379 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 380 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 381 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 382 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 383 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 384 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 385 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 386 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 387 |  iter 1 / 2 | time 0[s] | loss 0.90\n",
      "| epoch 388 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 389 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 390 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 391 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 392 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 393 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 394 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 395 |  iter 1 / 2 | time 0[s] | loss 1.03\n",
      "| epoch 396 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 397 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 398 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 399 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 400 |  iter 1 / 2 | time 0[s] | loss 0.93\n",
      "| epoch 401 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 402 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 403 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 404 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 405 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 406 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 407 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 408 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 409 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 410 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 411 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 412 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 413 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 414 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 415 |  iter 1 / 2 | time 0[s] | loss 0.97\n",
      "| epoch 416 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 417 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 418 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 419 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 420 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 421 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 422 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 423 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 424 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 425 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 426 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 427 |  iter 1 / 2 | time 0[s] | loss 0.95\n",
      "| epoch 428 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 429 |  iter 1 / 2 | time 0[s] | loss 0.98\n",
      "| epoch 430 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 431 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 432 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 433 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 434 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 435 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 436 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 437 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 438 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 439 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 440 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 441 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 442 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 443 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 444 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 445 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 446 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 447 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 448 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 449 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 450 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 451 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 452 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 453 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 454 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 455 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 456 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 457 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 458 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 459 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 460 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 461 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 462 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 463 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 464 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 465 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 466 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 467 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 468 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 469 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 470 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 471 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 472 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 473 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 474 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 475 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 476 |  iter 1 / 2 | time 0[s] | loss 0.96\n",
      "| epoch 477 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 478 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 479 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 480 |  iter 1 / 2 | time 0[s] | loss 1.00\n",
      "| epoch 481 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 482 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 483 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 484 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 485 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 486 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 487 |  iter 1 / 2 | time 0[s] | loss 0.88\n",
      "| epoch 488 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 489 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 490 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 491 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 492 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 493 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 494 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 495 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 496 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 497 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 498 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 499 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 500 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 501 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 502 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 503 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 504 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 505 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 506 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 507 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 508 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 509 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 510 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 511 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 512 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 513 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 514 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 515 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 516 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 517 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 518 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 519 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 520 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 521 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 522 |  iter 1 / 2 | time 0[s] | loss 0.92\n",
      "| epoch 523 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 524 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 525 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 526 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 527 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 528 |  iter 1 / 2 | time 0[s] | loss 0.91\n",
      "| epoch 529 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 530 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 531 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 532 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 533 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 534 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 535 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 536 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 537 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 538 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 539 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 540 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 541 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 542 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 543 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 544 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 545 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 546 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 547 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 548 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 549 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 550 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 551 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 552 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 553 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 554 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 555 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 556 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 557 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 558 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 559 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 560 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 561 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 562 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 563 |  iter 1 / 2 | time 0[s] | loss 0.89\n",
      "| epoch 564 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 565 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 566 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 567 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 568 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 569 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 570 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 571 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 572 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 573 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 574 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 575 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 576 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 577 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 578 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 579 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 580 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 581 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 582 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 583 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 584 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 585 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 586 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 587 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 588 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 589 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 590 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 591 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 592 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 593 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
      "| epoch 594 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 595 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 596 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 597 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 598 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 599 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 600 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 601 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 602 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 603 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 604 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 605 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 606 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 607 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 608 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 609 |  iter 1 / 2 | time 0[s] | loss 0.86\n",
      "| epoch 610 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
      "| epoch 611 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 612 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 613 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 614 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 615 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 616 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 617 |  iter 1 / 2 | time 0[s] | loss 0.94\n",
      "| epoch 618 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 619 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 620 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 621 |  iter 1 / 2 | time 0[s] | loss 0.81\n",
      "| epoch 622 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 623 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 624 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 625 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 626 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 627 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 628 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 629 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 630 |  iter 1 / 2 | time 0[s] | loss 0.87\n",
      "| epoch 631 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 632 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 633 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 634 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 635 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 636 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 637 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 638 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 639 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 640 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 641 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 642 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 643 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 644 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 645 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 646 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 647 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 648 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 649 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 650 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 651 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 652 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 653 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 654 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 655 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 656 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 657 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 658 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 659 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 660 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 661 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 662 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 663 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 664 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 665 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 666 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 667 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 668 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 669 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 670 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 671 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 672 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 673 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 674 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 675 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 676 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 677 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 678 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 679 |  iter 1 / 2 | time 0[s] | loss 0.80\n",
      "| epoch 680 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 681 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 682 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 683 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 684 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 685 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 686 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 687 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 688 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 689 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 690 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 691 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 692 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
      "| epoch 693 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 694 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 695 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
      "| epoch 696 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 697 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
      "| epoch 698 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 699 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 700 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 701 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 702 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 703 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 704 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 705 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 706 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 707 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 708 |  iter 1 / 2 | time 0[s] | loss 0.79\n",
      "| epoch 709 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 710 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 711 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 712 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 713 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 714 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 715 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 716 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
      "| epoch 717 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 718 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 719 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 720 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 721 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 722 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
      "| epoch 723 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 724 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 725 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 726 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 727 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 728 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 729 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 730 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 731 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 732 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 733 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 734 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 735 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 736 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 737 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 738 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 739 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 740 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
      "| epoch 741 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 742 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 743 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 744 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 745 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 746 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 747 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 748 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 749 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 750 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 751 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 752 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 753 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 754 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 755 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 756 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
      "| epoch 757 |  iter 1 / 2 | time 0[s] | loss 0.85\n",
      "| epoch 758 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 759 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 760 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 761 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 762 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 763 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 764 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 765 |  iter 1 / 2 | time 0[s] | loss 0.47\n",
      "| epoch 766 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 767 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 768 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 769 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 770 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 771 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 772 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 773 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 774 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 775 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 776 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 777 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 778 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 779 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 780 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 781 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 782 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 783 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 784 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 785 |  iter 1 / 2 | time 0[s] | loss 0.74\n",
      "| epoch 786 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 787 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 788 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 789 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 790 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 791 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 792 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 793 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 794 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 795 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 796 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
      "| epoch 797 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 798 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 799 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 800 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 801 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 802 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 803 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 804 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 805 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 806 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 807 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 808 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 809 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 810 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 811 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
      "| epoch 812 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 813 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 814 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 815 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 816 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
      "| epoch 817 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 818 |  iter 1 / 2 | time 0[s] | loss 0.84\n",
      "| epoch 819 |  iter 1 / 2 | time 0[s] | loss 0.35\n",
      "| epoch 820 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 821 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 822 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 823 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 824 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 825 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 826 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 827 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 828 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 829 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 830 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 831 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 832 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 833 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 834 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 835 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 836 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 837 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
      "| epoch 838 |  iter 1 / 2 | time 0[s] | loss 0.77\n",
      "| epoch 839 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 840 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 841 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 842 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 843 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 844 |  iter 1 / 2 | time 0[s] | loss 0.73\n",
      "| epoch 845 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
      "| epoch 846 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 847 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 848 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 849 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
      "| epoch 850 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 851 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 852 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 853 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 854 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 855 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
      "| epoch 856 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 857 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
      "| epoch 858 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 859 |  iter 1 / 2 | time 0[s] | loss 0.41\n",
      "| epoch 860 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 861 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 862 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 863 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 864 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 865 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 866 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 867 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 868 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 869 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 870 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 871 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 872 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 873 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 874 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 875 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 876 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 877 |  iter 1 / 2 | time 0[s] | loss 0.40\n",
      "| epoch 878 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 879 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 880 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 881 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 882 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 883 |  iter 1 / 2 | time 0[s] | loss 0.66\n",
      "| epoch 884 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 885 |  iter 1 / 2 | time 0[s] | loss 0.34\n",
      "| epoch 886 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 887 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 888 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 889 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 890 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
      "| epoch 891 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 892 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 893 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 894 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 895 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 896 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 897 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 898 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 899 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 900 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 901 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 902 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 903 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 904 |  iter 1 / 2 | time 0[s] | loss 0.83\n",
      "| epoch 905 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 906 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 907 |  iter 1 / 2 | time 0[s] | loss 0.48\n",
      "| epoch 908 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 909 |  iter 1 / 2 | time 0[s] | loss 0.78\n",
      "| epoch 910 |  iter 1 / 2 | time 0[s] | loss 0.37\n",
      "| epoch 911 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 912 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 913 |  iter 1 / 2 | time 0[s] | loss 0.58\n",
      "| epoch 914 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 915 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 916 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 917 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 918 |  iter 1 / 2 | time 0[s] | loss 0.44\n",
      "| epoch 919 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 920 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 921 |  iter 1 / 2 | time 0[s] | loss 0.72\n",
      "| epoch 922 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 923 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
      "| epoch 924 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 925 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 926 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 927 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 928 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 929 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 930 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 931 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 932 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 933 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 934 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 935 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
      "| epoch 936 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 937 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 938 |  iter 1 / 2 | time 0[s] | loss 0.76\n",
      "| epoch 939 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
      "| epoch 940 |  iter 1 / 2 | time 0[s] | loss 0.62\n",
      "| epoch 941 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 942 |  iter 1 / 2 | time 0[s] | loss 0.39\n",
      "| epoch 943 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 944 |  iter 1 / 2 | time 0[s] | loss 0.59\n",
      "| epoch 945 |  iter 1 / 2 | time 0[s] | loss 0.51\n",
      "| epoch 946 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 947 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 948 |  iter 1 / 2 | time 0[s] | loss 0.43\n",
      "| epoch 949 |  iter 1 / 2 | time 0[s] | loss 0.82\n",
      "| epoch 950 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 951 |  iter 1 / 2 | time 0[s] | loss 0.65\n",
      "| epoch 952 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 953 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 954 |  iter 1 / 2 | time 0[s] | loss 0.54\n",
      "| epoch 955 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 956 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 957 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 958 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 959 |  iter 1 / 2 | time 0[s] | loss 0.56\n",
      "| epoch 960 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 961 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 962 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 963 |  iter 1 / 2 | time 0[s] | loss 0.70\n",
      "| epoch 964 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 965 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
      "| epoch 966 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 967 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 968 |  iter 1 / 2 | time 0[s] | loss 0.55\n",
      "| epoch 969 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 970 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 971 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 972 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 973 |  iter 1 / 2 | time 0[s] | loss 0.32\n",
      "| epoch 974 |  iter 1 / 2 | time 0[s] | loss 0.57\n",
      "| epoch 975 |  iter 1 / 2 | time 0[s] | loss 0.75\n",
      "| epoch 976 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 977 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 978 |  iter 1 / 2 | time 0[s] | loss 0.46\n",
      "| epoch 979 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 980 |  iter 1 / 2 | time 0[s] | loss 0.61\n",
      "| epoch 981 |  iter 1 / 2 | time 0[s] | loss 0.60\n",
      "| epoch 982 |  iter 1 / 2 | time 0[s] | loss 0.68\n",
      "| epoch 983 |  iter 1 / 2 | time 0[s] | loss 0.42\n",
      "| epoch 984 |  iter 1 / 2 | time 0[s] | loss 0.71\n",
      "| epoch 985 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
      "| epoch 986 |  iter 1 / 2 | time 0[s] | loss 0.69\n",
      "| epoch 987 |  iter 1 / 2 | time 0[s] | loss 0.63\n",
      "| epoch 988 |  iter 1 / 2 | time 0[s] | loss 0.38\n",
      "| epoch 989 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 990 |  iter 1 / 2 | time 0[s] | loss 0.49\n",
      "| epoch 991 |  iter 1 / 2 | time 0[s] | loss 0.64\n",
      "| epoch 992 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 993 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
      "| epoch 994 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 995 |  iter 1 / 2 | time 0[s] | loss 0.52\n",
      "| epoch 996 |  iter 1 / 2 | time 0[s] | loss 0.50\n",
      "| epoch 997 |  iter 1 / 2 | time 0[s] | loss 0.53\n",
      "| epoch 998 |  iter 1 / 2 | time 0[s] | loss 0.45\n",
      "| epoch 999 |  iter 1 / 2 | time 0[s] | loss 0.67\n",
      "| epoch 1000 |  iter 1 / 2 | time 0[s] | loss 0.56\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you [-1.152637   1.2406265 -1.1476054  1.1026222  1.0784377]\n",
      "say [ 1.2119906   0.07885174  1.2025355  -0.71677864 -1.2493712 ]\n",
      "goodbye [-0.6545541   0.8700723  -0.6849925   0.8572864   0.66757965]\n",
      "and [ 0.8021923   1.7211291   0.8966693  -1.4857216  -0.79610026]\n",
      "i [-0.6484298   0.87395054 -0.6534116   0.85718924  0.6680209 ]\n",
      "hello [-1.1610407  1.228169  -1.1594363  1.1231062  1.0924869]\n",
      ". [ 1.2432971 -1.4944408  1.2402366  1.4756571 -1.2471172]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # 为了引入父目录的文件而进行的设定\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "from simple_cbow import SimpleCBOW\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "\n",
    "window_size = 1 # 上下文窗口大小\n",
    "hidden_size = 5 # 隐藏层大小\n",
    "batch_size = 3 # 批大小\n",
    "max_epoch = 1000 # 最大迭代次数\n",
    "\n",
    "text = 'You say goodbye and I say hello.' \n",
    "corpus, word_to_id, id_to_word = preprocess(text) # 预处理文本数据\n",
    "\n",
    "vocab_size = len(word_to_id) # 词汇表大小\n",
    "contexts, target = create_contexts_target(corpus, window_size) # 生成上下文和目标词\n",
    "target = convert_one_hot(target, vocab_size) # 转化为one-hot表示\n",
    "contexts = convert_one_hot(contexts, vocab_size) # 转化为one-hot表示\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size) # 生成CBOW模型\n",
    "optimizer = Adam() # 生成Adam优化器\n",
    "trainer = Trainer(model, optimizer) # 生成Trainer对象\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size) # 训练模型\n",
    "trainer.plot() # 绘制学习曲线\n",
    "\n",
    "word_vecs = model.word_vecs # 获取词向量，本质是输入层的权重W_in\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id]) # 输出单词及其词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0477738c",
   "metadata": {},
   "source": [
    "这里，使用 `word_vecs` 这个变量保存权重。`word_vecs` 的各行保存了对应的单词 ID 的分布式表示。实际运行一下，可以得到上述结果。\n",
    "\n",
    "我们终于将单词表示为了密集向量！这就是单词的分布式表示。我们有理由相信，这样的分布式表示能够很好地捕获单词含义。\n",
    "\n",
    "不过，遗憾的是，这里使用的小型语料库并没有给出很好的结果。当然，主要原因是语料库太小了。如果换成更大、更实用的语料库，相信会获得更好的结果。但是，这样在处理速度方面又会出现新的问题，这是因为当前这个 CBOW 模型的实现在处理效率方面存在几个问题。下一章我们将改进这个简单的 CBOW 模型，实现一个 “真正的” CBOW 模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20885b7",
   "metadata": {},
   "source": [
    "## Word2vec的补充说明 \n",
    "至此，我们详细探讨了word2vec的CBOW模型。接下来，我们将对word2vec补充说明几个非常重要的话题。首先，我们从概率的角度，再来看一下CBOW模型。\n",
    "\n",
    "## CBOW模型和概率 \n",
    "首先简单说明一下概率的表示方法。本书中将概率记为 $P(\\cdot)$，比如事件 $A$ 发生的概率记为 $P(A)$。**联合概率**记为 $P(A,B)$，表示事件 $A$ 和事件 $B$ 同时发生的概率。\n",
    "\n",
    "**后验概率**记为 $P(A|B)$，字面意思是“事件发生后的概率”。从另一个角度来看，也可以解释为“在给定事件 $B$（的信息）时事件 $A$ 发生的概率”。\n",
    "\n",
    "下面，我们用概率的表示方法来描述 CBOW 模型。CBOW 模型进行的处理是，当给定某个上下文时，输出目标词的概率。这里，我们使用包含单词 $w_1, w_2, \\cdots, w_T$ 的语料库。如图所示，对第 $t$ 个单词，考虑窗口大小为 1 的上下文。\n",
    "\n",
    "<img src=\"./fig/CBOW_word2vec.png\" alt=\"CBOW_word2vec\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "下面，我们用数学式来表示当给定上下文 $w_{t - 1}$ 和 $w_{t + 1}$ 时目标词为 $w_t$ 的概率。使用后验概率，有式 (3.1)：\n",
    "\n",
    "$$P(w_t | w_{t - 1}, w_{t + 1}) \\tag{3.1}$$\n",
    "\n",
    "式 (3.1) 表示“在 $w_{t - 1}$ 和 $w_{t + 1}$ 发生后，$w_t$ 发生的概率”，也可以解释为“当给定 $w_{t - 1}$ 和 $w_{t + 1}$ 时，$w_t$ 发生的概率”。也就是说，CBOW 模型可以建模为式 (3.1)。\n",
    "\n",
    "这里，使用式 (3.1) 可以简洁地表示 CBOW 模型的损失函数。我们把第 1 章介绍的交叉熵误差函数（式 (1.7)）套用在这里。式 (1.7) 是 $L = -\\sum_k t_k \\log y_k$，其中，$y_k$ 表示第 $k$ 个事件发生的概率。$t_k$ 是监督标签，它是 one-hot 向量的元素。这里需要注意的是，“$w_t$ 发生”这一事件是正确解，它对应的 one-hot 向量的元素是 1，其他元素都是 0（也就是说，当 $w_t$ 之外的事件发生时，对应的 one-hot 向量的元素均为 0）。考虑到这一点，可以推导出下式：\n",
    "\n",
    "$$L = -\\log P(w_t | w_{t - 1}, w_{t + 1}) \\tag{3.2}$$\n",
    "\n",
    "CBOW 模型的损失函数只是对式 (3.1) 的概率取 $\\log$，并加上负号。顺便提一下，这也称为**负对数似然**（negative log likelihood）。式 (3.2) 是一笔样本数据的损失函数。如果将其扩展到整个语料库，则损失函数可以写为：\n",
    "\n",
    "$$L = -\\frac{1}{T} \\sum_{t = 1}^{T} \\log P(w_t | w_{t - 1}, w_{t + 1}) \\tag{3.3}$$\n",
    "\n",
    "CBOW 模型学习的任务就是让式 (3.3) 表示的损失函数尽可能地小。那时的权重参数就是我们想要的单词的分布式表示。这里，我们只考虑了窗口大小为 1 的情况，不过其他的窗口大小（或者窗口大小为 $m$ 的一般情况）也很容易用数学式表示。\n",
    "\n",
    "## skip-gram模型\n",
    "如前所述，word2vec 有两个模型：一个是我们已经讨论过的 CBOW 模型；另一个是被称为 skip-gram 的模型。skip-gram 是反转了 CBOW 模型处理的上下文和目标词的模型。举例来说，两者要解决的问题如图所示。\n",
    "\n",
    "<img src=\"./fig/CBOW_skip_gram.png\" alt=\"CBOW_skip_gram\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "如图所示，CBOW模型从上下文的多个单词预测中间的单词（目标词），而skip-gram模型则从中间的单词（目标词）预测周围的多个单词（上下文）。此时，skip-gram模型的网络结构如下图所示。\n",
    "\n",
    "<img src=\"./fig/skip_gram_example.png\" alt=\"skip_gram_example\" style=\"display: block; margin: 0 auto;\">\n",
    "\n",
    "由图可知，skip - gram 模型的输入层只有一个，输出层的数量则与上下文的单词个数相等。因此，首先要分别求出各个输出层的损失（通过 Softmax with Loss 层等），然后将它们加起来作为最后的损失。\n",
    "\n",
    "现在，我们使用概率的表示方法来表示 skip - gram 模型。我们来考虑根据中间单词（目标词）$w_t$ 预测上下文 $w_{t - 1}$ 和 $w_{t + 1}$ 的情况。此时，skip - gram 可以建模为式 (3.4)：\n",
    "\n",
    "$$P(w_{t - 1}, w_{t + 1} | w_t) \\tag{3.4}$$\n",
    "\n",
    "式 (3.4) 表示“当给定 $w_t$ 时，$w_{t - 1}$ 和 $w_{t + 1}$ 同时发生的概率”。这里，在 skip - gram 模型中，假定上下文的单词之间没有相关性（正确地说是假定“条件独立”），将式 (3.4) 如下进行分解：\n",
    "\n",
    "$$P(w_{t - 1}, w_{t + 1} | w_t) = P(w_{t - 1} | w_t) P(w_{t + 1} | w_t) \\tag{3.5}$$\n",
    "\n",
    "通过将式 (3.5) 代入交叉熵误差函数，可以推导出 skip - gram 模型的损失函数：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L &= -\\log P(w_{t - 1}, w_{t + 1} | w_t) \\\\\n",
    "&= -\\log P(w_{t - 1} | w_t) P(w_{t + 1} | w_t) \\tag{3.6} \\\\\n",
    "&= -(\\log P(w_{t - 1} | w_t) + \\log P(w_{t + 1} | w_t))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "这里利用了对数的性质 $\\log xy = \\log x + \\log y$。如式 (3.6) 所示，skip - gram 模型的损失函数先分别求出各个上下文对应的损失，然后将它们加在一起。式 (3.6) 是一笔样本数据的损失函数。如果扩展到整个语料库，则 skip - gram 模型的损失函数可以表示为式 (3.7)：\n",
    "\n",
    "$$L = -\\frac{1}{T} \\sum_{t = 1}^{T} (\\log P(w_{t - 1} | w_t) + \\log P(w_{t + 1} | w_t)) \\tag{3.7}$$\n",
    "\n",
    "比较式 (3.7) 和 CBOW 模型的式 (3.3)，差异是非常明显的。因为 skip - gram 模型的预测次数和上下文单词数量一样多，所以它的损失函数需要求各个上下文单词对应的损失的总和。而 CBOW 模型只需要求目标词的损失。以上就是对 skip - gram 模型的介绍。\n",
    "\n",
    "那么，我们应该使用 CBOW 模型和 skip - gram 模型中的哪一个呢？答案应该是 skip - gram 模型。这是因为，从单词的分布式表示的准确度来看，在大多数情况下，skip - gram 模型的结果更好。特别是随着语料库规模的增大，在低频词和类推问题的性能方面，skip - gram 模型往往会有更好的表现（单词的分布式表示的评价方法会在之后的章节说明）。此外，就学习速度而言，CBOW 模型比 skip - gram 模型要快。这是因为 skip - gram 模型需要根据上下文数量计算相应个数的损失，计算成本变大。\n",
    "\n",
    "skip-gram模型根据一个单词预测其周围的单词，这是一个非常难的问题。假如我们来解决上图中的问题，此时，对于CBOW模型的问题，我们很容易回答“say”。但是，对于skip-gram模型的问题，则存在许多候选。因此，可以说skip-gram模型要解决的是更难的问题。经过这个更难的问题的锻炼，skip-gram模型能提供更好的单词的分布式表示。\n",
    "\n",
    "理解了 CBOW 模型的实现，在实现 skip - gram 模型时应该就不存在什么难点了。因此，这里就不再介绍 skip - gram 模型的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e63db149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') # 为了导入父目录的文件而进行的设定\n",
    "import numpy as np\n",
    "from common.layers import MatMul, SoftmaxWithLoss\n",
    "\n",
    "\n",
    "class SimpleSkipGram:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 初始化权重\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f') # 输入层权重\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f') # 输出层权重\n",
    "\n",
    "        # 生成层\n",
    "        self.in_layer = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer1 = SoftmaxWithLoss()\n",
    "        self.loss_layer2 = SoftmaxWithLoss()\n",
    "\n",
    "        # 将所有的权重和梯度整理到列表中\n",
    "        layers = [self.in_layer, self.out_layer]\n",
    "        self.params, self.grads = [], [] # 初始化参数和梯度的列表\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 将单词的分布式表示设置为成员变量\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = self.in_layer.forward(target)\n",
    "        s = self.out_layer.forward(h)\n",
    "        l1 = self.loss_layer1.forward(s, contexts[:, 0]) # 第一个上下文\n",
    "        l2 = self.loss_layer2.forward(s, contexts[:, 1]) # 第二个上下文\n",
    "        loss = l1 + l2 # 总损失\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dl1 = self.loss_layer1.backward(dout)\n",
    "        dl2 = self.loss_layer2.backward(dout)\n",
    "        ds = dl1 + dl2\n",
    "        dh = self.out_layer.backward(ds)\n",
    "        self.in_layer.backward(dh)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7467b273",
   "metadata": {},
   "source": [
    "## 基于计数与基于推理\n",
    "到目前为止，我们已经了解了基于计数的方法和基于推理的方法（特别是 word2vec）。两种方法在学习机制上存在显著差异：基于计数的方法通过对整个语料库的统计数据进行一次学习来获得单词的分布式表示，而基于推理的方法则反复观察语料库的一部分数据进行学习（mini - batch 学习）。这里，我们就其他方面来对比一下这两种方法。\n",
    "\n",
    "首先，我们考虑需要向词汇表添加新词并更新单词的分布式表示的场景。此时，基于计数的方法需要从头开始计算。即便是想稍微修改一下单词的分布式表示，也需要重新完成生成共现矩阵、进行 SVD 等一系列操作。相反，基于推理的方法（word2vec）允许参数的增量学习。具体来说，可以将之前学习到的权重作为下一次学习的初始值，在不损失之前学习到的经验的情况下，高效地更新单词的分布式表示。在这方面，基于推理的方法（word2vec）具有优势。\n",
    "\n",
    "其次，两种方法得到的单词的分布式表示的性质和准确度有什么差异呢？就分布式表示的性质而言，基于计数的方法主要是编码单词的相似性，而 word2vec（特别是 skip - gram 模型）除了单词的相似性以外，还能理解更复杂的单词之间的模式。关于这一点，word2vec 因能解开 “king - man + woman = queen” 这样的类推问题而知名（关于类推问题，我们将在之后的章节说明）。\n",
    "\n",
    "这里有一个常见的误解，那就是基于推理的方法在准确度方面优于基于计数的方法。实际上，有研究表明，就单词相似性的定量评价而言，基于推理的方法和基于计数的方法难分上下。\n",
    "\n",
    "2014年发表的题为 “Don't count, predict!”（不要计数，要预测！）的论文系统地比较了基于计数的方法和基于推理的方法，并给出了基于推理的方法在准确度上始终更好的结论。但是，之后又有其他的论文提出，就单词的相似性而言，结论高度依赖于超参数，基于计数的方法和基于推理的方法难分胜负。\n",
    "\n",
    "另外一个重要的事实是，基于推理的方法和基于计数的方法存在关联性。具体地说，使用了 skip - gram 和下一章介绍的 Negative Sampling 的模型被证明与对整个语料库的共现矩阵（实际上会对矩阵进行一定的修改）进行特殊矩阵分解的方法具有相同的作用。换句话说，这两个方法论（在某些条件下）是 “相通” 的。\n",
    "\n",
    "此外，在 word2vec 之后，有研究人员提出了 GloVe 方法。GloVe 方法融合了基于推理的方法和基于计数的方法。该方法的思想是，将整个语料库的统计数据的信息纳入损失函数，进行 mini - batch 学习。据此，这两个方法论成功地被融合在了一起。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800f3768",
   "metadata": {},
   "source": [
    "## 小结\n",
    "- 基于推理的方法以预测为目标，同时获得了作为副产物的单词的分布式表示\n",
    "- word2vec 是基于推理的方法，由简单的 2 层神经网络构成\n",
    "- word2vec 有 skip-gram 模型和 CBOW 模型\n",
    "- CBOW 模型从多个单词（上下文）预测 1 个单词（目标词）\n",
    "- skip-gram 模型反过来从 1 个单词（目标词）预测多个单词（上下文）\n",
    "- 由于 word2vec 可以进行权重的增量学习，所以能够高效地更新或添加单词的分布式表示"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
